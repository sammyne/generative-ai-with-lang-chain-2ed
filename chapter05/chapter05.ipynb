{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7e1ce8",
   "metadata": {},
   "source": [
    "# 05. Building Intelligent Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f19f6",
   "metadata": {},
   "source": [
    "## 0. 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-core~=0.3 langchain-openai~=0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e279bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langgraph~=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-community~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        embeddings_model = os.getenv(\"OPENAI_EMBEDDINGS_MODEL\")\n",
    "        hf_pretrained_embeddings_model = os.getenv(\"HF_PRETRAINED_EMBEDDINGS_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.hf_pretrained_embeddings_model = hf_pretrained_embeddings_model if hf_pretrained_embeddings_model else 'Qwen/Qwen3-Embedding-8B'\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_embeddings(self, **kwargs) -> OpenAIEmbeddings:\n",
    "        if not self.embeddings_model:\n",
    "            raise ValueError(\"OPENAI_EMBEDDINGS_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            model=self.embeddings_model,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.tiktoken_enabled\n",
    "            # 对于非 OpenAI 的官方实现，将这个参数置为 False。\n",
    "            # 回退到用 huggingface transformers 库 AutoTokenizer 来处理 token。\n",
    "            tiktoken_enabled=False,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.model\n",
    "            # 元宝说 Jina 的 embedding 模型 https://huggingface.co/jinaai/jina-embeddings-v4 最接近\n",
    "            # text-embedding-ada-002\n",
    "            # 个人喜好，选了 Qwen/Qwen3-Embedding-8B\n",
    "            # tiktoken_model_name='Qwen/Qwen3-Embedding-8B',\n",
    "            tiktoken_model_name=self.hf_pretrained_embeddings_model,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6c196",
   "metadata": {},
   "source": [
    "## What is a tool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af38b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "question = \"how old is the US president?\"\n",
    "\n",
    "raw_prompt_template = (\n",
    "  \"You have access to search engine that provides you an \"\n",
    "  \"information about fresh events and news given the query. \"\n",
    "  \"Given the question, decide whether you need an additional \"\n",
    "  \"information from the search engine (reply with 'SEARCH: \"\n",
    "   \"<generated query>' or you know enough to answer the user \"\n",
    "   \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "   \"Do not make any assumptions on recent events or things that can change.\"\n",
    "   \"Now, act to answer a user question:\\n{QUESTION}\"\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "(prompt_template | llm).invoke(question).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7305668",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What is the capital of Germany?\"\n",
    "\n",
    "(prompt_template | llm).invoke(question1).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"age of current US president\"\n",
    "search_result = (\n",
    "    \"Donald Trump › Age 78 years June 14, 1946\\n\"\n",
    "    \"Donald Trump 45th and 47th U.S. President Donald John Trump is an American \"\n",
    "    \"politician, media personality, and businessman who has served as the 47th \"\n",
    "    \"president of the United States since January 20, 2025. A member of the \"\n",
    "    \"Republican Party, he previously served as the 45th president from 2017 to 2021. Wikipedia\"\n",
    ")\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Today is {date}.\"\n",
    "    \"Now, act to answer a user question and \"\n",
    "    \"take into account your previous actions:\\n\"\n",
    "    \"HUMAN: {question}\\n\"\n",
    "    \"AI: SEARCH: {query}\\n\"\n",
    "    \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "result = (prompt_template | llm).invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"query\": query,\n",
    "        \"search_result\": search_result,\n",
    "        \"date\": \"Feb 2025\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cdcb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"current US president\"\n",
    "search_result = \"Donald Trump 45th and 47th U.S.\"\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Today is {date}.\"\n",
    "    \"Now, act to answer a user question and \"\n",
    "    \"take into account your previous actions:\\n\"\n",
    "    \"HUMAN: {question}\\n\"\n",
    "    \"AI: SEARCH: {query}\\n\"\n",
    "    \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "result = (prompt_template | llm).invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"query\": query,\n",
    "        \"search_result\": search_result,\n",
    "        \"date\": \"Feb 2025\",\n",
    "    }\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18600905",
   "metadata": {},
   "source": [
    "### Tools in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0853679",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"google_search\",\n",
    "        \"description\": \"Returns about common facts, fresh events and news from Google Search engine based on a query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"search_query\",\n",
    "                    \"description\": \"Search query to be sent to the search engine\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "step1 = llm.invoke(question, tools=[search_tool])\n",
    "\n",
    "step1.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b936e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "tool_result = ToolMessage(\n",
    "    content=\"Donald Trump › Age 78 years June 14, 1946\\n\",\n",
    "    tool_call_id=step1.tool_calls[0][\"id\"],\n",
    ")\n",
    "step2 = llm.invoke(\n",
    "    [HumanMessage(content=question), step1, tool_result], tools=[search_tool]\n",
    ")\n",
    "assert len(step2.tool_calls) == 0\n",
    "\n",
    "print(step2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a69fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind(tools=[search_tool])\n",
    "\n",
    "llm_with_tools.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835c720",
   "metadata": {},
   "source": [
    "### ReACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def mocked_google_search(query: str) -> str:\n",
    "    print(f\"CALLED GOOGLE_SEARCH with query={query}\")\n",
    "    return \"Donald Trump is a president of USA and he's 78 years old\"\n",
    "\n",
    "\n",
    "def mocked_calculator(expression: str) -> float:\n",
    "    print(f\"CALLED CALCULATOR with expression={expression}\")\n",
    "    if \"sqrt\" in expression:\n",
    "        return math.sqrt(78 * 132)\n",
    "    return 78 * 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28097b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "calculator_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Computes mathematical expressions\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"expression\",\n",
    "                    \"description\": \"A mathematical expression to be evaluated by a calculator\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"google_search\",\n",
    "        \"description\": \"Returns about common facts, fresh events and news from Google Search engine based on a query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"search_query\",\n",
    "                    \"description\": \"Search query to be sent to the search engine\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "system_prompt = (\n",
    "    \"Always use a calculator for mathematical computations, and use Google Search \"\n",
    "    \"for information about common facts, fresh events and news. Do not assume anything, keep in \"\n",
    "    \"mind that things are changing and always \"\n",
    "    \"check yourself with external sources if possible.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = prompt | llm.bind_tools([search_tool, calculator_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c990dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.graph import MessagesState, START, END\n",
    "\n",
    "\n",
    "def invoke_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "def call_tools(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_calls = last_message.tool_calls\n",
    "\n",
    "    new_messages = []\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call[\"name\"] == \"google_search\":\n",
    "            tool_result = mocked_google_search(**tool_call[\"args\"])\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "        elif tool_call[\"name\"] == \"calculator\":\n",
    "            tool_result = mocked_calculator(**tool_call[\"args\"])\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Tool {tool_call['name']} is not defined!\")\n",
    "    return {\"messages\": new_messages}\n",
    "\n",
    "\n",
    "def should_run_tools(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"call_tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"invoke_llm\", invoke_llm)\n",
    "builder.add_node(\"call_tools\", call_tools)\n",
    "\n",
    "builder.add_edge(START, \"invoke_llm\")\n",
    "builder.add_conditional_edges(\"invoke_llm\", should_run_tools)\n",
    "builder.add_edge(\"call_tools\", \"invoke_llm\")\n",
    "graph = builder.compile()\n",
    "\n",
    "question = \"What is a square root of the current US president’s age multiplied by 132?\"\n",
    "\n",
    "result = graph.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(model=llm, tools=[search_tool, calculator_tool], prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7f8bf",
   "metadata": {},
   "source": [
    "## Defining tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d7c66",
   "metadata": {},
   "source": [
    "### Built-in LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b144a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install ddgs~=9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ce891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun(api_wrapper_kwargs={\"backend\": \"api\"})\n",
    "print(f\"Tool's name = {search.name}\")\n",
    "print(f\"Tool's name = {search.description}\")\n",
    "print(f\"Tool's arg schema = {search.args_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc197587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search.tool import DDGInput\n",
    "\n",
    "\n",
    "DDGInput.model_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c332db",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the weather in Munich like tomorrow?\"\n",
    "search_input = DDGInput(query=query)\n",
    "search.invoke(search_input.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(query, tools=[search]) 会要求包 search 不是合法的 json 对象。\n",
    "llm.bind_tools([search]).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.bind_tools([search]).invoke(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Always use a duckduckgo_search tool for queries that require a fresh information\",\n",
    "        ),\n",
    "        (\"user\", \"How much is 2+2?\"),\n",
    "    ]\n",
    ")\n",
    "assert not result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "agent = create_react_agent(\n",
    "  model=llm,\n",
    "  tools=[search],\n",
    "  prompt=\"Always use a duckduckgo_search tool for queries that require a fresh information\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fb77ef72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in agent.stream({\"messages\": [(\"user\", query)]}):\n",
    "  messages = event.get(\"agent\", event.get(\"tools\", {})).get(\"messages\", [])\n",
    "  for m in messages:\n",
    "     m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d99fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n",
    "\n",
    "toolkit = RequestsToolkit(\n",
    "    requests_wrapper=TextRequestsWrapper(headers={}),\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "for tool in toolkit.get_tools():\n",
    "    print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b604141",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_spec = \"\"\"\n",
    "openapi: 3.0.0\n",
    "info:\n",
    "  title: Frankfurter Currency Exchange API\n",
    "  version: v1\n",
    "  description: API for retrieving currency exchange rates. Pay attention to the base currency and change it if needed.\n",
    "\n",
    "servers:\n",
    "  - url: https://api.frankfurter.dev/v1\n",
    "\n",
    "paths:\n",
    "  /v1/{date}:\n",
    "    get:\n",
    "      summary: Get exchange rates for a specific date.\n",
    "      parameters:\n",
    "        - in: path\n",
    "          name: date\n",
    "          schema:\n",
    "            type: string\n",
    "            pattern: '^\\d{4}-\\d{2}-\\d{2}$' # YYYY-MM-DD format\n",
    "          required: true\n",
    "          description: The date for which to retrieve exchange rates.  Use YYYY-MM-DD format.  Example: 2009-01-04\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: GBP,USD,EUR\n",
    "\n",
    "  /v1/latest:\n",
    "    get:\n",
    "      summary: Get the latest exchange rates.\n",
    "      parameters:\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: CHF,GBP\n",
    "        - in: query\n",
    "          name: base\n",
    "          schema:\n",
    "            type: string\n",
    "          description: The base currency for the exchange rates. If not provided, EUR is used as a base currency. Example: USD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578335d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = (\n",
    "    \"You're given the API spec:\\n{api_spec}\\n\"\n",
    "    \"Use the API to answer users' queries if possible. \"\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm, toolkit.get_tools(), prompt=system_message.format(api_spec=api_spec)\n",
    ")\n",
    "\n",
    "query = \"What is the swiss franc to US dollar exchange rate?\"\n",
    "\n",
    "events = agent.stream(\n",
    "    {\"messages\": [(\"user\", query)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee703b01",
   "metadata": {},
   "source": [
    "### Custom tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6f629",
   "metadata": {},
   "source": [
    "#### Wrapping a Python function as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install numexpr~=2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ee111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numexpr as ne\n",
    "import math\n",
    "\n",
    "math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp},\n",
    "ne.evaluate((\"2+2\"), local_dict=math_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
    "\n",
    "    Always add * to operations, examples:\n",
    "      73i -> 73*i\n",
    "      7pi**2 -> 7*pi**2\n",
    "    \"\"\"\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "\n",
    "assert isinstance(calculator, BaseTool)\n",
    "\n",
    "print(f\"Tool name: {calculator.name}\")\n",
    "print(f\"Tool description: {calculator.description}\")\n",
    "print(f\"Tool schema: {calculator.args_schema.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "query = \"How much is 2+3i squared?\"\n",
    "\n",
    "agent = create_react_agent(llm, [calculator])\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", query)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "question = \"What is a square root of the current US president's age multiplied by 132?\"\n",
    "\n",
    "system_hint = \"Think step-by-step. Always use search to get the fresh information about events or public facts that can change over time. Now is 2025 and remember president elections in the US recently happened.\"\n",
    "\n",
    "agent = create_react_agent(llm, [calculator, search], prompt=system_hint)\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", question)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729a9ec",
   "metadata": {},
   "source": [
    "#### Creating a tool from a Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import convert_runnable_to_tool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18513a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原书的错误调用方式：\n",
    "#   llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n",
    "# 具体错误提示片段为：\n",
    "#  Unable to serialize unknown type: <class 'method'>\n",
    "\n",
    "llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    expression: str = Field(description=\"Mathematical expression to be evaluated\")\n",
    "\n",
    "\n",
    "def calculator(state: CalculatorArgs, config: RunnableConfig) -> str:\n",
    "    expression = state[\"expression\"]\n",
    "    math_constants = config[\"configurable\"].get(\"math_constants\", {})\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")\n",
    "\n",
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    args_schema=CalculatorArgs,\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")\n",
    "\n",
    "assert isinstance(calculator_tool, BaseTool)\n",
    "\n",
    "print(f\"Tool name: {calculator_tool.name}\")\n",
    "print(f\"Tool description: {calculator_tool.description}\")\n",
    "print(f\"Args schema: {calculator_tool.args_schema.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "config = {\"configurable\": {\"math_constants\": math_constants}}\n",
    "\n",
    "# 错误调用方式：tool_call = llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n",
    "tool_call = llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\").tool_calls[0]\n",
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b53efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_tool.invoke(tool_call[\"args\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666c3d5",
   "metadata": {},
   "source": [
    "#### Subclass StructuredTool or BaseTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_tool = StructuredTool.from_function(\n",
    "    name=\"calculator\",\n",
    "    description=(\"Calculates a single mathematical expression, incl. complex numbers.\"),\n",
    "    func=calculator,\n",
    "    args_schema=CalculatorArgs,\n",
    ")\n",
    "\n",
    "llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\").tool_calls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4974cd1",
   "metadata": {},
   "source": [
    "### Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\"\"\"\n",
    "    return str(ne.evaluate(expression.strip(), local_dict={}))\n",
    "\n",
    "calculator_tool = StructuredTool.from_function(\n",
    "    func=calculator,\n",
    "    handle_tool_error=True\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm, [calculator_tool])\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", \"How much is (2+3i)^2\")]}, stream_mode=\"values\", config=config):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfaaf4",
   "metadata": {},
   "source": [
    "## Advanced tool-calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e02899",
   "metadata": {},
   "source": [
    "## Incorporating tools into workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dea58",
   "metadata": {},
   "source": [
    "### Controlled generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2ee2b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99d90f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"Prepare a step-by-step plan to solve the given task. \"\n",
    "    \" Return the answer in JSON format.\"\n",
    "    \" Each step should be a recorded in plan's steps field.\\n\"\n",
    "    \"TASK:\\n{task}\\n\"\n",
    ")\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "\n",
    "result = (prompt | llm.with_structured_output(Plan)).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d6d62ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of steps: 15\n",
      "Research the current market on Amazon to identify top-selling books about generative AI, including their titles, structures, pricing, and customer reviews.\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(result, Plan)\n",
    "\n",
    "print(f\"Amount of steps: {len(result.steps)}\")\n",
    "\n",
    "for step in result.steps:\n",
    "  print(step)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2a441",
   "metadata": {},
   "source": [
    "#### Controlled generation provided by the vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa16e96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mHow to write a bestseller on Amazon about generative AI?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m result = (prompt | llm.with_structured_output(schema=plan_schema, method=\u001b[33m\"\u001b[39m\u001b[33mjson_mode\u001b[39m\u001b[33m\"\u001b[39m)).invoke(query)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m))\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAmount of steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "plan_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "          \"properties\": {\n",
    "              \"step\": {\"type\": \"STRING\"},\n",
    "          },\n",
    "      },\n",
    "}\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "\n",
    "result = (prompt | llm.with_structured_output(schema=plan_schema, method=\"json_mode\")).invoke(query)\n",
    "\n",
    "# FIXME: result is a dict with a single key \"steps\"\n",
    "assert(isinstance(result, list))\n",
    "print(f\"Amount of steps: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b6606",
   "metadata": {},
   "source": [
    "### ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c3e4b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numexpr as ne\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
    "\n",
    "    Always add * to operations, examples:\n",
    "      73i -> 73*i\n",
    "      7pi**2 -> 7*pi**2\n",
    "    \"\"\"\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "llm_with_tools = Config().new_openai_like().bind_tools([search, calculator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "532d7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "def invoke_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"invoke_llm\", invoke_llm)\n",
    "builder.add_node(\"tools\", ToolNode([search, calculator]))\n",
    "\n",
    "builder.add_edge(START, \"invoke_llm\")\n",
    "builder.add_conditional_edges(\"invoke_llm\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"invoke_llm\")\n",
    "# builder.add_edge(\"tools\", END)\n",
    "builder.add_edge(\"invoke_llm\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "105cf623",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# FIXME: 因未知原因画不出来。\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m display(Image(\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py:702\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    693\u001b[39m     draw_mermaid_png,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    696\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    697\u001b[39m     curve_style=curve_style,\n\u001b[32m    698\u001b[39m     node_colors=node_colors,\n\u001b[32m    699\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    700\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    701\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py:310\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    304\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    305\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    306\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    307\u001b[39m         )\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    318\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py:463\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    459\u001b[39m     msg = (\n\u001b[32m    460\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# FIXME: 因未知原因画不出来。\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'invoke_llm': {'messages': [AIMessage(content='calculator{\"expression\": \"2+2\"}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 282, 'total_tokens': 291, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-plus-2025-09-11', 'system_fingerprint': None, 'id': 'chatcmpl-16a284df-7717-4aac-957e-9544f2b63a88', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6948fa3d-e369-4056-9c3a-119d1912360f-0', usage_metadata={'input_tokens': 282, 'output_tokens': 9, 'total_tokens': 291, 'input_token_details': {}, 'output_token_details': {}})]}}\n"
     ]
    }
   ],
   "source": [
    "# FIXME: 只输出一条消息不合理。\n",
    "for e in graph.stream({\"messages\": (\"human\", \"How much is 2+2\")}):\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8931a19",
   "metadata": {},
   "source": [
    "### Tool-calling paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "77f788db",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "  \"I signed my contract 2 years ago\",\n",
    "  \"I started the deal with your company in February last year\",\n",
    "  \"Our contract started on March 24th two years ago\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cce6f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_date(year: int, month: int = 1, day: int = 1) -> date:\n",
    "    \"\"\"Returns a date object given year, month and day.\n",
    "\n",
    "      Default month and day are 1 (January) and 1.\n",
    "      Examples in YYYY-MM-DD format:\n",
    "        2023-07-27 -> date(2023, 7, 27)\n",
    "        2022-12-15 -> date(2022, 12, 15)\n",
    "        March 2022 -> date(2022, 3)\n",
    "        2021 -> date(2021)\n",
    "    \"\"\"\n",
    "    return date(year, month, day).isoformat()\n",
    "\n",
    "\n",
    "@tool\n",
    "def time_difference(days: int = 0, weeks: int = 0, months: int = 0, years: int = 0) -> date:\n",
    "    \"\"\"Returns a date given a difference in days, weeks, months and years relative to the current date.\n",
    "\n",
    "    By default, dayss, weeks, months and years are 0.\n",
    "    Examples:\n",
    "      two weeks ago -> time_difference(weeks=2)\n",
    "      last year -> time_difference(years=1)\n",
    "    \"\"\"\n",
    "    dt = date.today() - timedelta(days=days, weeks=weeks)\n",
    "    new_year = dt.year+(dt.month-months) // 12 - years\n",
    "    new_month = (dt.month-months) % 12\n",
    "    return dt.replace(year=new_year, month=new_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ce08f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I signed my contract 2 years ago Your contract was signed on September 27, 2023.\n",
      "\n",
      "I started the deal with your company in February last year The contract started on **February 1, 2024**.\n",
      "\n",
      "Our contract started on March 24th two years ago The contract started on March 24, 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    [get_date, time_difference],\n",
    "    prompt=\"Extract the starting date of a contract. Current year is 2025.\",\n",
    ")\n",
    "\n",
    "\n",
    "for example in examples:\n",
    "    result = agent.invoke({\"messages\": [(\"user\", example)]})\n",
    "    print(example, result[\"messages\"][-1].content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5959957",
   "metadata": {},
   "source": [
    "## What are agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed094a3",
   "metadata": {},
   "source": [
    "### Plan-and-solve agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4379fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "\n",
    "system_prompt_template = (\n",
    "    \"For the given task, come up with a step by step plan.\\n\"\n",
    "    \"This plan should involve individual tasks, that if executed correctly will \"\n",
    "    \"yield the correct answer in JSON format and recorded in field 'steps'.\\n\"\n",
    "    \"Do not add any superfluous steps.\\n\"\n",
    "    \"The result of the final step should be the final answer. Make sure that each \"\n",
    "    \"step has all the information needed - do not skip steps.\"\n",
    ")\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like(temperature=1.0)\n",
    "\n",
    "planner = planner_prompt | llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "456e72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "from langchain_core.tools import tool, convert_runnable_to_tool\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    expression: str = Field(description=\"Mathematical expression to be evaluated\")\n",
    "\n",
    "def calculator(state: CalculatorArgs, config: RunnableConfig) -> str:\n",
    "    expression = state[\"expression\"]\n",
    "    math_constants = config[\"configurable\"].get(\"math_constants\", {})\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")\n",
    "\n",
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    args_schema=CalculatorArgs,\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "462d00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 1.16s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                           \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m     0 B/35.82 KiB           \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m     0 B/35.82 KiB           \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m     0 B/102.65 KiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[3A\n",
      "\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[3A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[3A\n",
      "\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 16.00 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 35.82 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 35.82 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.88 KiB/102.65 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)--------------\u001b[0m\u001b[0m 30.88 KiB/102.65 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)--------------\u001b[0m\u001b[0m 30.88 KiB/102.65 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)--------------\u001b[0m\u001b[0m 46.88 KiB/102.65 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)[2m-----------\u001b[0m\u001b[0m 62.88 KiB/102.65 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)----\u001b[2m------\u001b[0m\u001b[0m 78.88 KiB/102.65 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)--------\u001b[2m--\u001b[0m\u001b[0m 94.88 KiB/102.65 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)----------\u001b[2m\u001b[0m\u001b[0m 102.65 KiB/102.65 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 486ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/3] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m.5                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.13.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msoupsieve\u001b[0m\u001b[2m==2.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install arxiv==2.2.0 wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0e0d3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm) + [calculator_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d6258599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a smart assistant that carefully helps to solve complex tasks.\\n\"\n",
    "    \" Given a general plan to solve a task and a specific step, work on this step. \"\n",
    "    \" Don't assume anything, keep in minds things might change and always try to \"\n",
    "    \"use tools to double-check yourself.\\n\"\n",
    "    \" Use a calculator for mathematical computations, use Search to gather\"\n",
    "    \"for information about common facts, fresh events and news, use Arxiv to get \"\n",
    "    \"ideas on recent research and use Wikipedia for common knowledge.\"\n",
    ")\n",
    "\n",
    "step_template = (\n",
    "    \"Given the task and the plan, try to execute on a specific step of the plan.\\n\"\n",
    "    \"TASK:\\n{task}\\n\\nPLAN:\\n{plan}\\n\\nSTEP TO EXECUTE:\\n{step}\\n\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", step_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class StepState(AgentState):\n",
    "    plan: str\n",
    "    step: str\n",
    "    task: str\n",
    "\n",
    "\n",
    "execution_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    state_schema=StepState,\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "af5a0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "import operator\n",
    "\n",
    "\n",
    "class PlanState(TypedDict):\n",
    "    task: str\n",
    "    plan: Plan\n",
    "    past_steps: Annotated[list[str], operator.add]\n",
    "    final_response: str\n",
    "\n",
    "\n",
    "def get_current_step(state: PlanState) -> int:\n",
    "    \"\"\"Returns the number of current step to be executed.\"\"\"\n",
    "    return len(state.get(\"past_steps\", []))\n",
    "\n",
    "\n",
    "def get_full_plan(state: PlanState) -> str:\n",
    "    \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n",
    "    full_plan = []\n",
    "    for i, step in enumerate(state[\"plan\"].steps):\n",
    "        full_step = f\"# {i+1}. Planned step: {step}\\n\"\n",
    "        if i < get_current_step(state):\n",
    "            full_step += f\"Result: {state['past_steps'][i]}\\n\"\n",
    "        full_plan.append(full_step)\n",
    "    return \"\\n\".join(full_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "246ee35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "async def _build_initial_plan(state: PlanState) -> PlanState:\n",
    "    print(f\"task: {state['task']}\")\n",
    "    plan = await planner.ainvoke(state[\"task\"])\n",
    "    print(f\"plan: {plan}\")\n",
    "    print(f\"plan.steps: {len(plan.steps)}\")\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "async def _run_step(state: PlanState) -> PlanState:\n",
    "    print(f\"state: {state}\")\n",
    "    plan = state[\"plan\"]\n",
    "    current_step = get_current_step(state)\n",
    "    step = await execution_agent.ainvoke(\n",
    "        {\n",
    "            \"plan\": get_full_plan(state),\n",
    "            \"step\": plan.steps[current_step],\n",
    "            \"task\": state[\"task\"],\n",
    "        }\n",
    "    )\n",
    "    return {\"past_steps\": [step[\"messages\"][-1].content]}\n",
    "\n",
    "\n",
    "async def _get_final_response(state: PlanState) -> PlanState:\n",
    "    final_response = await (final_prompt | llm).ainvoke(\n",
    "        {\"task\": state[\"task\"], \"plan\": get_full_plan(state)}\n",
    "    )\n",
    "    return {\"final_response\": final_response}\n",
    "\n",
    "\n",
    "def _should_continue(state: PlanState) -> Literal[\"run\", \"response\"]:\n",
    "    if get_current_step(state) < len(state[\"plan\"].steps):\n",
    "        return \"run\"\n",
    "    return \"response\"\n",
    "\n",
    "\n",
    "builder = StateGraph(PlanState)\n",
    "builder.add_node(\"initial_plan\", _build_initial_plan)\n",
    "builder.add_node(\"run\", _run_step)\n",
    "builder.add_node(\"response\", _get_final_response)\n",
    "\n",
    "builder.add_edge(START, \"initial_plan\")\n",
    "builder.add_edge(\"initial_plan\", \"run\")\n",
    "builder.add_conditional_edges(\"run\", _should_continue)\n",
    "builder.add_edge(\"response\", END)\n",
    "\n",
    "graph = builder.compile().with_config({\"recursion_limit\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f20109f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50156116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: Write a strategic one-pager of building an AI startup\n",
      "plan: steps=['Define the core mission and vision of the AI startup, specifying the problem it solves and the target market.', 'Identify the unique value proposition (UVP) that differentiates the startup from competitors, focusing on proprietary technology, data advantage, or operational efficiency.', 'Outline the key AI technologies and methodologies the startup will leverage (e.g., generative AI, computer vision, NLP, reinforcement learning) with specific use cases.', 'Map the target customer segments and describe how the product/service addresses their pain points with measurable outcomes.', 'Detail the go-to-market strategy including sales channels, pricing model, and early adopter acquisition plan.', 'Describe the founding team’s relevant expertise in AI, engineering, and business, highlighting key hires or partnerships needed.', 'Present a high-level financial roadmap: initial funding requirements, revenue projections for years 1–3, and key cost drivers (e.g., compute, talent, data acquisition).', 'Identify key milestones and timelines for product development, pilot deployments, and funding rounds (e.g., MVP in 6 months, Series A in 18 months).', 'Highlight key risks (e.g., regulatory, data privacy, model bias) and mitigation strategies to demonstrate strategic foresight.', 'Summarize the exit potential or long-term vision (e.g., acquisition by tech giant, IPO, licensing model) to appeal to investors.', 'Compile all sections into a single-page, visually clean document using concise bullet points, headings, and minimal text, optimized for executive readability.', 'Review and refine the one-pager for clarity, consistency, and persuasive impact, ensuring alignment with investor expectations and industry standards.']\n",
      "plan.steps: 12\n",
      "state: {'task': 'Write a strategic one-pager of building an AI startup', 'plan': Plan(steps=['Define the core mission and vision of the AI startup, specifying the problem it solves and the target market.', 'Identify the unique value proposition (UVP) that differentiates the startup from competitors, focusing on proprietary technology, data advantage, or operational efficiency.', 'Outline the key AI technologies and methodologies the startup will leverage (e.g., generative AI, computer vision, NLP, reinforcement learning) with specific use cases.', 'Map the target customer segments and describe how the product/service addresses their pain points with measurable outcomes.', 'Detail the go-to-market strategy including sales channels, pricing model, and early adopter acquisition plan.', 'Describe the founding team’s relevant expertise in AI, engineering, and business, highlighting key hires or partnerships needed.', 'Present a high-level financial roadmap: initial funding requirements, revenue projections for years 1–3, and key cost drivers (e.g., compute, talent, data acquisition).', 'Identify key milestones and timelines for product development, pilot deployments, and funding rounds (e.g., MVP in 6 months, Series A in 18 months).', 'Highlight key risks (e.g., regulatory, data privacy, model bias) and mitigation strategies to demonstrate strategic foresight.', 'Summarize the exit potential or long-term vision (e.g., acquisition by tech giant, IPO, licensing model) to appeal to investors.', 'Compile all sections into a single-page, visually clean document using concise bullet points, headings, and minimal text, optimized for executive readability.', 'Review and refine the one-pager for clarity, consistency, and persuasive impact, ensuring alignment with investor expectations and industry standards.']), 'past_steps': []}\n",
      "state: {'task': 'Write a strategic one-pager of building an AI startup', 'plan': Plan(steps=['Define the core mission and vision of the AI startup, specifying the problem it solves and the target market.', 'Identify the unique value proposition (UVP) that differentiates the startup from competitors, focusing on proprietary technology, data advantage, or operational efficiency.', 'Outline the key AI technologies and methodologies the startup will leverage (e.g., generative AI, computer vision, NLP, reinforcement learning) with specific use cases.', 'Map the target customer segments and describe how the product/service addresses their pain points with measurable outcomes.', 'Detail the go-to-market strategy including sales channels, pricing model, and early adopter acquisition plan.', 'Describe the founding team’s relevant expertise in AI, engineering, and business, highlighting key hires or partnerships needed.', 'Present a high-level financial roadmap: initial funding requirements, revenue projections for years 1–3, and key cost drivers (e.g., compute, talent, data acquisition).', 'Identify key milestones and timelines for product development, pilot deployments, and funding rounds (e.g., MVP in 6 months, Series A in 18 months).', 'Highlight key risks (e.g., regulatory, data privacy, model bias) and mitigation strategies to demonstrate strategic foresight.', 'Summarize the exit potential or long-term vision (e.g., acquisition by tech giant, IPO, licensing model) to appeal to investors.', 'Compile all sections into a single-page, visually clean document using concise bullet points, headings, and minimal text, optimized for executive readability.', 'Review and refine the one-pager for clarity, consistency, and persuasive impact, ensuring alignment with investor expectations and industry standards.']), 'past_steps': ['Sorry, need more steps to process this request.']}\n",
      "state: {'task': 'Write a strategic one-pager of building an AI startup', 'plan': Plan(steps=['Define the core mission and vision of the AI startup, specifying the problem it solves and the target market.', 'Identify the unique value proposition (UVP) that differentiates the startup from competitors, focusing on proprietary technology, data advantage, or operational efficiency.', 'Outline the key AI technologies and methodologies the startup will leverage (e.g., generative AI, computer vision, NLP, reinforcement learning) with specific use cases.', 'Map the target customer segments and describe how the product/service addresses their pain points with measurable outcomes.', 'Detail the go-to-market strategy including sales channels, pricing model, and early adopter acquisition plan.', 'Describe the founding team’s relevant expertise in AI, engineering, and business, highlighting key hires or partnerships needed.', 'Present a high-level financial roadmap: initial funding requirements, revenue projections for years 1–3, and key cost drivers (e.g., compute, talent, data acquisition).', 'Identify key milestones and timelines for product development, pilot deployments, and funding rounds (e.g., MVP in 6 months, Series A in 18 months).', 'Highlight key risks (e.g., regulatory, data privacy, model bias) and mitigation strategies to demonstrate strategic foresight.', 'Summarize the exit potential or long-term vision (e.g., acquisition by tech giant, IPO, licensing model) to appeal to investors.', 'Compile all sections into a single-page, visually clean document using concise bullet points, headings, and minimal text, optimized for executive readability.', 'Review and refine the one-pager for clarity, consistency, and persuasive impact, ensuring alignment with investor expectations and industry standards.']), 'past_steps': ['Sorry, need more steps to process this request.', 'Sorry, need more steps to process this request.']}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[178]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m task = \u001b[33m\"\u001b[39m\u001b[33mWrite a strategic one-pager of building an AI startup\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m graph.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: task})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3112\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3109\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3110\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3113\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3114\u001b[39m     config,\n\u001b[32m   3115\u001b[39m     context=context,\n\u001b[32m   3116\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3118\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3119\u001b[39m     print_mode=print_mode,\n\u001b[32m   3120\u001b[39m     output_keys=output_keys,\n\u001b[32m   3121\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3122\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3123\u001b[39m     durability=durability,\n\u001b[32m   3124\u001b[39m     **kwargs,\n\u001b[32m   3125\u001b[39m ):\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:474\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[177]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36m_run_step\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     24\u001b[39m plan = state[\u001b[33m\"\u001b[39m\u001b[33mplan\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     25\u001b[39m current_step = get_current_step(state)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m step = \u001b[38;5;28;01mawait\u001b[39;00m execution_agent.ainvoke(\n\u001b[32m     27\u001b[39m     {\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplan\u001b[39m\u001b[33m\"\u001b[39m: get_full_plan(state),\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m: plan.steps[current_step],\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     31\u001b[39m     }\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mpast_steps\u001b[39m\u001b[33m\"\u001b[39m: [step[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3112\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3109\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3110\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3113\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3114\u001b[39m     config,\n\u001b[32m   3115\u001b[39m     context=context,\n\u001b[32m   3116\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3118\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3119\u001b[39m     print_mode=print_mode,\n\u001b[32m   3120\u001b[39m     output_keys=output_keys,\n\u001b[32m   3121\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3122\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3123\u001b[39m     durability=durability,\n\u001b[32m   3124\u001b[39m     **kwargs,\n\u001b[32m   3125\u001b[39m ):\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:465\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m         run = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m         ret = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(coro, context=context)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    467\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:655\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.acall_model\u001b[39m\u001b[34m(state, runtime, config)\u001b[39m\n\u001b[32m    653\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m dynamic_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m static_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    658\u001b[39m response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3290\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3288\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3289\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3290\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3291\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3292\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5723\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5716\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5717\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5721\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5722\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5723\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5724\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5725\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5726\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5727\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:417\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    409\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m     **kwargs: Any,\n\u001b[32m    415\u001b[39m ) -> BaseMessage:\n\u001b[32m    416\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    418\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    419\u001b[39m         stop=stop,\n\u001b[32m    420\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    421\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    422\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    423\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    424\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    425\u001b[39m         **kwargs,\n\u001b[32m    426\u001b[39m     )\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1034\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     **kwargs: Any,\n\u001b[32m   1032\u001b[39m ) -> LLMResult:\n\u001b[32m   1033\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1035\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1036\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:954\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    942\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    943\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     run_id=run_id,\n\u001b[32m    949\u001b[39m )\n\u001b[32m    951\u001b[39m input_messages = [\n\u001b[32m    952\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    953\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    955\u001b[39m     *[\n\u001b[32m    956\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    957\u001b[39m             m,\n\u001b[32m    958\u001b[39m             stop=stop,\n\u001b[32m    959\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    960\u001b[39m             **kwargs,\n\u001b[32m    961\u001b[39m         )\n\u001b[32m    962\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    963\u001b[39m     ],\n\u001b[32m    964\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    965\u001b[39m )\n\u001b[32m    966\u001b[39m exceptions = []\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1162\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1160\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1161\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1163\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1164\u001b[39m     )\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1423\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1416\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1417\u001b[39m             response,\n\u001b[32m   1418\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1419\u001b[39m             metadata=generation_info,\n\u001b[32m   1420\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1421\u001b[39m         )\n\u001b[32m   1422\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1423\u001b[39m         raw_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.with_raw_response.create(\n\u001b[32m   1424\u001b[39m             **payload\n\u001b[32m   1425\u001b[39m         )\n\u001b[32m   1426\u001b[39m         response = raw_response.parse()\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2585\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2539\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2540\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2541\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2582\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2583\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2584\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2585\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2587\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2588\u001b[39m             {\n\u001b[32m   2589\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2590\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2591\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2592\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2593\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2594\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2595\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2596\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2597\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2598\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2599\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2600\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2601\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2602\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2603\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2604\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2605\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2606\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2621\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2622\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2623\u001b[39m             },\n\u001b[32m   2624\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2625\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2627\u001b[39m         ),\n\u001b[32m   2628\u001b[39m         options=make_request_options(\n\u001b[32m   2629\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2630\u001b[39m         ),\n\u001b[32m   2631\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2632\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2633\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2634\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/openai/_base_client.py:1529\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1527\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1530\u001b[39m         request,\n\u001b[32m   1531\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1532\u001b[39m         **kwargs,\n\u001b[32m   1533\u001b[39m     )\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1535\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:237\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:180\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter05/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1263\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1258\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1261\u001b[39m ):\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "task = \"Write a strategic one-pager of building an AI startup\"\n",
    "\n",
    "# FIXME: 生成计划后，无法正确执行，反复报 'Sorry, need more steps to process this request.'。\n",
    "result = await graph.ainvoke({\"task\": task})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
