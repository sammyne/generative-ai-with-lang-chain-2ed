{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7e1ce8",
   "metadata": {},
   "source": [
    "# 05. Building Intelligent Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f19f6",
   "metadata": {},
   "source": [
    "## ÂÆâË£Ö‰æùËµñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba16328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=1.0 langchain-classic~=1.0 langchain-core~=1.0 langchain-openai~=1.0 langgraph~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601fb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 45ms\u001b[0m\u001b[0m                                           \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235a80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m46 packages\u001b[0m \u001b[2min 93ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/20] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m20 packages\u001b[0m \u001b[2min 83ms\u001b[0m\u001b[0m.0.0                           \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgreenlet\u001b[0m\u001b[2m==3.2.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==1.0.0a1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.44\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install 'langchain-community==1.0.0a1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        embeddings_model = os.getenv(\"OPENAI_EMBEDDINGS_MODEL\")\n",
    "        hf_pretrained_embeddings_model = os.getenv(\"HF_PRETRAINED_EMBEDDINGS_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.hf_pretrained_embeddings_model = (\n",
    "            hf_pretrained_embeddings_model\n",
    "            if hf_pretrained_embeddings_model\n",
    "            else \"Qwen/Qwen3-Embedding-8B\"\n",
    "        )\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # ÂèÇËÄÉÔºöhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # ÂèÇËÄÉÔºöhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI ÊñáÊ°£ÂèÇËÄÉÔºöhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6c196",
   "metadata": {},
   "source": [
    "## What is a tool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af38b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEARCH: current US president age'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "question = \"how old is the US president?\"\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Do not make any assumptions on recent events or things that can change.\"\n",
    "    \"Now, act to answer a user question:\\n{QUESTION}\"\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "(prompt_template | llm).invoke(question).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7305668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RESPONSE Berlin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1 = \"What is the capital of Germany?\"\n",
    "\n",
    "(prompt_template | llm).invoke(question1).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d87f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE: As of February 2025, the U.S. president is Donald Trump, who is 78 years old (born June 14, 1946).\n"
     ]
    }
   ],
   "source": [
    "query = \"age of current US president\"\n",
    "search_result = (\n",
    "    \"Donald Trump ‚Ä∫ Age 78 years June 14, 1946\\n\"\n",
    "    \"Donald Trump 45th and 47th U.S. President Donald John Trump is an American \"\n",
    "    \"politician, media personality, and businessman who has served as the 47th \"\n",
    "    \"president of the United States since January 20, 2025. A member of the \"\n",
    "    \"Republican Party, he previously served as the 45th president from 2017 to 2021. Wikipedia\"\n",
    ")\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Today is {date}.\"\n",
    "    \"Now, act to answer a user question and \"\n",
    "    \"take into account your previous actions:\\n\"\n",
    "    \"HUMAN: {question}\\n\"\n",
    "    \"AI: SEARCH: {query}\\n\"\n",
    "    \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "result = (prompt_template | llm).invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"query\": query,\n",
    "        \"search_result\": search_result,\n",
    "        \"date\": \"Feb 2025\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4cdcb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH: Donald Trump birth date\n"
     ]
    }
   ],
   "source": [
    "query = \"current US president\"\n",
    "search_result = \"Donald Trump 45th and 47th U.S.\"\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Today is {date}.\"\n",
    "    \"Now, act to answer a user question and \"\n",
    "    \"take into account your previous actions:\\n\"\n",
    "    \"HUMAN: {question}\\n\"\n",
    "    \"AI: SEARCH: {query}\\n\"\n",
    "    \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "result = (prompt_template | llm).invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"query\": query,\n",
    "        \"search_result\": search_result,\n",
    "        \"date\": \"Feb 2025\",\n",
    "    }\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18600905",
   "metadata": {},
   "source": [
    "### Tools in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0853679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'google_search',\n",
       "  'args': {'query': 'current US president age'},\n",
       "  'id': 'call_3790f052401b4e81a92eb737',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"google_search\",\n",
    "        \"description\": \"Returns about common facts, fresh events and news from Google Search engine based on a query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"search_query\",\n",
    "                    \"description\": \"Search query to be sent to the search engine\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "step1 = llm.invoke(question, tools=[search_tool])\n",
    "\n",
    "step1.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b936e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of now, the current U.S. president is Joe Biden, not Donald Trump. Joe Biden was born on November 20, 1942, which makes him 81 years old (as of 2024).\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage, ToolMessage\n",
    "\n",
    "tool_result = ToolMessage(\n",
    "    content=\"Donald Trump ‚Ä∫ Age 78 years June 14, 1946\\n\",\n",
    "    tool_call_id=step1.tool_calls[0][\"id\"],\n",
    ")\n",
    "step2 = llm.invoke(\n",
    "    [HumanMessage(content=question), step1, tool_result], tools=[search_tool]\n",
    ")\n",
    "assert len(step2.tool_calls) == 0\n",
    "\n",
    "print(step2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a69fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 292, 'total_tokens': 316, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-44cae3b4-8db1-4f7a-94bf-0b5fa426ce76', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--b7b908f9-8031-4a61-9fd9-29aaf4b038e8-0', tool_calls=[{'name': 'google_search', 'args': {'query': 'current US president age'}, 'id': 'call_92d1e0aae70942b79da26503', 'type': 'tool_call'}], usage_metadata={'input_tokens': 292, 'output_tokens': 24, 'total_tokens': 316, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind(tools=[search_tool])\n",
    "\n",
    "llm_with_tools.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835c720",
   "metadata": {},
   "source": [
    "### ReACT\n",
    "\n",
    "Give the LLM access to tools as a way to interact with an external environment, and let the LLM run in a loop:\n",
    "- **Reason**: Generate a text output with observations about the current situation and a plan to solve the task.\n",
    "- **Act**: Take an action based on the reasoning above (interact with the environment by calling a tool, or respond to the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83cb1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def mocked_google_search(query: str) -> str:\n",
    "    print(f\"CALLED GOOGLE_SEARCH with query={query}\")\n",
    "    return \"Donald Trump is a president of USA and he's 78 years old\"\n",
    "\n",
    "\n",
    "def mocked_calculator(expression: str) -> float:\n",
    "    print(f\"CALLED CALCULATOR with expression={expression}\")\n",
    "    if \"sqrt\" in expression:\n",
    "        return math.sqrt(78 * 132)\n",
    "    return 78 * 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28097b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "calculator_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Computes mathematical expressions\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"expression\",\n",
    "                    \"description\": \"A mathematical expression to be evaluated by a calculator\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"google_search\",\n",
    "        \"description\": \"Returns about common facts, fresh events and news from Google Search engine based on a query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"search_query\",\n",
    "                    \"description\": \"Search query to be sent to the search engine\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "system_prompt = (\n",
    "    \"Always use a calculator for mathematical computations, and use Google Search \"\n",
    "    \"for information about common facts, fresh events and news. Do not assume anything, keep in \"\n",
    "    \"mind that things are changing and always \"\n",
    "    \"check yourself with external sources if possible.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a659b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Config().new_openai_like()\n",
    "\n",
    "llm_with_tools = prompt | llm.bind_tools([search_tool, calculator_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c990dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import ToolMessage\n",
    "from langgraph.graph import MessagesState, START, END\n",
    "\n",
    "\n",
    "def invoke_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "def call_tools(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_calls = last_message.tool_calls\n",
    "\n",
    "    new_messages = []\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call[\"name\"] == \"google_search\":\n",
    "            tool_result = mocked_google_search(**tool_call[\"args\"])\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "        elif tool_call[\"name\"] == \"calculator\":\n",
    "            tool_result = mocked_calculator(**tool_call[\"args\"])\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Tool {tool_call['name']} is not defined!\")\n",
    "    return {\"messages\": new_messages}\n",
    "\n",
    "\n",
    "def should_run_tools(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"call_tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2febead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALLED GOOGLE_SEARCH with query=current US president age\n",
      "CALLED CALCULATOR with expression=sqrt(78 * 132)\n",
      "The square root of the current U.S. president's age (78 years) multiplied by 132 is approximately **101.47**.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"invoke_llm\", invoke_llm)\n",
    "builder.add_node(\"call_tools\", call_tools)\n",
    "\n",
    "builder.add_edge(START, \"invoke_llm\")\n",
    "builder.add_conditional_edges(\"invoke_llm\", should_run_tools)\n",
    "builder.add_edge(\"call_tools\", \"invoke_llm\")\n",
    "graph = builder.compile()\n",
    "\n",
    "question = \"What is a square root of the current US president‚Äôs age multiplied by 132?\"\n",
    "\n",
    "result = graph.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9d4c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is a square root of the current US president‚Äôs age multiplied by 132?', additional_kwargs={}, response_metadata={}, id='4af32121-83e0-4422-92a3-e51c3b10cd41'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 422, 'total_tokens': 446, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-6cd6be9d-54e4-4588-a207-5c6131ad51ce', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--99c3342a-d9b4-4004-a4c2-65ae79453d2e-0', tool_calls=[{'name': 'google_search', 'args': {'query': 'current US president age'}, 'id': 'call_016171fa926d4a0592970b66', 'type': 'tool_call'}], usage_metadata={'input_tokens': 422, 'output_tokens': 24, 'total_tokens': 446, 'input_token_details': {}, 'output_token_details': {}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm, tools=[search_tool, calculator_tool], system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7f8bf",
   "metadata": {},
   "source": [
    "## Defining tools\n",
    "\n",
    "A LangChain tool has three essential components:\n",
    "- `Name`: A unique identifier for the tool\n",
    "- `Description`: Text that helps the LLM understand when and how to use the tool\n",
    "- `Payload schema`: A structured definition of the inputs the tool accepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d7c66",
   "metadata": {},
   "source": [
    "### Built-in LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b144a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m17 packages\u001b[0m \u001b[2min 90ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/9] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m9 packages\u001b[0m \u001b[2min 29ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbrotli\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mddgs\u001b[0m\u001b[2m==9.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh2\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhpack\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhyperframe\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprimp\u001b[0m\u001b[2m==0.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msocksio\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install ddgs~=9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac4ce891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool's name = duckduckgo_search\n",
      "Tool's name = A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "Tool's arg schema = <class 'langchain_community.tools.ddg_search.tool.DDGInput'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun(api_wrapper_kwargs={\"backend\": \"api\"})\n",
    "print(f\"Tool's name = {search.name}\")\n",
    "print(f\"Tool's name = {search.description}\")\n",
    "print(f\"Tool's arg schema = {search.args_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc197587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': FieldInfo(annotation=str, required=True, description='search query to look up')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.ddg_search.tool import DDGInput\n",
    "\n",
    "\n",
    "DDGInput.model_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09c332db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Munich , Bavaria, Germany Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days. 4 days ago ¬∑ Get the latest hourly weather updates for Munich tomorrow . Detailed forecast including temperature, wind, rain, snow, and UV index. Stay informed about tomorrow 's weather conditions in Munich . 6 days ago ¬∑ Munich , Germany - Detailed weather forecast for tomorrow . Hourly forecast for tomorrow - including weather conditions, temperature, pressure, humidity, precipitation, dewpoint, wind, visibility, and UV index data. Oct 25, 2025 ¬∑ Detailed weather forecast ‚ö° in Munich , Bavaria today, tomorrow and 7 days. Wind, precipitation, üå°Ô∏è air temperature, clouds and atmospheric pressure - World-Weather.info Oct 24, 2025 ¬∑ Latest weather forecast for Munich for tomorrow 's, hourly weather forecast, including tomorrow 's temperatures in Munich , wind, rain and more. Oct 3, 2025 ¬∑ Today's and tonight's professional weather forecast for Munich . Precipitation radar, HD satellite images, and current weather warnings, hourly temperature, chance of rain, and sunshine hours. 2 days ago ¬∑ Get the detailed weather forecast for Munich , Bavaria, Germany. Access daily, 10 day and 16 day views, maps, meteograms and timelines for accurate predicitions.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the weather in Munich like tomorrow?\"\n",
    "search_input = DDGInput(query=query)\n",
    "search.invoke(search_input.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d7e9484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 295, 'total_tokens': 321, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-41d3c644-1a17-4f7f-8e59-1e9511952090', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--d0cba704-9624-4846-823e-74afb4fead6a-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'weather in Munich tomorrow'}, 'id': 'call_b444671c69a64c41a150b83c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 295, 'output_tokens': 26, 'total_tokens': 321, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm.invoke(query, tools=[search]) ‰ºöÊä• search ‰∏çÊòØÂêàÊ≥ïÁöÑ json ÂØπË±°„ÄÇ\n",
    "llm.bind_tools([search]).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de91aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.bind_tools([search]).invoke(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Always use a duckduckgo_search tool for queries that require a fresh information\",\n",
    "        ),\n",
    "        (\"user\", \"How much is 2+2?\"),\n",
    "    ]\n",
    ")\n",
    "assert not result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dbb9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[search],\n",
    "    system_prompt=\"Always use a duckduckgo_search tool for queries that require a fresh information\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb77ef72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b5fc18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: duckduckgo_search\n",
      "\n",
      "Munich , Bavaria, Germany Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days. Be prepared with the most accurate 10-day forecast for M√∫nich , Bavaria, Germany with highs, lows, chance of precipitation from The Weather Channel and Weather.com 4 days ago ¬∑ Get the latest hourly weather updates for Munich tomorrow . Detailed forecast including temperature, wind, rain, snow, and UV index. Stay informed about tomorrow 's weather conditions in Munich . 6 days ago ¬∑ Munich , Germany - Detailed weather forecast for tomorrow . Hourly forecast for tomorrow - including weather conditions, temperature, pressure, humidity, precipitation, dewpoint, wind, visibility, and UV index data. Oct 24, 2025 ¬∑ Latest weather forecast for Munich for tomorrow 's, hourly weather forecast, including tomorrow 's temperatures in Munich , wind, rain and more. Oct 25, 2025 ¬∑ Detailed weather forecast ‚ö° in Munich , Bavaria today, tomorrow and 7 days. Wind, precipitation, üå°Ô∏è air temperature, clouds and atmospheric pressure - World-Weather.info. Yesterday's weather Passing clouds. 46 / 37 ¬∞F Humidity: 73%. Wind: 17 mph ‚Üë from Southwest More weather last week\n"
     ]
    }
   ],
   "source": [
    "for event in agent.stream({\"messages\": [(\"user\", query)]}):\n",
    "    messages = event.get(\"agent\", event.get(\"tools\", {})).get(\"messages\", [])\n",
    "    for m in messages:\n",
    "        m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67514217",
   "metadata": {},
   "source": [
    "Other types of tools\n",
    "- Tools that enhance the LLM‚Äôs knowledge. e.g. Knowledge bases: Wikipedia and Wikidata\n",
    "- Tools that enhance your productivity. e.g. GmailToolkit\n",
    "- Tools that give an LLM access to a code interpreter. e.g. Code execution: Python REPL and Bash\n",
    "- Tools that give an LLM access to databases by writing and executing SQL code. e.g. SQLDatabase\n",
    "- Tools for using other AI systems or automation. e.g. Hugging Face Hub\n",
    "\n",
    "When integrating such tools with LangChain, consider these key aspects:\n",
    "1. Authentication: Secure access to the external system\n",
    "1. Payload schema: Define proper data structures for input/output\n",
    "1. Error handling: Plan for failures and edge cases\n",
    "1. Safety considerations: For example, when developing a SQL-to-text agent, restrict access to read-only operations to prevent unintended modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07d99fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests_get\n",
      "requests_post\n",
      "requests_patch\n",
      "requests_put\n",
      "requests_delete\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n",
    "\n",
    "toolkit = RequestsToolkit(\n",
    "    requests_wrapper=TextRequestsWrapper(headers={}),\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "for tool in toolkit.get_tools():\n",
    "    print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b604141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_2478089/2022118673.py:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  pattern: '^\\d{4}-\\d{2}-\\d{2}$' # YYYY-MM-DD format\n"
     ]
    }
   ],
   "source": [
    "api_spec = \"\"\"\n",
    "openapi: 3.0.0\n",
    "info:\n",
    "  title: Frankfurter Currency Exchange API\n",
    "  version: v1\n",
    "  description: API for retrieving currency exchange rates. Pay attention to the base currency and change it if needed.\n",
    "\n",
    "servers:\n",
    "  - url: https://api.frankfurter.dev/v1\n",
    "\n",
    "paths:\n",
    "  /v1/{date}:\n",
    "    get:\n",
    "      summary: Get exchange rates for a specific date.\n",
    "      parameters:\n",
    "        - in: path\n",
    "          name: date\n",
    "          schema:\n",
    "            type: string\n",
    "            pattern: '^\\d{4}-\\d{2}-\\d{2}$' # YYYY-MM-DD format\n",
    "          required: true\n",
    "          description: The date for which to retrieve exchange rates.  Use YYYY-MM-DD format.  Example: 2009-01-04\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: GBP,USD,EUR\n",
    "\n",
    "  /v1/latest:\n",
    "    get:\n",
    "      summary: Get the latest exchange rates.\n",
    "      parameters:\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: CHF,GBP\n",
    "        - in: query\n",
    "          name: base\n",
    "          schema:\n",
    "            type: string\n",
    "          description: The base currency for the exchange rates. If not provided, EUR is used as a base currency. Example: USD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "578335d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the swiss franc to US dollar exchange rate?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  requests_get (call_846b8d325ed741f69d31f5e0)\n",
      " Call ID: call_846b8d325ed741f69d31f5e0\n",
      "  Args:\n",
      "    url: https://api.frankfurter.dev/v1/latest?base=CHF&symbols=USD\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: requests_get\n",
      "\n",
      "{\"amount\":1.0,\"base\":\"CHF\",\"date\":\"2025-10-31\",\"rates\":{\"USD\":1.2441}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The Swiss Franc (CHF) to US Dollar (USD) exchange rate is **1.2441** as of October 31, 2025. This means 1 CHF is equivalent to 1.2441 USD.\n"
     ]
    }
   ],
   "source": [
    "system_message = (\n",
    "    \"You're given the API spec:\\n{api_spec}\\n\"\n",
    "    \"Use the API to answer users' queries if possible. \"\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    llm, toolkit.get_tools(), system_prompt=system_message.format(api_spec=api_spec)\n",
    ")\n",
    "\n",
    "query = \"What is the swiss franc to US dollar exchange rate?\"\n",
    "\n",
    "events = agent.stream(\n",
    "    {\"messages\": [(\"user\", query)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee703b01",
   "metadata": {},
   "source": [
    "### Custom tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6f629",
   "metadata": {},
   "source": [
    "#### Wrapping a Python function as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d73b701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 52ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumexpr\u001b[0m\u001b[2m==2.14.1\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install numexpr~=2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "697ee111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4, dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numexpr as ne\n",
    "import math\n",
    "\n",
    "math_constants = ({\"pi\": math.pi, \"i\": 1j, \"e\": math.exp},)\n",
    "ne.evaluate((\"2+2\"), local_dict=math_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0507ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
    "\n",
    "    Always add * to operations, examples:\n",
    "      73i -> 73*i\n",
    "      7pi**2 -> 7*pi**2\n",
    "    \"\"\"\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ca9edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name: calculator\n",
      "Tool description: Calculates a single mathematical expression, incl. complex numbers.\n",
      "\n",
      "    Always add * to operations, examples:\n",
      "      73i -> 73*i\n",
      "      7pi**2 -> 7*pi**2\n",
      "Tool schema: {'description': 'Calculates a single mathematical expression, incl. complex numbers.\\n\\nAlways add * to operations, examples:\\n  73i -> 73*i\\n  7pi**2 -> 7*pi**2', 'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "assert isinstance(calculator, BaseTool)\n",
    "\n",
    "print(f\"Tool name: {calculator.name}\")\n",
    "print(f\"Tool description: {calculator.description}\")\n",
    "print(f\"Tool schema: {calculator.args_schema.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5491aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How much is 2+3i squared?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculator (call_8324b94d7cb14d0599a53217)\n",
      " Call ID: call_8324b94d7cb14d0599a53217\n",
      "  Args:\n",
      "    expression: (2+3*i)**2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculator\n",
      "\n",
      "(-5+12j)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The square of $ 2 + 3i $ is $ -5 + 12i $.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "query = \"How much is 2+3i squared?\"\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "agent = create_agent(llm, [calculator])\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", query)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5ef452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is a square root of the current US president's age multiplied by 132?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To calculate the square root of the current U.S. president's age multiplied by 132, I first need to determine the current president's age. Since it is 2025 and recent U.S. presidential elections have occurred, I will find out who the current president is and their age.\n",
      "Tool Calls:\n",
      "  duckduckgo_search (call_685407e558684105806589ba)\n",
      " Call ID: call_685407e558684105806589ba\n",
      "  Args:\n",
      "    query: current US president 2025 age\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: duckduckgo_search\n",
      "\n",
      "9 hours ago - It adds a work requirement to Medicaid of 20-hour a week with some exceptions such as disability or a dependent child under age 15. It also increases money for border security and other Republican priorities. Over the next 10 years, OBBBA is expected to add a total of $3 trillion to the national ... 1 day ago - Joseph Robinette Biden Jr. ( born November 20, 1942 ) is an American politician who served as the 46th president of the United States from 2021 to 2025. A member of the Democratic Party, he represented Delaware in the United States Senate from ... 1 week ago - In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 47 presidencies spanning 60 four-year terms. Donald Trump is the 47th and current president ... 6 hours ago - Trump won the election in November 2024 with 312 electoral votes to incumbent vice president Kamala Harris's 226. He also won the popular vote with 49.8% to Harris's 48.3%. His victory in 2024 was part of a global backlash against incumbent parties, in large part due to the 2021‚Äì2023 inflation surge. Several outlets described his reelection as an extraordinary comeback. Trump began his second term upon his inauguration on January 20, 2025. 3 days ago - How old is Trump? Donald Trump is 79 years old in 2025. Explore his age, timeline, comparisons, public activity, and why it matters in U.S. politics.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current U.S. president in 2025 is Donald Trump, who is 79 years old. Now, I will calculate the square root of his age multiplied by 132.\n",
      "Tool Calls:\n",
      "  calculator (call_eff3afc0cf1e40589c532daf)\n",
      " Call ID: call_eff3afc0cf1e40589c532daf\n",
      "  Args:\n",
      "    expression: sqrt(79 * 132)\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculator\n",
      "\n",
      "102.11757928975794\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The square root of the current U.S. president's age (79) multiplied by 132 is approximately **102.12**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "question = \"What is a square root of the current US president's age multiplied by 132?\"\n",
    "\n",
    "system_hint = \"Think step-by-step. Always use search to get the fresh information about events or public facts that can change over time. Now is 2025 and remember president elections in the US recently happened.\"\n",
    "\n",
    "agent = create_agent(llm, [calculator, search], system_prompt=system_hint)\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", question)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729a9ec",
   "metadata": {},
   "source": [
    "#### Creating a tool from a Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a0d742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import convert_runnable_to_tool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18513a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78a9251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 309, 'total_tokens': 337, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-0b7ce397-6473-461b-9c16-300f57259e58', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--1f4faffe-dada-4443-983d-fc8388f45031-0', tool_calls=[{'name': 'calculator', 'args': {'__arg1': '(2+3*i)**2'}, 'id': 'call_9e63ba38fab54505abf30c59', 'type': 'tool_call'}], usage_metadata={'input_tokens': 309, 'output_tokens': 28, 'total_tokens': 337, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Âéü‰π¶ÁöÑÈîôËØØË∞ÉÁî®ÊñπÂºèÔºö\n",
    "#   llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n",
    "# ÂÖ∑‰ΩìÈîôËØØÊèêÁ§∫ÁâáÊÆµ‰∏∫Ôºö\n",
    "#  Unable to serialize unknown type: <class 'method'>\n",
    "\n",
    "llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "745d8742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name: calculator\n",
      "Tool description: Calculates a single mathematical expression, incl. complex numbers.'\n",
      "Always add * to operations, examples:\n",
      "73i -> 73*i\n",
      "7pi**2 -> 7*pi**2\n",
      "Args schema: {'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import convert_runnable_to_tool\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    expression: str = Field(description=\"Mathematical expression to be evaluated\")\n",
    "\n",
    "\n",
    "def calculator(state: CalculatorArgs, config: RunnableConfig) -> str:\n",
    "    expression = state[\"expression\"]\n",
    "    math_constants = config[\"configurable\"].get(\"math_constants\", {})\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")\n",
    "\n",
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    args_schema=CalculatorArgs,\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")\n",
    "\n",
    "assert isinstance(calculator_tool, BaseTool)\n",
    "\n",
    "print(f\"Tool name: {calculator_tool.name}\")\n",
    "print(f\"Tool description: {calculator_tool.description}\")\n",
    "print(f\"Args schema: {calculator_tool.args_schema.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cb5c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'calculator', 'args': {'expression': '(2+3*i)**2'}, 'id': 'call_3f56e586793b49e19c2cfb6a', 'type': 'tool_call'}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "config = {\"configurable\": {\"math_constants\": math_constants}}\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# ÈîôËØØË∞ÉÁî®ÊñπÂºèÔºötool_call = llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n",
    "tool_call = (\n",
    "    llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\").tool_calls[0]\n",
    ")\n",
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8b53efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(-5+12j)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculator_tool.invoke(tool_call[\"args\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666c3d5",
   "metadata": {},
   "source": [
    "#### Subclass StructuredTool or BaseTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cea617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'calculator',\n",
       " 'args': {'expression': '(2+3j)**2'},\n",
       " 'id': 'call_0453f9588b5c4b90adbb1266',\n",
       " 'type': 'tool_call'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_tool = StructuredTool.from_function(\n",
    "    name=\"calculator\",\n",
    "    description=(\"Calculates a single mathematical expression, incl. complex numbers.\"),\n",
    "    func=calculator,\n",
    "    args_schema=CalculatorArgs,\n",
    ")\n",
    "\n",
    "llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\").tool_calls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4974cd1",
   "metadata": {},
   "source": [
    "### Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0b3686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How much is (2+3i)^2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculator (call_13f279828fb944be9bb96ff2)\n",
      " Call ID: call_13f279828fb944be9bb96ff2\n",
      "  Args:\n",
      "    expression: (2+3i)^2\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (<expr>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3699\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[50]\u001b[39m\u001b[92m, line 14\u001b[39m\n    for event in agent.stream(\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2679\u001b[39m in \u001b[95mstream\u001b[39m\n    for _ in runner.tick(\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001b[39m in \u001b[95mtick\u001b[39m\n    run_with_retry(\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m in \u001b[95mrun_with_retry\u001b[39m\n    return task.proc.invoke(task.input, config)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m in \u001b[95minvoke\u001b[39m\n    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m in \u001b[95minvoke\u001b[39m\n    ret = self.func(*args, **kwargs)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:716\u001b[39m in \u001b[95m_func\u001b[39m\n    outputs = list(\n",
      "  File \u001b[92m/usr/local/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m in \u001b[95mresult_iterator\u001b[39m\n    yield _result_or_cancel(fs.pop())\n",
      "  File \u001b[92m/usr/local/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m in \u001b[95m_result_or_cancel\u001b[39m\n    return fut.result(timeout)\n",
      "  File \u001b[92m/usr/local/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m in \u001b[95mresult\u001b[39m\n    return self.__get_result()\n",
      "  File \u001b[92m/usr/local/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m in \u001b[95m__get_result\u001b[39m\n    raise self._exception\n",
      "  File \u001b[92m/usr/local/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m in \u001b[95mrun\u001b[39m\n    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:546\u001b[39m in \u001b[95m_wrapped_fn\u001b[39m\n    return contexts.pop().run(fn, *args)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:931\u001b[39m in \u001b[95m_run_one\u001b[39m\n    return self._execute_tool_sync(tool_request, input_type, config)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:880\u001b[39m in \u001b[95m_execute_tool_sync\u001b[39m\n    content = _handle_tool_error(e, flag=self._handle_tool_errors)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:401\u001b[39m in \u001b[95m_handle_tool_error\u001b[39m\n    content = flag(e)  # type: ignore [assignment, call-arg]\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:358\u001b[39m in \u001b[95m_default_handle_tool_errors\u001b[39m\n    raise e\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:833\u001b[39m in \u001b[95m_execute_tool_sync\u001b[39m\n    response = tool.invoke(call_args, config)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:591\u001b[39m in \u001b[95minvoke\u001b[39m\n    return self.run(tool_input, **kwargs)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:897\u001b[39m in \u001b[95mrun\u001b[39m\n    raise error_to_raise\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:866\u001b[39m in \u001b[95mrun\u001b[39m\n    response = context.run(self._run, *tool_args, **tool_kwargs)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tools/structured.py:90\u001b[39m in \u001b[95m_run\u001b[39m\n    return self.func(*args, **kwargs)\n",
      "  Cell \u001b[92mIn[50]\u001b[39m\u001b[92m, line 7\u001b[39m in \u001b[95mcalculator\u001b[39m\n    return str(ne.evaluate(expression.strip(), local_dict={}))\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/numexpr/necompiler.py:991\u001b[39m in \u001b[95mevaluate\u001b[39m\n    raise e\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/numexpr/necompiler.py:894\u001b[39m in \u001b[95mvalidate\u001b[39m\n    _names_cache.c[expr_key] = getExprNames(ex, context, sanitize=sanitize)\n",
      "  File \u001b[92m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/numexpr/necompiler.py:737\u001b[39m in \u001b[95mgetExprNames\u001b[39m\n    ex = stringToExpression(text, {}, context, sanitize)\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/numexpr/necompiler.py:307\u001b[39m\u001b[36m in \u001b[39m\u001b[35mstringToExpression\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mc = compile(s, '<expr>', 'eval', flags)\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m<expr>:1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m(2+3i)^2\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\nDuring task with name 'tools' and id '7003c6e8-584a-9762-5275-42da2dcdfd97'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\"\"\"\n",
    "    return str(ne.evaluate(expression.strip(), local_dict={}))\n",
    "\n",
    "\n",
    "calculator_tool = StructuredTool.from_function(func=calculator, handle_tool_error=True)\n",
    "\n",
    "agent = create_agent(llm, [calculator_tool])\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"How much is (2+3i)^2\")]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfaaf4",
   "metadata": {},
   "source": [
    "## Advanced tool-calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e02899",
   "metadata": {},
   "source": [
    "## Incorporating tools into workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dea58",
   "metadata": {},
   "source": [
    "### Controlled generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ee2b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99d90f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Prepare a step-by-step plan to solve the given task. \"\n",
    "    \"{format_instructions}.\"\n",
    "    \"TASK:\\n{task}\\n\"\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "\n",
    "result = (prompt | llm.with_structured_output(Plan)).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6d62ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of steps: 11\n",
      "Research the market to identify popular generative AI topics and gaps in existing books on Amazon.\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(result, Plan)\n",
    "\n",
    "print(f\"Amount of steps: {len(result.steps)}\")\n",
    "\n",
    "for step in result.steps:\n",
    "    print(step)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2a441",
   "metadata": {},
   "source": [
    "#### Controlled generation provided by the vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8aa16e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Prepare a step-by-step plan to solve the given task.\\n\"\n",
    "    \"Output the planned steps as a JSON array.\\n\"\n",
    "    \"TASK:\\n{task}\\n\"\n",
    ")\n",
    "\n",
    "plan_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"step\": {\"type\": \"STRING\"},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "\n",
    "result = (\n",
    "    prompt | llm.with_structured_output(schema=plan_schema, method=\"json_mode\")\n",
    ").invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad7f5006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of steps: 14\n",
      "Define your target audience (e.g., beginners, professionals, entrepreneurs) and clarify their pain points related to generative AI.\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(result, list)\n",
    "print(f\"Amount of steps: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27c110c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import TypeAdapter\n",
    "\n",
    "\n",
    "class ReviewTone(str, Enum):\n",
    "    positive = \"positive\"\n",
    "    negative = \"negative\"\n",
    "    neutral = \"neutral\"\n",
    "\n",
    "\n",
    "model_json_schema = TypeAdapter(ReviewTone).json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "363ab585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"enum\": [\n",
      "    \"positive\",\n",
      "    \"negative\",\n",
      "    \"neutral\"\n",
      "  ],\n",
      "  \"title\": \"ReviewTone\",\n",
      "  \"type\": \"string\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(model_json_schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cdbc474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tone': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Classify the tone of the following customer's review:\"\n",
    "    \"\\n{review}\\n\"\n",
    "    \"Output in JSON.\"\n",
    ")\n",
    "\n",
    "review = \"I like this movie!\"\n",
    "llm_enum = (\n",
    "    Config()\n",
    "    .new_openai_like()\n",
    "    .with_structured_output(schema=model_json_schema, method=\"json_mode\")\n",
    ")\n",
    "result = (prompt | llm_enum).invoke(review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b6606",
   "metadata": {},
   "source": [
    "### ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3e4b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numexpr as ne\n",
    "from langchain.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
    "\n",
    "    Always add * to operations, examples:\n",
    "      73i -> 73*i\n",
    "      7pi**2 -> 7*pi**2\n",
    "    \"\"\"\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "llm_with_tools = Config().new_openai_like().bind_tools([search, calculator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "532d7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "\n",
    "def invoke_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"invoke_llm\", invoke_llm)\n",
    "builder.add_node(\"tools\", ToolNode([search, calculator]))\n",
    "\n",
    "builder.add_edge(START, \"invoke_llm\")\n",
    "builder.add_conditional_edges(\"invoke_llm\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"invoke_llm\")\n",
    "# builder.add_edge(\"tools\", END)\n",
    "builder.add_edge(\"invoke_llm\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "105cf623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAQAElEQVR4nOydB3wUxR7HZ/daeu+NJAQChA6BJyI1CD6KyMMHBJAixUSKAoKF9mgCoiKCICIiKiC9CIIgTYihBIHQAgECIZWEtEu9u93339vkcoG7kITs3u7dfD/5XPZ2Z/f2Zn83M///zPxHStM0wmBMjRRhMAIACxEjCLAQMYIACxEjCLAQMYIACxEjCLAQnyYzWXXt71xltrqoQK3WUJoyhAga0QRNIAIxG4QEUtE0xb6jSZKgNdoNgqBoJh1BIppiLsVuEBJtOqr8OuVHSdhmksApJFyYTayhCQmTkk3DXEv7qcxb8LJRBHuH8Jb9IN09SxWETEFa20l8gq3b9XRCIoTAfkSWlNtlp3Zn5DwuhfwgJYStg0wihQ2kLqUIKaLVTBpGJVqtMJLUaJVEEKyAmKOkNjNhk0SIYqVJ0hRNwum09kQ4i9Ymo2gmDVyFYi6BtJJEWrWxp7Bp4NqwB85CJPNCacqfFNwenEFRlQ8OVEhRSFVClxZr1GpaqiB9g637jfNC4gELEWU+1Bz4LhkeoZO7vEUnxxavOCBRQ6ET27Pu31QWF2o8A6wGT/FFYsDShbhjZUpWSolfqF3/cZ7IvMhKUx/c8KioQNP9Tc8m4XZI2Fi0EDfMvi+TkqPmN0Dmy43YgtN7Mv0a2fQb540EjOUKcePcJJ+GNn1GeSALAH5y4a+6tOriiISKhQrx2w/vNmrl0GOYO7IYvvskydPfasA7ArVgSGR5/DAvKSDUzqJUCIxfHJjxsOiv3VlIkFicEPevTweHy2tjzM00qQnjFwTHx+QhQWJhQtSg5ATl6HmByDKRIt+G1hvn3kfCw7KE+PPyZM8AG2TBvB7lA/7F23FKJDAsS4h5WaWDp/ggywa6Af/a9xgJDAsS4oH1adb2MkQgPvnwww/37duHak+vXr1SUlIQB/Qd71us1CCBYUFCTLtfEtDYGvHLjRs3UO1JS0vLyclB3CCXI4UV+ec2YZnPFiREVRnVPsIVccPZs2cnTpzYuXPngQMHzps3LyuLeczt27dPTU1duHBht27d4K1SqVy3bt2oUaPYZF9++WVJSQl7es+ePbdu3Tp+/Hg45dSpU/3794edr7/++vTp0xEHOHkq0u4VISFhKUK8e7WYlCAnDwnigFu3bk2dOjU8PHznzp0zZ868ffv2/PnzkVad8DpnzpyTJ0/CxrZt2zZt2jRy5MiVK1dC+qNHj65fv569gkwm27NnT2ho6Jo1a15++WVIADuhTv/8888RB4BnG0wWJCQsZTxi2v0iiZSr5uHly5etrKzGjh1LkqSXl1ezZs0SExOfTTZixAgo+YKCgti3V65ciYmJmTJlCtKOJnN0dJwxYwbiBc8AxY1YCgkJSxEiNM8Jkishtm7dGirZ9957r2PHjl26dPH394ca9tlkUOz9/fffUHFDkalWMyMcXVxcdEdBvogvXNzl+sMZhYClVM0UhQjO7OUmTZqsWrXK3d3966+/fuONN6Kjo6G0ezYZHIW6GBLs3bv34sWLY8aM0T8qByOCN6QSxLP74HlYihCtbEmNmsMyoFOnTtAWPHDgALQO8/LyoHRkyzwdNE3v2rVryJAhIESovmFPQUEBMhG5mSUc/i7rhKUI0cvfSjfUvt6Ji4uD1h5sQKHYr18/MHVBZOCC0U+jUqmKi4s9PMpHnZWVlZ0+fRqZiMfJpVIZFqIpCA23gxKxrIgTLUJFDMby7t27wfl37do1sI5Bkd7e3gqFApQXGxsLFTHYMYGBgfv373/06FFubu6CBQugZZmfn19YWPjsBSElvIJZDVdDHJCWVCK3EtajtyA/IkkSMYc48eKCOQwV7ooVK6A7ZMKECba2ttAWlEoZQxBM6QsXLkAZCcXhkiVLwLgePHgwOBE7dOgwadIkeBsREQG+xqcu6OfnB65EcDpCsxJxQE5mqaefFRISFjQwdsfKR4X56tFzA5HFs2ZG4ti5Da0dBFQ7W1CJ2HOoR0GOClk8h35IA5eqoFSILGqCvYuX3MpGsveblIHRhmdYajQacDgbPAS2BXgBCUOWZnBw8MaNGxE3bNJi8JCdnR30GRo8FBYWBj00yAgPbha16eaMBIZlzVlJvVu6e03ypC9CjCZ4prnGAo8cHrzBQ9AW1NnC9U6BFoOHwIUOTUyDh+A3A9aSwUN/bMm8f1U5cWkwEhgWN3lq6/JkjYYe8VEAskhWT0scFB3gE8Kj87xmWNyclWEz/ZW5qthDT5DlsXFekn8jGwGqEFnmLL53ljWMO5GT/9iyqoItyx7JFZLXowQ6QN1yJ9ivmXE3YohXaLgtsgB+XPjQ1Ufe723hhmWy6JAj30y/6xNsPfBdM5/FAjWylbUk8kN/JGAsPQgTPKSyEk3HPq5tuosyrGD17P46BXrzGrex6zVC6PO4cVg6dHZ/9pXTOaSUgIZ8n5HeEiE25WvHvfii80eyn6SX2thLR88ORJwMS69nsBDLObnr8e2LBaUlGqmMCb1q5yizsZOSUkpVVpk/5TE2daFgKyLDsoeYWJrsaNOK0Ju6lOVoA3iSTLxXxjVefilt8E/YgN4OdqCa3qcQ2tC0VfdQ2gemDfqp2w++drWaKM5XKwvUJdohwPbO0i6D3P15nyxWZ7AQnybmQPajO0VFSkpdRlEU0h/FyEZwReUyKA9FXHGMCTJckZd05bBTfVFqA8Zqgx5rhViepjwxIUG0psoZbD8Oc07l52o/VXsVJpRsxQ3I5AQpIRTWpIOLrHEb+1DBR0N8FixEvpk8eXJkZORLL72EMHrgYO58o1ar2RFiGH1wjvANFqJBcI7wDRaiQXCO8I1KpZLJZAhTFSxEvsElokFwjvANFqJBcI7wDRaiQXCO8A0IEbcRnwULkW9wiWgQnCN8g4VoEJwjfIOFaBCcI3yDhWgQnCN8Aw5tLMRnwTnCKzRNUxQlkYhhqCq/YCHyCq6XjYEzhVewEI2BM4VX8IgHY2Ah8gouEY2BM4VXsBCNgTOFV7AQjYEzhVewEI2BM4VXsLFiDCxEXsElojFwpvCNsViuFg4WIq9A5156ejrCPAMWIq9AvfzU0mgYFixEXsFCNAYWIq9gIRoDC5FXsBCNgYXIK1iIxsBC5BUsRGNgIfIKFqIxsBB5BQvRGFiIvAJC1Gg0CPMMlrjylGmBzhWsxWfBQuQbXDsbBAuRb7AQDYLbiHyDhWgQLES+wUI0CBYi32AhGgQLkW+wEA2CV57iidatW5NkuWkIeQ7b8NqvX78FCxYgDLaaeaNly5bwSmoBVyJBEN7e3iNGjEAYLViIPPHWW2/Z2trq72nVqlXjxo0RRgsWIk9EREToy87V1XXYsGEIUwEWIn+MHj3awcGB3W7SpEmLFi0QpgIsRP545ZVXQkNDYcPR0XH48OEIo4eZW83nf8/NySwpKytfRF63JjcpQZRGu5h8xfLysM2sx01V5IZ2tXkCXmltFumt/00SBFU100iSWTacXYiefUtRNHNBujx72aXmYU9Obm58/DU7W7u2bVszH61dDRyVp6lY6167TH3lbdBIuzo4rfdx2n1U5Q0Q2s9i1henyhMw//W+GnwIVTHQAiwljabyatY2sqCWdg1bmHite7MV4qmd2TfP55FSAmShKq14JhUPlJDQtIbQl5f2kVeuSM+uUQ8PmGLkWXUhevZcPbQ60DtXK6nKc/Wupg1dzEiG0W65ECtO1N2b3k1pF7rXS2Po4yo+kdal1v5+kO7BMm+JynsmJPCbqbx5hYIsU1EyheTteQ2Q6UIqm6cQ447mxh3P6f2Wr4uPHGFqwIXfnyRcyov6NMhUWjRDIV48nP/P6eyhs4IQpjbciSu6eDRjwqemyTczNFaunM0JDHNEmFrSqJ2NVEoc25KFTIEZ9jWXlqibd3ZGmNpj7yrPeFiETIEZCpHS0NY2BMLUHrCkiotMM43BDKtmaPTiKSF1A9w6tIkGBuFhYBhBgIWIEQRYiJhKwPVNmKixhoWIqYRg+nxM41c2TyFim7lugMOBUpsm88y0RMTTH8SGeQqRxkVinSAI5s8k4DYiphJab8wOz5ilEHGBWEewsVK/ELiJWDdoZrwkNlbqD1wiig48ZwXt2r2tZ68OqL65dy+xe8/2V6/+g+oD9mrx8Zdhe/7/Zs34IBpxAHZo1zO1qpqbNW0+csQ4hNHOg9GfCsMn2GpGTZs2hz+EMSm4aq5SNQ8cFLFv/87NP22APf0GdP3fgg+zs5kRy5Onvj1z1iT9sz765L3oSaPZbUg/fOTA3q91Gjlq0OdfLKYoA6UKpOnz75dv3roO20+eZC9a/MnQyH7wcYs/nZOc/ADVifv370J9ff361anvj4eNYZH94eYfPkwaNWYw3P+7k8fcSrhRqwuasGo2TyHW2ViRyWS//rqZJMm9e/788Ydd8dcub/rxW9jfvWuvuEvnCwsL2WQlJSUXL8ZG9OgD2z9sWrd33/aoie/t3HHk7bHRJ08d3bHzl6cue+zPw5BszidLmjYJ02g070+fePlK3Pvvfbxxw6/OTi7R745KSX2Eag+7BvnqNStGvTXh+LELYc1bfbfh65VfLZ01c/6R32MUcsWqr5fX6oIm9CPiEvFpfH39Rwwfa29n7+rqFt7+pdu3b8LOrl0joJz768xxNs2ZsyfhbbduvQqUBVu3/QhNzM6du8Ep3bpGvDFwyM+/fK9SqXQXvHw5btny+RMnTHn55a7wFgwOKLQ+/mhhxw6dXFxco955z8HRadeuLaiu9OzZp22bcPAAdusSAT+VAQMGQ6tXKpV26dIzMTGhdpPjaJP1jpqnEF8kMxs3bqrbtrd3KCxUIiZUjVvrVu3+OnOC3X/27Ml2bTuAjKBWBc3pNzHhdKVSmZKSzL59mJw0e+60nj36DB3yFrsHSlkoyUA67FsQEFz5ytVLqK74+weyG7Z2dvAaHBTCvrW2soZ7KysrQ2LAHI0V+oX8iISR3lYo/6AShEpZIpH8HfvXlMkzEdPaY1qQVgorXTJraxt4LS4uUmh3frVqmVqtBsnqEiiVBaAPaNLpX9zJqe6zvXRhFw2+rS00wj0r9QXBSV6CEKHJFfP3ablcztTLXXvBTltbphAqLinWJSsqYtqRLi5ubFHa+9V+TZqEgQXTvv2/2FIQCldra+vFi77Uv7iENF2MBT3AUnlBHdcZ3LNSUxwdHKE6Pn8+prS05OVOXW1smJKvYcPGUEBev34FrBA22c2b16Cx6O7uwQrx1V59W7Zsc+HC34uXzN74/Xa4CJxSXFzs4eHl6+PHnpKaluLkKJj5ryayVrCxUgvAZLl69VJc3DkoHdk9DvYOvSL+/fMvG2NiTucX5P/xx8E9e38dPHj4U+XKzA/mgfWwdNk82AY1d+jQacWKhRkZ6Xl5uXv37XgnauThw/uRAGAc2nj0TT3CUWZCdfzFl0sUCgWUiLqd70ZPB9ktXPwxtAV9fPwih40ZNnTUUyfa2trOm7N00pSx7fKUHgAAEABJREFUu/f8OuiNIZ8uXrn/wK4Fiz66cSPe379BRMRrgwYNRZaNGca++fr9xMiPQ+Q4+lLt+e27h8on6vFLghHv4DYiphKSGY+Ih4HVH2Is5Lds3bR16yaDhxoEBq9etRFxjwaP0K5fxFgi9u//n+7dXzV4SCrh6TERputZwSWiUACnD/whS8Use1bwnJU6wkxYIXDPSn1BYB3WEWbCCo2NFYwFg9uImEpIPGcFIwQoPGelfsE96HWDlBCmGgZknkI00a9a9DDRwEwU9hlXzRhBgIWIEQRmKERo6AhiuLMIkVtJFDamadeYYbNeIiVTEksQpvYUKzW2dqYpm8xQiE5usqtnTLOOl9gpyFWFv+aCTIEZCnHoDL/8rLLLJ/IRpjb8uuKBu7ciINQ0Czeb7XrN389OkltLAprY2bsrKHWNllOi9caP6W8TVbtqoDP2qYEBdMUC3/oQNdjzfAjaYOev7vaMHNf7OPo5o+IISpL+oDD1XlFoG/sug12RiTDnFex3r0l/klqkViGVynADvHw97/LJvFXWETcmGr0l56tcByEDO5+751mZGDir6p0QFefQzxyu2KTLr1T+7ZgX/cs+dUGpAllbyZqGO3bs64RMhzkLsYZ8+SUzxfj9999HvDB16tQhQ4Z06tQJccD27dvh68hkMltbW3d398DAwNatWzfVgoSNRQsxPj6+RYsW169fDwsLQ3yxcOHCAQMGtGrVCnEDqPzOnTskSbJByaAwdHR0tLe337dvHxIwFtorCz+/6Ojo9PR02OZThcCcOXO4UyHQt29fKysm2gmpBYSYn5+fnJyMhI0llojZ2dnweBITEzt0qP+Ixc8F1O/s7KxQKBA3FBcXjxw5MikpSbfHxsbm9OnTSNhYVolYWlo6ceJEeFQuLi4mUSEwa9Ys+A0gzrC2tu7Vq5culBRU0IsWLUKCx7KEePDgwQkTJvj5+SHT4enpycbN4Y5BgwZ5eXkhrQovXbq0d+/etWvXImFjEULMy8ubMWMG0j6hdu3aIZOyfPnyoKAgxCVgL3fr1g02fHx84PWLL76Qy+WTJ09GAsYihLhgwYK3334bCYOUlBR1zRzsL8L06dOhJfrbb7+xb+HrR0ZG9ujR49GjusRI5gFzNlbALDh58uTQocKKbwS+m3Xr1rFlFc+A+fzWW29FRUX17t0bCQyzLRGLiorGjRvXpUsXJDCg9Qb2BDIFDg4O0F4EC5r14QsKMywR09LSCgoKfH19oXcBYQyxZcuW48ePb9iwAQkGcysRb968ydrFglXhw4cPDS7EwifQXgTb5aWXXrp9+zYSBuYjxNTUVKT1FB44cIBr/8iLMGLEiJIS04/bhd4dqKPnz58PlTUSAGYiRBDfvHnzYAP6+JGwATNFLowoojKZDOroa9euLV68GJka0bcRc3NznZycdu/eDT5ChKkTe/bs2blz5+bNmyUSk832EbcQv/vuO8i7sWPHIvHw4MGDBg0aIIGRkJAwatSob7/9ltMBGdUg1qoZ2oLZ2dnQ6heXCqF1OHz4cCQ8QkNDY2NjV61atXXrVmQKRCnE9evXg+0JNfLEiRORqID6JzjYBKHSa8j3338PNt/s2bMR74hPiIcOHYLXRo0ambBBU2fAlQ1NMSRgoG+wc+fO0OAGXyziETG1EeERQg9VXl6eo6MjEicajQb87aYd/lMToMKBJuPSpUs7duyIeEE0JeKsWbPYgcfiVSHw+PHjd955BwmegICAEydOwC9/40Y+VjNAohDi2bNn4XXatGn//e9/kcghCEKAJrMx1qxZA0YhVNaIewQtRLVaPWDAAHZUvaenJxI/8C3g6SLxEBUVBY+gT58+mZmZiEuE20ZMT0+HHgjwd5hkxBRHlJWVZWVlie4bwT1D63zZsmUtWrRA3CDQEhG6nuLj411cXMxJhUg7swm6IkXXieDm5gbOCvAyZmRkIG4QqBChOATrGJkdYGl988030DNu8gE4deDy5cvcNZBwpAfTkJycTJKkr68vEgl37tyZO3cud/0uAi0RNVqQ+eLv7x8dHV1YWIhEAggROhEQZwhUiFB//fLLL8is2bdvX0JCglKpRGLg7t27ISEhiDMEKkTuAiEIirZt26akpMTExCDBAyUip0IUaAztCRMmIMsgNDR0ypQpLVu2tLOzQwImMTHREktEs28j6gNukfz8fMHOOEbaCAXQxeLh4YE4Q6BChF7OdevWIYsB3KU5OTmmGgv4XLguDpGQ24iEha12C50Wqamp4PFGwoMHIWI/orAoKiq6desWGDFISCxatKh58+YDBw5EnIHbiMLCxsbGyspqyZIlSEhAicipExEJVoh79uz57LPPkEXSrFmzJk2aICFhuW1EuVxuaW1Efdipsfv370cCAHoj3d3dufbsClSIAwYMmDVrFrJswHxhwzqaFq4791gEKkSKongIIihwgoKCRo8ejUwND/UyEqwQjx49yoYQsXDAVkUVK8GYCosWokwmI0kLXXrjWaBcNOGUK36qZuxHFAcFBQX29vbQXJFKmeEBffr0gd/qgQMHEMdAz16PHj3Y+WucgtuI4gBUiLSz3wsLC/v165eVlQVdgkeOHEEcw4MHkUWgQoyNjeVnFqO4+Oqrr1577TV2wSzoDPzzzz8Rx3A9+kuHcNuIluxHNMaQIUOgD5DdhvxJSEhgRckd/FgqSLBCDA8PX7lyJcLoERkZeffuXf09GRkZp06dQlzCj6WCBCtEMKFUKhXC6AHtZj8/P/3QU2VlZeDnQlzC9QwBHQIdoR0fHw8lIm+BV0TBtm3bLl26dOHChXPnzimVyrS0NE/btnS+y9Hdt729vaokfc5i40aABFSVdczBVA9065p8g0hG+VVOr2ZZ9ar7SZLw8FO4+T4/VLOw3Dfjxo2DLIZbglewCj08PKAYgFbRsWPHEEaPHxbcK8rTECTSMK6FpxvTNddh3XY+V9I6pDIQGCGTEy1fdu74b6fqUiIh0axZs59//lnnymZHz0OPO8Lo8e1H9zwCrAdHeSNBxIR/Ptdj8uLPPvEOVAQ0M7rSkbDaiCNGjHg2dqCp1rMVJus/vtesvWtEpGhUCIR1chzyQdDBH9Mu/mE0eoewhAh1cd++ffX3uLq6CjPotEn4/cdMqUzSOkKUESKbdXS6fCrb2FHBWc3Dhg3TLxRbt27duHFjhNGS8bDEzdsKiZO2PV1UKrrMSDwBwQnRwcGhf//+bI+qi4vLyJEjEaYCValaaiXisSAUhbIyDM8OE+K30hWKzbUgTAXqMlpdJmL3Kq2hKSMzkV7IalYVo7MHH6fdLykp0kAegd4pDQ0+BZoCBxJY+IxviEDaF4JGcERCUBRd6Usgyx1XzDu6YkN7sFuDTzV+GplUsnbmPVJKaOCyWocBc3GaSUSQ2rS09uLMAeZz2I+ueKf3JaWQngRXgp2TxK+Rzb/+7YIwpoBGRh0/dRTikc0ZD24VqkooUiYBb4tMIZXZkDQFT58mCQL+a3uKWc2xrkrmLdt9zHou2QSV+mN1ViEgBatI7bmM5mhal6Dc91mhWv0vVqm/qp4uqVQCPxJNmSY7XZ3x8MnFY09sHKSN29h3ft0VYYRBrYX4+w8Z964rJVLS3s3eN0yURYtGRadcz756Jhf+2nZ3xgUkfzAFleEjtRPitx/eh7KoQUtvO3cRR+uSyIiA1m4IuT2+mxd3POfm+YIx80UT6V/cQD1mJFJuTY2VlMSS1dMS7d1tm3T1F7UK9XFv6BjWMxCart9Mv4vEALQ9xD02jkbGvkCNhJj3WL13bUqzHkE+zcywURUY7u3VxH3NDFFokaBJMUvReC/184V490rRL8sfhEUEkuJb+q6muPjZBncIEL4WGSuNEvMcIwJRdS4RD/+Y1qiDPzJ3rO1Jt0DndR+Ko44WKzQi61Yirp+dZOdhJ7Mz38JQD8+GjhKZdMtnyQjDDYz3zYjiqhPiiR1Z6lJNg1ZuyGJo1MnvSVppelIZEiYin8bDeHnrYDXfiM11D7I4H5u9i82hTWlImIh8DjrT41DbEvHM3mzQr3uQAxIkl+OPzZjTUVmYg+qbBu08i/JVeY+FGJ2Rcd9I+C4VBw6K2PzTBlQfMD1itS0Rb5zPs3ES64ijF0RmJT26hdtpmnWD6ULV1K5U/N+CDw/9vg8JBRoZmSVsVIhlJZR3YwtqHerj4G7/JKMUmQUJCTeQgKg6GkUPw118t84VgnCtHWWIG5IeXv3jxIbkRzfsbJ2bhnZ+tfs4Kytb2H82dsfRUxujxq7dvO2jjMx73p4hXToNC2/bjz3rt8NfX7xySCG3adOyt4dbAOIMj2DH7OT6r/T5p3vP9vD62YqFa9d9eWDfScSswn7qx83rHzy87+joFBISOnXyLE/P8hmA1RxiAS/mrt1bjxz5LfnRgwYBQe3b/2vsmCj96a3PBy5B1qZEvHejgJRy5bLJyk7+dtNklap00oQNoyKXpWXcWbsxSqOdjiaRyoqLC/YeXPHfgR9/tiC2ZfMe2/cuysllasmY87tizu8c1PeDqRN/cHX2OXrie8QZEjlBkMTtC8JbnKyW7cPDh5jgSR/MmMOq8GLcubnzP3j11b7btx2aN2dpRkbaylVL2ZTVHNKxe/e2n3/ZOPg/kdu2/Na//38OHtq77dfNqFYQzNAsg0cMC1GZo5HKuBoze+nKYalENnrYMk/3QC+P4Ddf/yQlLeHazfKIBRqNqlf3cQ38W8A9t2/dF35CKWm3Yf+Zv7e3DOsJ0rSxcYAyMiS4PeISkiQyHgmudmYeyQt08W38YW2XV3qAkqDMCwtrGR01LTb2zC1t3V3NIR1Xrl4KDW3Wu3c/Jyfnfn3fWLN6U8cOL6N6wrDa1GoNdz4rqJf9/ZrZ2pbPcnVx9nZ18bv/4LIuQYBvGLthY83Y7MUlBSDHrCfJnh5BujR+PtyGO4efQXGR4MZCM6XJC3Tx3bt3p0mTMN3b0MbN4PXWrevVH9LRvHmruLhzyz9bcPjIgbz8PF8fv5CQ2k0nIpAxW8X4MDDuvBfFJcrklBvgfNHfmV9QOb/r2fBLJaWFFKVRKGx0e+Rya8QpBJIQAg2DUTeUSmVpaalCUekJsbFh8rOoqLCaQ/pXgPLSxsb2bMypZcv/J5VKu3XrNXH8FDe32s06N/YzMpzXcgU0QbkKT2hv7xrUoHXvHlWWfbS1rW6KpJXCliQlKlWJbk9pWRHiEpqiFTYi78eoipUVo7OSksq5S4Vanbm6uFVzSP8KJElCjQx/SUn3Ll06v2nz+sJC5ZJFtQirTBNgMxuuhA0L0dFNlp3GVcXk49ko7sqh4MA2uogO6Zn33F2rs4KhjHR28k56GN+1ok1yM4HbGKYURXsFcVzo8guUYaGNm16/flW3h90ObtiomkP6VwB7uXHjpkFBDQMDg+GvQFlw8NAeVCtogqANe7QNy7NhSzttM5ETwCNDUdT+378sKyvJfPzgtyOrP18dmU2R1/cAAATjSURBVJaRWP1ZrZpHxN84AR0qsH38r80PHl1DnFGm1ECXaEgrGyQwSAldq7CRCoXC3d3j4sXYfy5fVKvVbwwccubsyV27tuYX5MOeb9Z+0bZNeKOQUEhZzSEdfx4/DJZ1TMxpaCCCKfPXmePNw1qhesJwiRjcwgbsg4LHJfbu9d+5AmbvjElbTvz108p1ozIfJwX4hb058JPnGh8RXccUFubsPfT5z9s/gZp9wGvvbdkxl6MIUpn3c2SCnD5MaWodNGt45NgfNq07fyFm65bfwDvzOCvz1x0/rf7mc/ARtm/3r/HjJrHJqjmkY/q02avXrPhkzjTETDl3hTr6zcEjUO0wevNGv9hPix+qNNLgcE9keSScSvZqYPV6lBcSGGtn3vUNse4+xAeJk03zE994x9cv1ECbx+jvvvlLjsV53BoEgkVVphagChE76MGsLKhKjHoo2vRwPH80Oz0h1yvUcFi73LyMFasjDR6yVtgVlxrulvByD5404TtUf8xe3NPYIeitkUgMfMHAgJbjRhq19RLPpdpx1rf5gjCDHmgRK5GgjRZ91bnK2vV0OXc425gQ7e1cp0X/ZPAQWCFyueHGJUnWs3PO2D0wt6EqlcsMTDiUSqqL6FaSXxq1lI9gvXVFxGMSacLowNjqZNE+win+TN79i2lB7b2fPQqFjYuz6Rsr9XsPt/9K9m9kIxFP6EGz4Tm24Zj5DYrzS3PTLKKx+OhqFng2X48SqykgfAj0AvOao5c3fHQ9E5k76TdzCrILxy0KRAJGG+tHzGHpmJdaDoythEBRyxteO3o/J9Vsy8VH8dl5mflRy4ORsNGWKBQSLRWBuQxQo58XdDxP+iIk9UbG/QtCHED/gtw++6got3DiUqGr0Dwg6hzpQce7n4cQtPrmyQfpCU+QWZD0TyaU9M7O0glLghDGpNTOmTJ6XoMLf+ReOvEkJ7XAyk7h0cjF1kl8FmZOijIrKa+0qExuJRkUFeDTSDRfgZQQhMiXsTbmBa21Vy/8VSf4u3iMCS6YFJeK2NxBBCklUUXA1vKP1J8o81QYKMJAcE7EtAGY4LD6J+jCfSI2XCx7fWZaInses58kaKo8Wiwbx5Mp6GldhFlKO1YASTVqNU1RqlIKdjq4yHsN9Q1sLrLxNZSGZuLyipnajUd8LuBihD/YSPynMDG+ICe9TFVGa7OpMg0hoWmNVhlExcdX3AXj1aYQm6WkDFEVI84kclqjDbIAP3uKHYpMgJXIhKBldkoQpakY5gvSk5TPrZTImNiburcEyb6SlJqGjhWNmln/SG4Nr3JnD2nTcAfvhhY6TVbIvGg/R0gbW/hDGMyLYVaj4c0emVwilYk4IJZUSiAj0Q2xEMWEzIooLRJxGxHa9H7Bhk1DcZtglkZgU/vsdLGGoIjZn6WwliAjBToWopjo+h8XeGDHt4iyx/XBtfweb3oYOyqs9ZoxNWHzoofgFGjb3a1BmAjMf2UufenY4we3CkbNDrR1NNrAxUIUJTtWpjxJKwWfq6bGwcGYpb9qEDSBqNcBj+BjJglkbSd9dbinT0h1PxssRDFThoqLq51sqS8rkqiMEmFMbuxcBIoyvGC9boU6Wuvp1Y4X19tfNdIX+1YisbZDNQELESMIsPsGIwiwEDGCAAsRIwiwEDGCAAsRIwiwEDGC4P8AAAD//0t/lWgAAAAGSURBVAMADUbvrUGecrcAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e945d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'invoke_llm': {'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 389, 'total_tokens': 412, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-2a57beae-3d7a-4efc-a9cc-ec78c03e001d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--6e2290eb-7fd7-44c8-ba75-31404a2d4e73-0', tool_calls=[{'name': 'calculator', 'args': {'expression': '2+2'}, 'id': 'call_97c56c94b6524dfcb4d7deee', 'type': 'tool_call'}], usage_metadata={'input_tokens': 389, 'output_tokens': 23, 'total_tokens': 412, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='4', name='calculator', id='7b49626d-6f4b-4b9c-bc42-49698aa5b200', tool_call_id='call_97c56c94b6524dfcb4d7deee')]}}\n",
      "{'invoke_llm': {'messages': [AIMessage(content='2 + 2 equals 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 428, 'total_tokens': 436, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-75c3c775-0954-4333-b240-bb40595cd02d', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--f7b78379-c3b3-4512-a3f8-8ea35128de71-0', usage_metadata={'input_tokens': 428, 'output_tokens': 8, 'total_tokens': 436, 'input_token_details': {}, 'output_token_details': {}})]}}\n"
     ]
    }
   ],
   "source": [
    "for e in graph.stream({\"messages\": (\"human\", \"How much is 2+2\")}):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8931a19",
   "metadata": {},
   "source": [
    "### Tool-calling paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77f788db",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"I signed my contract 2 years ago\",\n",
    "    \"I started the deal with your company in February last year\",\n",
    "    \"Our contract started on March 24th two years ago\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cce6f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_date(year: int, month: int = 1, day: int = 1) -> date:\n",
    "    \"\"\"Returns a date object given year, month and day.\n",
    "\n",
    "    Default month and day are 1 (January) and 1.\n",
    "    Examples in YYYY-MM-DD format:\n",
    "      2023-07-27 -> date(2023, 7, 27)\n",
    "      2022-12-15 -> date(2022, 12, 15)\n",
    "      March 2022 -> date(2022, 3)\n",
    "      2021 -> date(2021)\n",
    "    \"\"\"\n",
    "    return date(year, month, day).isoformat()\n",
    "\n",
    "\n",
    "@tool\n",
    "def time_difference(\n",
    "    days: int = 0, weeks: int = 0, months: int = 0, years: int = 0\n",
    ") -> date:\n",
    "    \"\"\"Returns a date given a difference in days, weeks, months and years relative to the current date.\n",
    "\n",
    "    By default, dayss, weeks, months and years are 0.\n",
    "    Examples:\n",
    "      two weeks ago -> time_difference(weeks=2)\n",
    "      last year -> time_difference(years=1)\n",
    "    \"\"\"\n",
    "    dt = date.today() - timedelta(days=days, weeks=weeks)\n",
    "    new_year = dt.year + (dt.month - months) // 12 - years\n",
    "    new_month = (dt.month - months) % 12\n",
    "    return dt.replace(year=new_year, month=new_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce08f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I signed my contract 2 years ago The starting date of your contract is November 1, 2023.\n",
      "\n",
      "I started the deal with your company in February last year The contract started in February 2024.\n",
      "\n",
      "Our contract started on March 24th two years ago The contract started on March 24th, 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "agent = create_agent(\n",
    "    llm,\n",
    "    [get_date, time_difference],\n",
    "    system_prompt=\"Extract the starting date of a contract. Current year is 2025.\",\n",
    ")\n",
    "\n",
    "\n",
    "for example in examples:\n",
    "    result = agent.invoke({\"messages\": [(\"user\", example)]})\n",
    "    print(example, result[\"messages\"][-1].content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5959957",
   "metadata": {},
   "source": [
    "## What are agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed094a3",
   "metadata": {},
   "source": [
    "### Plan-and-solve agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4379fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "\n",
    "system_prompt_template = (\n",
    "    \"For the given task, come up with a step by step plan.\\n\"\n",
    "    \"This plan should involve individual tasks, that if executed correctly will \"\n",
    "    \"yield the correct answer in JSON format and recorded in field 'steps'.\\n\"\n",
    "    \"Do not add any superfluous steps.\\n\"\n",
    "    \"The result of the final step should be the final answer. Make sure that each \"\n",
    "    \"step has all the information needed - do not skip steps.\"\n",
    ")\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like(temperature=1.0)\n",
    "\n",
    "planner = planner_prompt | llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "456e72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "from langchain_core.tools import convert_runnable_to_tool\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    expression: str = Field(description=\"Mathematical expression to be evaluated\")\n",
    "\n",
    "\n",
    "def calculator(state: CalculatorArgs, config: RunnableConfig) -> str:\n",
    "    expression = state[\"expression\"]\n",
    "    math_constants = config[\"configurable\"].get(\"math_constants\", {})\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")\n",
    "\n",
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    args_schema=CalculatorArgs,\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "462d00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 73ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/6] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.14.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msoupsieve\u001b[0m\u001b[2m==2.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install arxiv==2.2.0 wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e0d3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import load_tools\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm) + [\n",
    "    calculator_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6258599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState, create_agent\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a smart assistant that carefully helps to solve complex tasks.\\n\"\n",
    "    \" Given a general plan to solve a task and a specific step, work on this step. \"\n",
    "    \" Don't assume anything, keep in minds things might change and always try to \"\n",
    "    \"use tools to double-check yourself.\\n\"\n",
    "    \" Use a calculator for mathematical computations, use Search to gather\"\n",
    "    \"for information about common facts, fresh events and news, use Arxiv to get \"\n",
    "    \"ideas on recent research and use Wikipedia for common knowledge.\"\n",
    ")\n",
    "\n",
    "step_template = (\n",
    "    \"Given the task and the plan, try to execute on a specific step of the plan.\\n\"\n",
    "    \"TASK:\\n{task}\\n\\nPLAN:\\n{plan}\\n\\nSTEP TO EXECUTE:\\n{step}\\n\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", step_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class StepState(AgentState):\n",
    "    plan: str\n",
    "    step: str\n",
    "    task: str\n",
    "\n",
    "\n",
    "execution_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    state_schema=StepState,\n",
    "    system_prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af5a0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "import operator\n",
    "\n",
    "\n",
    "class PlanState(TypedDict):\n",
    "    task: str\n",
    "    plan: Plan\n",
    "    past_steps: Annotated[list[str], operator.add]\n",
    "    final_response: str\n",
    "\n",
    "\n",
    "def get_current_step(state: PlanState) -> int:\n",
    "    \"\"\"Returns the number of current step to be executed.\"\"\"\n",
    "    return len(state.get(\"past_steps\", []))\n",
    "\n",
    "\n",
    "def get_full_plan(state: PlanState) -> str:\n",
    "    \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n",
    "    full_plan = []\n",
    "    for i, step in enumerate(state[\"plan\"].steps):\n",
    "        full_step = f\"# {i+1}. Planned step: {step}\\n\"\n",
    "        if i < get_current_step(state):\n",
    "            full_step += f\"Result: {state['past_steps'][i]}\\n\"\n",
    "        full_plan.append(full_step)\n",
    "    return \"\\n\".join(full_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "246ee35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "async def _build_initial_plan(state: PlanState) -> PlanState:\n",
    "    print(f\"task: {state['task']}\")\n",
    "    plan = await planner.ainvoke(state[\"task\"])\n",
    "    print(f\"plan: {plan}\")\n",
    "    print(f\"plan.steps: {len(plan.steps)}\")\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "async def _run_step(state: PlanState) -> PlanState:\n",
    "    print(f\"state: {state}\")\n",
    "    plan = state[\"plan\"]\n",
    "    current_step = get_current_step(state)\n",
    "    step = await execution_agent.ainvoke(\n",
    "        {\n",
    "            \"plan\": get_full_plan(state),\n",
    "            \"step\": plan.steps[current_step],\n",
    "            \"task\": state[\"task\"],\n",
    "        }\n",
    "    )\n",
    "    return {\"past_steps\": [step[\"messages\"][-1].content]}\n",
    "\n",
    "\n",
    "async def _get_final_response(state: PlanState) -> PlanState:\n",
    "    final_response = await (final_prompt | llm).ainvoke(\n",
    "        {\"task\": state[\"task\"], \"plan\": get_full_plan(state)}\n",
    "    )\n",
    "    return {\"final_response\": final_response}\n",
    "\n",
    "\n",
    "def _should_continue(state: PlanState) -> Literal[\"run\", \"response\"]:\n",
    "    if get_current_step(state) < len(state[\"plan\"].steps):\n",
    "        return \"run\"\n",
    "    return \"response\"\n",
    "\n",
    "\n",
    "builder = StateGraph(PlanState)\n",
    "builder.add_node(\"initial_plan\", _build_initial_plan)\n",
    "builder.add_node(\"run\", _run_step)\n",
    "builder.add_node(\"response\", _get_final_response)\n",
    "\n",
    "builder.add_edge(START, \"initial_plan\")\n",
    "builder.add_edge(\"initial_plan\", \"run\")\n",
    "builder.add_conditional_edges(\"run\", _should_continue)\n",
    "builder.add_edge(\"response\", END)\n",
    "\n",
    "graph = builder.compile().with_config({\"recursion_limit\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f20109f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50156116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: Write a strategic one-pager of building an AI startup\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m task = \u001b[33m\"\u001b[39m\u001b[33mWrite a strategic one-pager of building an AI startup\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# qwen3-max-2025-09-11 Êó†Ê≥ïÊ≠£Á°ÆÁîüÊàêÁ≠îÊ°àÔºåqwen3-max-2025-09-23 ÂèØ‰ª•„ÄÇ\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ËøêË°åÊó∂Èó¥ÂæàÈïø„ÄÇ\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# TODO: Ë∑ëÂÆåÊï¥„ÄÇ\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m graph.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: task})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3182\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3179\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3180\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3182\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3183\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3184\u001b[39m     config,\n\u001b[32m   3185\u001b[39m     context=context,\n\u001b[32m   3186\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3188\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3189\u001b[39m     print_mode=print_mode,\n\u001b[32m   3190\u001b[39m     output_keys=output_keys,\n\u001b[32m   3191\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3192\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3193\u001b[39m     durability=durability,\n\u001b[32m   3194\u001b[39m     **kwargs,\n\u001b[32m   3195\u001b[39m ):\n\u001b[32m   3196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3197\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36m_build_initial_plan\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_build_initial_plan\u001b[39m(state: PlanState) -> PlanState:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     plan = \u001b[38;5;28;01mawait\u001b[39;00m planner.ainvoke(state[\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplan: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplan\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplan.steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(plan.steps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3130\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3129\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3130\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3131\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5502\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5495\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5496\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5497\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5500\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5501\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5503\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5504\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5505\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5506\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    394\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m     **kwargs: Any,\n\u001b[32m    400\u001b[39m ) -> AIMessage:\n\u001b[32m    401\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    403\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    404\u001b[39m         stop=stop,\n\u001b[32m    405\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    407\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    408\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    409\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    410\u001b[39m         **kwargs,\n\u001b[32m    411\u001b[39m     )\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    413\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    414\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1099\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1092\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1096\u001b[39m     **kwargs: Any,\n\u001b[32m   1097\u001b[39m ) -> LLMResult:\n\u001b[32m   1098\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1100\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1101\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1006\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   1008\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1013\u001b[39m     run_id=run_id,\n\u001b[32m   1014\u001b[39m )\n\u001b[32m   1016\u001b[39m input_messages = [\n\u001b[32m   1017\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m   1018\u001b[39m ]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1020\u001b[39m     *[\n\u001b[32m   1021\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m   1022\u001b[39m             m,\n\u001b[32m   1023\u001b[39m             stop=stop,\n\u001b[32m   1024\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1025\u001b[39m             **kwargs,\n\u001b[32m   1026\u001b[39m         )\n\u001b[32m   1027\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m   1028\u001b[39m     ],\n\u001b[32m   1029\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1030\u001b[39m )\n\u001b[32m   1031\u001b[39m exceptions = []\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1310\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1308\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1311\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1509\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1507\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1508\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m     raw_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root_async_client.chat.completions.with_raw_response.parse(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   1510\u001b[39m         **payload\n\u001b[32m   1511\u001b[39m     )\n\u001b[32m   1512\u001b[39m     response = raw_response.parse()\n\u001b[32m   1513\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1630\u001b[39m, in \u001b[36mAsyncCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m   1624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m   1625\u001b[39m         response_format=response_format,\n\u001b[32m   1626\u001b[39m         chat_completion=raw_completion,\n\u001b[32m   1627\u001b[39m         input_tools=tools,\n\u001b[32m   1628\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1631\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1632\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1633\u001b[39m         {\n\u001b[32m   1634\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1635\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1636\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1637\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1638\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1639\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1640\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1641\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1642\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1643\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1644\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1645\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1646\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1647\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1648\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1650\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   1651\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1652\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: _type_to_response_format(response_format),\n\u001b[32m   1653\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   1654\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1655\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1656\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1659\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1660\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1661\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1662\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1663\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1664\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1665\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1666\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   1667\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   1668\u001b[39m         },\n\u001b[32m   1669\u001b[39m         completion_create_params.CompletionCreateParams,\n\u001b[32m   1670\u001b[39m     ),\n\u001b[32m   1671\u001b[39m     options=make_request_options(\n\u001b[32m   1672\u001b[39m         extra_headers=extra_headers,\n\u001b[32m   1673\u001b[39m         extra_query=extra_query,\n\u001b[32m   1674\u001b[39m         extra_body=extra_body,\n\u001b[32m   1675\u001b[39m         timeout=timeout,\n\u001b[32m   1676\u001b[39m         post_parser=parser,\n\u001b[32m   1677\u001b[39m     ),\n\u001b[32m   1678\u001b[39m     \u001b[38;5;66;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;00m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;66;03m# in the `parser` function above\u001b[39;00m\n\u001b[32m   1680\u001b[39m     cast_to=cast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\n\u001b[32m   1681\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1682\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_base_client.py:1529\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1527\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1530\u001b[39m         request,\n\u001b[32m   1531\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1532\u001b[39m         **kwargs,\n\u001b[32m   1533\u001b[39m     )\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1535\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:237\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:180\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1263\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1258\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1261\u001b[39m ):\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "task = \"Write a strategic one-pager of building an AI startup\"\n",
    "\n",
    "# qwen3-max-2025-09-11 Êó†Ê≥ïÊ≠£Á°ÆÁîüÊàêÁ≠îÊ°àÔºåqwen3-max-2025-09-23 ÂèØ‰ª•„ÄÇ\n",
    "# ËøêË°åÊó∂Èó¥ÂæàÈïø„ÄÇ\n",
    "# TODO: Ë∑ëÂÆåÊï¥„ÄÇ\n",
    "result = await graph.ainvoke({\"task\": task})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-with-lang-chain-2ed (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
