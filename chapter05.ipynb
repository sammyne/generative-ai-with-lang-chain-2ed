{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7e1ce8",
   "metadata": {},
   "source": [
    "# 05. Building Intelligent Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f19f6",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba16328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m42 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/41] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m41 packages\u001b[0m \u001b[2min 72ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.10.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgreenlet\u001b[0m\u001b[2m==3.2.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.11.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonpatch\u001b[0m\u001b[2m==1.33\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonpointer\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==0.3.27\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==0.3.79\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==0.3.35\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==0.3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==0.6.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==0.6.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.4.38\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.10.23\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.44\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzstandard\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=0.3 langchain-core~=0.3 langchain-openai~=0.3 langgraph~=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601fb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                            \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235a80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m46 packages\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/16] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m16 packages\u001b[0m \u001b[2min 80ms\u001b[0m\u001b[0m=0.3.31                        \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.3.31\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install 'langchain-community>=0.3,<0.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22f338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        embeddings_model = os.getenv(\"OPENAI_EMBEDDINGS_MODEL\")\n",
    "        hf_pretrained_embeddings_model = os.getenv(\"HF_PRETRAINED_EMBEDDINGS_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.hf_pretrained_embeddings_model = (\n",
    "            hf_pretrained_embeddings_model\n",
    "            if hf_pretrained_embeddings_model\n",
    "            else \"Qwen/Qwen3-Embedding-8B\"\n",
    "        )\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_embeddings(self, **kwargs) -> OpenAIEmbeddings:\n",
    "        if not self.embeddings_model:\n",
    "            raise ValueError(\"OPENAI_EMBEDDINGS_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            model=self.embeddings_model,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.tiktoken_enabled\n",
    "            # 对于非 OpenAI 的官方实现，将这个参数置为 False。\n",
    "            # 回退到用 huggingface transformers 库 AutoTokenizer 来处理 token。\n",
    "            tiktoken_enabled=False,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.model\n",
    "            # 元宝说 Jina 的 embedding 模型 https://huggingface.co/jinaai/jina-embeddings-v4 最接近\n",
    "            # text-embedding-ada-002\n",
    "            # 个人喜好，选了 Qwen/Qwen3-Embedding-8B\n",
    "            # tiktoken_model_name='Qwen/Qwen3-Embedding-8B',\n",
    "            tiktoken_model_name=self.hf_pretrained_embeddings_model,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6c196",
   "metadata": {},
   "source": [
    "## What is a tool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af38b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEARCH: current US president age'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "question = \"how old is the US president?\"\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Do not make any assumptions on recent events or things that can change.\"\n",
    "    \"Now, act to answer a user question:\\n{QUESTION}\"\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "(prompt_template | llm).invoke(question).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7305668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RESPONSE Berlin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1 = \"What is the capital of Germany?\"\n",
    "\n",
    "(prompt_template | llm).invoke(question1).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d87f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE: As of February 2025, the current U.S. president is Donald Trump, who is 78 years old (born June 14, 1946).\n"
     ]
    }
   ],
   "source": [
    "query = \"age of current US president\"\n",
    "search_result = (\n",
    "    \"Donald Trump › Age 78 years June 14, 1946\\n\"\n",
    "    \"Donald Trump 45th and 47th U.S. President Donald John Trump is an American \"\n",
    "    \"politician, media personality, and businessman who has served as the 47th \"\n",
    "    \"president of the United States since January 20, 2025. A member of the \"\n",
    "    \"Republican Party, he previously served as the 45th president from 2017 to 2021. Wikipedia\"\n",
    ")\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Today is {date}.\"\n",
    "    \"Now, act to answer a user question and \"\n",
    "    \"take into account your previous actions:\\n\"\n",
    "    \"HUMAN: {question}\\n\"\n",
    "    \"AI: SEARCH: {query}\\n\"\n",
    "    \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "result = (prompt_template | llm).invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"query\": query,\n",
    "        \"search_result\": search_result,\n",
    "        \"date\": \"Feb 2025\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cdcb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH: Donald Trump birth date\n"
     ]
    }
   ],
   "source": [
    "query = \"current US president\"\n",
    "search_result = \"Donald Trump 45th and 47th U.S.\"\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"You have access to search engine that provides you an \"\n",
    "    \"information about fresh events and news given the query. \"\n",
    "    \"Given the question, decide whether you need an additional \"\n",
    "    \"information from the search engine (reply with 'SEARCH: \"\n",
    "    \"<generated query>' or you know enough to answer the user \"\n",
    "    \"then reply with 'RESPONSE <final response>').\\n\"\n",
    "    \"Today is {date}.\"\n",
    "    \"Now, act to answer a user question and \"\n",
    "    \"take into account your previous actions:\\n\"\n",
    "    \"HUMAN: {question}\\n\"\n",
    "    \"AI: SEARCH: {query}\\n\"\n",
    "    \"RESPONSE FROM SEARCH: {search_result}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(raw_prompt_template)\n",
    "\n",
    "result = (prompt_template | llm).invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"query\": query,\n",
    "        \"search_result\": search_result,\n",
    "        \"date\": \"Feb 2025\",\n",
    "    }\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18600905",
   "metadata": {},
   "source": [
    "### Tools in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0853679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'google_search',\n",
       "  'args': {'query': 'current US president age'},\n",
       "  'id': 'call_a33b4f1da9de495b94a1d3ba',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"google_search\",\n",
    "        \"description\": \"Returns about common facts, fresh events and news from Google Search engine based on a query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"search_query\",\n",
    "                    \"description\": \"Search query to be sent to the search engine\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "step1 = llm.invoke(question, tools=[search_tool])\n",
    "\n",
    "step1.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b936e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of now, the current U.S. president is Joe Biden, not Donald Trump. Joe Biden was born on November 20, 1942, which makes him 81 years old.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "tool_result = ToolMessage(\n",
    "    content=\"Donald Trump › Age 78 years June 14, 1946\\n\",\n",
    "    tool_call_id=step1.tool_calls[0][\"id\"],\n",
    ")\n",
    "step2 = llm.invoke(\n",
    "    [HumanMessage(content=question), step1, tool_result], tools=[search_tool]\n",
    ")\n",
    "assert len(step2.tool_calls) == 0\n",
    "\n",
    "print(step2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a69fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_a17f99edf28d4171bef4e1ab', 'function': {'arguments': '{\"query\": \"current US president age\"}', 'name': 'google_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 292, 'total_tokens': 316, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-bf581961-f6f6-45e6-bc5f-f1c7bf076080', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--0524fa66-0f40-43ae-8af2-0694d28c95d0-0', tool_calls=[{'name': 'google_search', 'args': {'query': 'current US president age'}, 'id': 'call_a17f99edf28d4171bef4e1ab', 'type': 'tool_call'}], usage_metadata={'input_tokens': 292, 'output_tokens': 24, 'total_tokens': 316, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind(tools=[search_tool])\n",
    "\n",
    "llm_with_tools.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835c720",
   "metadata": {},
   "source": [
    "### ReACT\n",
    "\n",
    "Give the LLM access to tools as a way to interact with an external environment, and let the LLM run in a loop:\n",
    "- **Reason**: Generate a text output with observations about the current situation and a plan to solve the task.\n",
    "- **Act**: Take an action based on the reasoning above (interact with the environment by calling a tool, or respond to the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83cb1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def mocked_google_search(query: str) -> str:\n",
    "    print(f\"CALLED GOOGLE_SEARCH with query={query}\")\n",
    "    return \"Donald Trump is a president of USA and he's 78 years old\"\n",
    "\n",
    "\n",
    "def mocked_calculator(expression: str) -> float:\n",
    "    print(f\"CALLED CALCULATOR with expression={expression}\")\n",
    "    if \"sqrt\" in expression:\n",
    "        return math.sqrt(78 * 132)\n",
    "    return 78 * 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28097b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "calculator_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Computes mathematical expressions\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"expression\",\n",
    "                    \"description\": \"A mathematical expression to be evaluated by a calculator\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"google_search\",\n",
    "        \"description\": \"Returns about common facts, fresh events and news from Google Search engine based on a query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"title\": \"search_query\",\n",
    "                    \"description\": \"Search query to be sent to the search engine\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "system_prompt = (\n",
    "    \"Always use a calculator for mathematical computations, and use Google Search \"\n",
    "    \"for information about common facts, fresh events and news. Do not assume anything, keep in \"\n",
    "    \"mind that things are changing and always \"\n",
    "    \"check yourself with external sources if possible.\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a659b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Config().new_openai_like()\n",
    "\n",
    "llm_with_tools = prompt | llm.bind_tools([search_tool, calculator_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c990dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.graph import MessagesState, START, END\n",
    "\n",
    "\n",
    "def invoke_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "def call_tools(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_calls = last_message.tool_calls\n",
    "\n",
    "    new_messages = []\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call[\"name\"] == \"google_search\":\n",
    "            tool_result = mocked_google_search(**tool_call[\"args\"])\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "        elif tool_call[\"name\"] == \"calculator\":\n",
    "            tool_result = mocked_calculator(**tool_call[\"args\"])\n",
    "            new_messages.append(\n",
    "                ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Tool {tool_call['name']} is not defined!\")\n",
    "    return {\"messages\": new_messages}\n",
    "\n",
    "\n",
    "def should_run_tools(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"call_tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2febead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALLED GOOGLE_SEARCH with query=current US president age\n",
      "CALLED CALCULATOR with expression=sqrt(78 * 132)\n",
      "The square root of the current U.S. president's age (78 years) multiplied by 132 is approximately **101.47**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"invoke_llm\", invoke_llm)\n",
    "builder.add_node(\"call_tools\", call_tools)\n",
    "\n",
    "builder.add_edge(START, \"invoke_llm\")\n",
    "builder.add_conditional_edges(\"invoke_llm\", should_run_tools)\n",
    "builder.add_edge(\"call_tools\", \"invoke_llm\")\n",
    "graph = builder.compile()\n",
    "\n",
    "question = \"What is a square root of the current US president’s age multiplied by 132?\"\n",
    "\n",
    "result = graph.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b9862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is a square root of the current US president’s age multiplied by 132?', additional_kwargs={}, response_metadata={}, id='252f76d5-8945-4f59-bb4c-52f00eced598'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_0ecd45fd84e94c60b146dfb3', 'function': {'arguments': '{\"query\": \"current US president age\"}', 'name': 'google_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 422, 'total_tokens': 446, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-b4e522da-d62a-4068-b28e-a04e9dbb83eb', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--460f9d79-ff6f-4a95-8b1d-6012be205a61-0', tool_calls=[{'name': 'google_search', 'args': {'query': 'current US president age'}, 'id': 'call_0ecd45fd84e94c60b146dfb3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 422, 'output_tokens': 24, 'total_tokens': 446, 'input_token_details': {}, 'output_token_details': {}})]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm, tools=[search_tool, calculator_tool], prompt=system_prompt\n",
    ")\n",
    "\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7f8bf",
   "metadata": {},
   "source": [
    "## Defining tools\n",
    "\n",
    "A LangChain tool has three essential components:\n",
    "- `Name`: A unique identifier for the tool\n",
    "- `Description`: Text that helps the LLM understand when and how to use the tool\n",
    "- `Payload schema`: A structured definition of the inputs the tool accepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d7c66",
   "metadata": {},
   "source": [
    "### Built-in LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b144a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m⠼\u001b[0m \u001b[2m                                                                              \u001b[0m\u001b[2mResolved \u001b[1m17 packages\u001b[0m \u001b[2min 732ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/8)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/8)--------------\u001b[0m\u001b[0m     0 B/12.70 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/8)----------\u001b[2m\u001b[0m\u001b[0m 12.70 KiB/12.70 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/8)0/8)                                                   \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/8)--------------\u001b[0m\u001b[0m     0 B/33.55 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.89 KiB/33.55 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.89 KiB/33.55 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.89 KiB/33.55 KiB         \u001b[1A\n",
      "\u001b[2mhpack               \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.89 KiB/33.55 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m     0 B/60.33 KiB           \u001b[2A\n",
      "\u001b[2mhpack               \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.89 KiB/33.55 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.88 KiB/60.33 KiB         \u001b[2A\n",
      "\u001b[2mhpack               \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 30.89 KiB/33.55 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.88 KiB/60.33 KiB         \u001b[2A\n",
      "\u001b[2mhpack               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.55 KiB/33.55 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.88 KiB/60.33 KiB         \u001b[2A\n",
      "\u001b[2mhpack               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.55 KiB/33.55 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.88 KiB/60.33 KiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 14.88 KiB/60.33 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m 30.88 KiB/60.33 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)----\u001b[2m------\u001b[0m\u001b[0m 46.88 KiB/60.33 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)----------\u001b[2m\u001b[0m\u001b[0m 60.33 KiB/60.33 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)1/8)                                                   \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/8)--------------\u001b[0m\u001b[0m     0 B/40.60 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 14.03 KiB/40.60 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 14.03 KiB/40.60 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 14.03 KiB/40.60 KiB         \u001b[1A\n",
      "\u001b[2msocksio             \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/12.46 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 14.03 KiB/40.60 KiB         \u001b[2A\n",
      "\u001b[2msocksio             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 12.46 KiB/12.46 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 14.03 KiB/40.60 KiB         \u001b[2A\n",
      "\u001b[2msocksio             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 12.46 KiB/12.46 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 14.03 KiB/40.60 KiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 14.03 KiB/40.60 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)---\u001b[2m-------\u001b[0m\u001b[0m 30.03 KiB/40.60 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)---\u001b[2m-------\u001b[0m\u001b[0m 30.03 KiB/40.60 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m     0 B/3.13 MiB            \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 16.00 KiB/3.13 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/8)--------------\u001b[0m\u001b[0m 48.00 KiB/3.13 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 78.04 KiB/3.13 MiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 126.04 KiB/3.13 MiB         \u001b[1A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 188.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m     0 B/5.01 MiB            \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 220.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 16.00 KiB/5.01 MiB          \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 252.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 47.89 KiB/5.01 MiB          \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 80.00 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 284.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 64.00 KiB/5.01 MiB          \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 128.00 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 332.78 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 112.00 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 144.00 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 348.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 144.00 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 208.00 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 396.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 192.00 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 249.81 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 460.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 249.81 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 281.81 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 492.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 265.81 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 329.81 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 556.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 345.81 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 425.81 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 604.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 409.81 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 489.71 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 668.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 489.81 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 521.81 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 716.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 537.81 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 617.71 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 796.88 KiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 617.81 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 642.38 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 1.00 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 642.38 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 642.38 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 1.01 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 927.88 KiB/5.01 MiB         \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 674.38 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 1.11 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 1.00 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 908.19 KiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 1.18 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 1.03 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 1.04 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 1.34 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 1.19 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 1.14 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 1.45 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 1.30 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 1.32 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 1.64 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 1.48 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 1.54 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 1.85 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 1.69 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 1.71 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 1.99 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 1.88 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 2.00 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 2.19 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 2.00 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 2.22 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 2.67 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 2.19 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mbrotli              \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 2.67 MiB/2.78 MiB\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 2.67 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)--------------\u001b[0m\u001b[0m 2.65 MiB/5.01 MiB           \u001b[3A\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 2.87 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)2m------------\u001b[0m\u001b[0m 2.85 MiB/5.01 MiB           \u001b[2A\n",
      "\u001b[2mprimp               \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 3.00 MiB/3.13 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)2m------------\u001b[0m\u001b[0m 2.98 MiB/5.01 MiB           \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/8)[2m-----------\u001b[0m\u001b[0m 3.02 MiB/5.01 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (7/8)---\u001b[2m-------\u001b[0m\u001b[0m 3.83 MiB/5.01 MiB           \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m8 packages\u001b[0m \u001b[2min 2.07s\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/9] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m9 packages\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbrotli\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mddgs\u001b[0m\u001b[2m==9.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh2\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhpack\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhyperframe\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprimp\u001b[0m\u001b[2m==0.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msocksio\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install ddgs~=9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac4ce891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool's name = duckduckgo_search\n",
      "Tool's name = A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "Tool's arg schema = <class 'langchain_community.tools.ddg_search.tool.DDGInput'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun(api_wrapper_kwargs={\"backend\": \"api\"})\n",
    "print(f\"Tool's name = {search.name}\")\n",
    "print(f\"Tool's name = {search.description}\")\n",
    "print(f\"Tool's arg schema = {search.args_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc197587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': FieldInfo(annotation=str, required=True, description='search query to look up')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.ddg_search.tool import DDGInput\n",
    "\n",
    "\n",
    "DDGInput.model_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09c332db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Munich , Bavaria, Germany Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days. 3 days ago · Get the latest hourly weather updates for Munich tomorrow . Detailed forecast including temperature, wind, rain, snow, and UV index. Stay informed about tomorrow 's weather conditions in Munich . 5 days ago · Munich , Germany - Detailed weather forecast for tomorrow . Hourly forecast for tomorrow - including weather conditions, temperature, pressure, humidity, precipitation, dewpoint, wind, visibility, and UV index data. 6 days ago · Detailed weather forecast ⚡ in Munich , Bavaria today, tomorrow and 7 days. Wind, precipitation, 🌡️ air temperature, clouds and atmospheric pressure - World-Weather.info Oct 24, 2025 · Latest weather forecast for Munich for tomorrow 's, hourly weather forecast, including tomorrow 's temperatures in Munich , wind, rain and more. 1 day ago · Get the detailed weather forecast for Munich , Bavaria, Germany. Access daily, 10 day and 16 day views, maps, meteograms and timelines for accurate predicitions. 5 days ago · Munich Extended Forecast with high and low temperatures °F Last 2 weeks of weather See weather overview\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the weather in Munich like tomorrow?\"\n",
    "search_input = DDGInput(query=query)\n",
    "search.invoke(search_input.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d7e9484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_b1d15243ec894442a7da7b22', 'function': {'arguments': '{\"query\": \"weather in Munich tomorrow\"}', 'name': 'duckduckgo_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 295, 'total_tokens': 321, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-2d30531f-8222-4e00-88dc-f0197b35cf14', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--771906ba-fafd-4d21-aec6-592826144736-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'weather in Munich tomorrow'}, 'id': 'call_b1d15243ec894442a7da7b22', 'type': 'tool_call'}], usage_metadata={'input_tokens': 295, 'output_tokens': 26, 'total_tokens': 321, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm.invoke(query, tools=[search]) 会报 search 不是合法的 json 对象。\n",
    "llm.bind_tools([search]).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de91aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.bind_tools([search]).invoke(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Always use a duckduckgo_search tool for queries that require a fresh information\",\n",
    "        ),\n",
    "        (\"user\", \"How much is 2+2?\"),\n",
    "    ]\n",
    ")\n",
    "assert not result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "049b56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[search],\n",
    "    prompt=\"Always use a duckduckgo_search tool for queries that require a fresh information\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb77ef72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b5fc18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  duckduckgo_search (call_eb302091849b4a76a32b83d6)\n",
      " Call ID: call_eb302091849b4a76a32b83d6\n",
      "  Args:\n",
      "    query: weather in Munich tomorrow\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: duckduckgo_search\n",
      "\n",
      "Munich , Bavaria, Germany Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days. Be prepared with the most accurate 10-day forecast for Múnich , Bavaria, Germany with highs, lows, chance of precipitation from The Weather Channel and Weather.com 3 days ago · Get the latest hourly weather updates for Munich tomorrow . Detailed forecast including temperature, wind, rain, snow, and UV index. Stay informed about tomorrow 's weather conditions in Munich . 5 days ago · Munich , Germany - Detailed weather forecast for tomorrow . Hourly forecast for tomorrow - including weather conditions, temperature, pressure, humidity, precipitation, dewpoint, wind, visibility, and UV index data. Oct 24, 2025 · Latest weather forecast for Munich for tomorrow 's, hourly weather forecast, including tomorrow 's temperatures in Munich , wind, rain and more. 6 days ago · Detailed weather forecast ⚡ in Munich , Bavaria today, tomorrow and 7 days. Wind, precipitation, 🌡️ air temperature, clouds and atmospheric pressure - World-Weather.info. 14-day weather forecast for Munich .\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It seems that the search results provide general information about weather forecasting services for Munich but don't include specific details about tomorrow's weather. To get the most accurate and up-to-date forecast, I recommend checking a trusted weather website like [Weather.com](https://weather.com) or using a weather app. Would you like me to guide you further?\n"
     ]
    }
   ],
   "source": [
    "for event in agent.stream({\"messages\": [(\"user\", query)]}):\n",
    "    messages = event.get(\"agent\", event.get(\"tools\", {})).get(\"messages\", [])\n",
    "    for m in messages:\n",
    "        m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67514217",
   "metadata": {},
   "source": [
    "Other types of tools\n",
    "- Tools that enhance the LLM’s knowledge. e.g. Knowledge bases: Wikipedia and Wikidata\n",
    "- Tools that enhance your productivity. e.g. GmailToolkit\n",
    "- Tools that give an LLM access to a code interpreter. e.g. Code execution: Python REPL and Bash\n",
    "- Tools that give an LLM access to databases by writing and executing SQL code. e.g. SQLDatabase\n",
    "- Tools for using other AI systems or automation. e.g. Hugging Face Hub\n",
    "\n",
    "When integrating such tools with LangChain, consider these key aspects:\n",
    "1. Authentication: Secure access to the external system\n",
    "1. Payload schema: Define proper data structures for input/output\n",
    "1. Error handling: Plan for failures and edge cases\n",
    "1. Safety considerations: For example, when developing a SQL-to-text agent, restrict access to read-only operations to prevent unintended modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07d99fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests_get\n",
      "requests_post\n",
      "requests_patch\n",
      "requests_put\n",
      "requests_delete\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n",
    "\n",
    "toolkit = RequestsToolkit(\n",
    "    requests_wrapper=TextRequestsWrapper(headers={}),\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "for tool in toolkit.get_tools():\n",
    "    print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b604141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_2390820/2022118673.py:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  pattern: '^\\d{4}-\\d{2}-\\d{2}$' # YYYY-MM-DD format\n"
     ]
    }
   ],
   "source": [
    "api_spec = \"\"\"\n",
    "openapi: 3.0.0\n",
    "info:\n",
    "  title: Frankfurter Currency Exchange API\n",
    "  version: v1\n",
    "  description: API for retrieving currency exchange rates. Pay attention to the base currency and change it if needed.\n",
    "\n",
    "servers:\n",
    "  - url: https://api.frankfurter.dev/v1\n",
    "\n",
    "paths:\n",
    "  /v1/{date}:\n",
    "    get:\n",
    "      summary: Get exchange rates for a specific date.\n",
    "      parameters:\n",
    "        - in: path\n",
    "          name: date\n",
    "          schema:\n",
    "            type: string\n",
    "            pattern: '^\\d{4}-\\d{2}-\\d{2}$' # YYYY-MM-DD format\n",
    "          required: true\n",
    "          description: The date for which to retrieve exchange rates.  Use YYYY-MM-DD format.  Example: 2009-01-04\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: GBP,USD,EUR\n",
    "\n",
    "  /v1/latest:\n",
    "    get:\n",
    "      summary: Get the latest exchange rates.\n",
    "      parameters:\n",
    "        - in: query\n",
    "          name: symbols\n",
    "          schema:\n",
    "            type: string\n",
    "          description: Comma-separated list of currency symbols to retrieve rates for. Example: CHF,GBP\n",
    "        - in: query\n",
    "          name: base\n",
    "          schema:\n",
    "            type: string\n",
    "          description: The base currency for the exchange rates. If not provided, EUR is used as a base currency. Example: USD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "578335d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the swiss franc to US dollar exchange rate?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  requests_get (call_c2470c8b57b64cd48d262f90)\n",
      " Call ID: call_c2470c8b57b64cd48d262f90\n",
      "  Args:\n",
      "    url: https://api.frankfurter.dev/v1/latest?base=CHF&symbols=USD\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: requests_get\n",
      "\n",
      "{\"amount\":1.0,\"base\":\"CHF\",\"date\":\"2025-10-30\",\"rates\":{\"USD\":1.2445}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current exchange rate from Swiss Franc (CHF) to US Dollar (USD) is **1 CHF = 1.2445 USD**.\n"
     ]
    }
   ],
   "source": [
    "system_message = (\n",
    "    \"You're given the API spec:\\n{api_spec}\\n\"\n",
    "    \"Use the API to answer users' queries if possible. \"\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm, toolkit.get_tools(), prompt=system_message.format(api_spec=api_spec)\n",
    ")\n",
    "\n",
    "query = \"What is the swiss franc to US dollar exchange rate?\"\n",
    "\n",
    "events = agent.stream(\n",
    "    {\"messages\": [(\"user\", query)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee703b01",
   "metadata": {},
   "source": [
    "### Custom tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6f629",
   "metadata": {},
   "source": [
    "#### Wrapping a Python function as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73b701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 382ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/433.23 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 16.00 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 16.00 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 32.00 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 48.00 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 61.56 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 77.56 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 77.56 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 93.56 KiB/433.23 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 109.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 125.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 141.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 157.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 173.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 173.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 189.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 205.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 221.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m-------------\u001b[0m\u001b[0m 237.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)2m------------\u001b[0m\u001b[0m 253.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)[2m-----------\u001b[0m\u001b[0m 269.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\u001b[2m----------\u001b[0m\u001b[0m 285.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-\u001b[2m---------\u001b[0m\u001b[0m 301.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[2m--------\u001b[0m\u001b[0m 317.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----\u001b[2m------\u001b[0m\u001b[0m 333.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----\u001b[2m------\u001b[0m\u001b[0m 333.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----\u001b[2m-----\u001b[0m\u001b[0m 349.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)------\u001b[2m----\u001b[0m\u001b[0m 365.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------\u001b[2m---\u001b[0m\u001b[0m 381.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------\u001b[2m--\u001b[0m\u001b[0m 397.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---------\u001b[2m-\u001b[0m\u001b[0m 413.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[2m\u001b[0m\u001b[0m 429.56 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[2m\u001b[0m\u001b[0m 433.23 KiB/433.23 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 867ms\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 18ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumexpr\u001b[0m\u001b[2m==2.14.1\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install numexpr~=2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697ee111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4, dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numexpr as ne\n",
    "import math\n",
    "\n",
    "math_constants = ({\"pi\": math.pi, \"i\": 1j, \"e\": math.exp},)\n",
    "ne.evaluate((\"2+2\"), local_dict=math_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0507ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
    "\n",
    "    Always add * to operations, examples:\n",
    "      73i -> 73*i\n",
    "      7pi**2 -> 7*pi**2\n",
    "    \"\"\"\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca9edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name: calculator\n",
      "Tool description: Calculates a single mathematical expression, incl. complex numbers.\n",
      "\n",
      "    Always add * to operations, examples:\n",
      "      73i -> 73*i\n",
      "      7pi**2 -> 7*pi**2\n",
      "Tool schema: {'description': 'Calculates a single mathematical expression, incl. complex numbers.\\n\\nAlways add * to operations, examples:\\n  73i -> 73*i\\n  7pi**2 -> 7*pi**2', 'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "\n",
    "assert isinstance(calculator, BaseTool)\n",
    "\n",
    "print(f\"Tool name: {calculator.name}\")\n",
    "print(f\"Tool description: {calculator.description}\")\n",
    "print(f\"Tool schema: {calculator.args_schema.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5491aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How much is 2+3i squared?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculator (call_5552135be4aa4610bca5d82f)\n",
      " Call ID: call_5552135be4aa4610bca5d82f\n",
      "  Args:\n",
      "    expression: (2+3*i)**2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculator\n",
      "\n",
      "(-5+12j)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The square of $ 2 + 3i $ is $ -5 + 12i $.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "query = \"How much is 2+3i squared?\"\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "agent = create_react_agent(llm, [calculator])\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", query)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ef452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is a square root of the current US president's age multiplied by 132?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To solve this, I need to find out the current U.S. president's age. Since it's 2025 and recent U.S. presidential elections have occurred, I will first determine who the current president is and their age.\n",
      "Tool Calls:\n",
      "  duckduckgo_search (call_948211259ee144a494982231)\n",
      " Call ID: call_948211259ee144a494982231\n",
      "  Args:\n",
      "    query: current US president 2025 age\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: duckduckgo_search\n",
      "\n",
      "List of presidents of the United States by age The first table below charts the age of each president of the United States at the time of their presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. See the age spectrum of living U.S . presidents following Jimmy Carter's passing, from Joe Biden, the eldest, to Barack Obama, the youngest. The 44th president is now the youngest president alive. He was also the fourth youngest presidents to enter office at the age of 47. How old is Joe Biden? Biden, 82, was born on November 20, 1942. Trump, 79, is the oldest person to have taken the oath of office as president . He was sworn in at 78 years and 220 days old in 2025 . The second-oldest person to become president was Trump's ... Who are the oldest living U.S . presidents ? The oldest living president is Biden, who turned 82 on Nov. 20, 2024, just weeks after the election. Mr. Trump, who turned 79 on June 14, 2025 , is second ...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the search results, Donald Trump is the current U.S. president as of 2025, and he is 79 years old.\n",
      "\n",
      "Now I will calculate the square root of his age multiplied by 132:\n",
      "\n",
      "$$\n",
      "\\sqrt{79 \\times 132}\n",
      "$$\n",
      "Tool Calls:\n",
      "  calculator (call_3515e6acc2b8480498e23891)\n",
      " Call ID: call_3515e6acc2b8480498e23891\n",
      "  Args:\n",
      "    expression: sqrt(79*132)\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculator\n",
      "\n",
      "102.11757928975794\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The square root of the current U.S. president's age (79) multiplied by 132 is approximately **102.12**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "question = \"What is a square root of the current US president's age multiplied by 132?\"\n",
    "\n",
    "system_hint = \"Think step-by-step. Always use search to get the fresh information about events or public facts that can change over time. Now is 2025 and remember president elections in the US recently happened.\"\n",
    "\n",
    "agent = create_react_agent(llm, [calculator, search], prompt=system_hint)\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"user\", question)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729a9ec",
   "metadata": {},
   "source": [
    "#### Creating a tool from a Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0d742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import convert_runnable_to_tool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18513a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78a9251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_60e95af3251641dd80aa00b1', 'function': {'arguments': '{\"__arg1\": \"(2+3*i)**2\"}', 'name': 'calculator'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 309, 'total_tokens': 337, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-3f55878f-e1b9-40d0-8fc9-d63c544fa7c0', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b8fcdfd0-d43e-4e3c-9a15-f779f7525c03-0', tool_calls=[{'name': 'calculator', 'args': {'__arg1': '(2+3*i)**2'}, 'id': 'call_60e95af3251641dd80aa00b1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 309, 'output_tokens': 28, 'total_tokens': 337, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原书的错误调用方式：\n",
    "#   llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n",
    "# 具体错误提示片段为：\n",
    "#  Unable to serialize unknown type: <class 'method'>\n",
    "\n",
    "llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745d8742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name: calculator\n",
      "Tool description: Calculates a single mathematical expression, incl. complex numbers.'\n",
      "Always add * to operations, examples:\n",
      "73i -> 73*i\n",
      "7pi**2 -> 7*pi**2\n",
      "Args schema: {'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool, convert_runnable_to_tool\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    expression: str = Field(description=\"Mathematical expression to be evaluated\")\n",
    "\n",
    "\n",
    "def calculator(state: CalculatorArgs, config: RunnableConfig) -> str:\n",
    "    expression = state[\"expression\"]\n",
    "    math_constants = config[\"configurable\"].get(\"math_constants\", {})\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")\n",
    "\n",
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    args_schema=CalculatorArgs,\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")\n",
    "\n",
    "assert isinstance(calculator_tool, BaseTool)\n",
    "\n",
    "print(f\"Tool name: {calculator_tool.name}\")\n",
    "print(f\"Tool description: {calculator_tool.description}\")\n",
    "print(f\"Args schema: {calculator_tool.args_schema.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb5c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'calculator', 'args': {'expression': '(2+3*i)**2'}, 'id': 'call_2b9b34b3c954444e8f8c0db4', 'type': 'tool_call'}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "config = {\"configurable\": {\"math_constants\": math_constants}}\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# 错误调用方式：tool_call = llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n",
    "tool_call = (\n",
    "    llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\").tool_calls[0]\n",
    ")\n",
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b53efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(-5+12j)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculator_tool.invoke(tool_call[\"args\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666c3d5",
   "metadata": {},
   "source": [
    "#### Subclass StructuredTool or BaseTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cea617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'calculator',\n",
       " 'args': {'expression': '(2+3j)**2'},\n",
       " 'id': 'call_c763e6537fda4dc89487278a',\n",
       " 'type': 'tool_call'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_tool = StructuredTool.from_function(\n",
    "    name=\"calculator\",\n",
    "    description=(\"Calculates a single mathematical expression, incl. complex numbers.\"),\n",
    "    func=calculator,\n",
    "    args_schema=CalculatorArgs,\n",
    ")\n",
    "\n",
    "llm.bind_tools([calculator_tool]).invoke(\"How much is (2+3i)**2\").tool_calls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4974cd1",
   "metadata": {},
   "source": [
    "### Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b3686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How much is (2+3i)^2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculator (call_4bf1b21f2aa64eff80b8d943)\n",
      " Call ID: call_4bf1b21f2aa64eff80b8d943\n",
      "  Args:\n",
      "    expression: (2+3i)^2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculator\n",
      "\n",
      "Error: SyntaxError('invalid decimal literal', ('<expr>', 1, 4, '(2+3i)^2', 1, 4))\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculator (call_fb00a4207eb148e3a516d9ed)\n",
      " Call ID: call_fb00a4207eb148e3a516d9ed\n",
      "  Args:\n",
      "    expression: (2+3j)**2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculator\n",
      "\n",
      "(-5+12j)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The square of $ (2 + 3i) $ is $ -5 + 12i $.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\"\"\"\n",
    "    return str(ne.evaluate(expression.strip(), local_dict={}))\n",
    "\n",
    "\n",
    "calculator_tool = StructuredTool.from_function(func=calculator, handle_tool_error=True)\n",
    "\n",
    "agent = create_react_agent(llm, [calculator_tool])\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"How much is (2+3i)^2\")]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfaaf4",
   "metadata": {},
   "source": [
    "## Advanced tool-calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e02899",
   "metadata": {},
   "source": [
    "## Incorporating tools into workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dea58",
   "metadata": {},
   "source": [
    "### Controlled generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ee2b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d90f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Prepare a step-by-step plan to solve the given task. \"\n",
    "    \"{format_instructions}.\"\n",
    "    \"TASK:\\n{task}\\n\"\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "\n",
    "result = (prompt | llm.with_structured_output(Plan)).invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6d62ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of steps: 11\n",
      "Research the market to identify gaps and popular topics in generative AI books on Amazon.\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(result, Plan)\n",
    "\n",
    "print(f\"Amount of steps: {len(result.steps)}\")\n",
    "\n",
    "for step in result.steps:\n",
    "    print(step)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2a441",
   "metadata": {},
   "source": [
    "#### Controlled generation provided by the vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aa16e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Prepare a step-by-step plan to solve the given task.\\n\"\n",
    "    \"Output the planned steps as a JSON array.\\n\"\n",
    "    \"TASK:\\n{task}\\n\"\n",
    ")\n",
    "\n",
    "plan_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"step\": {\"type\": \"STRING\"},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "query = \"How to write a bestseller on Amazon about generative AI?\"\n",
    "\n",
    "result = (\n",
    "    prompt | llm.with_structured_output(schema=plan_schema, method=\"json_mode\")\n",
    ").invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f5006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of steps: 11\n",
      "{'step': 1, 'title': 'Define Your Target Audience', 'description': 'Identify who your book is for—e.g., beginners, developers, business professionals, or creatives—and understand their pain points, interests, and knowledge level regarding generative AI.'}\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(result, list)\n",
    "print(f\"Amount of steps: {len(result)}\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa64677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pydantic\n",
      "Version: 2.12.3\n",
      "Location: /github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages\n",
      "Requires: annotated-types, pydantic-core, typing-extensions, typing-inspection\n",
      "Required-by: langchain, langchain-core, langgraph, langsmith, openai, pydantic-settings\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip show pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c110c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import TypeAdapter\n",
    "\n",
    "\n",
    "class ReviewTone(str, Enum):\n",
    "    positive = \"positive\"\n",
    "    negative = \"negative\"\n",
    "    neutral = \"neutral\"\n",
    "\n",
    "\n",
    "model_json_schema = TypeAdapter(ReviewTone).json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "363ab585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"enum\": [\n",
      "    \"positive\",\n",
      "    \"negative\",\n",
      "    \"neutral\"\n",
      "  ],\n",
      "  \"title\": \"ReviewTone\",\n",
      "  \"type\": \"string\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(model_json_schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cdbc474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tone': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Classify the tone of the following customer's review:\"\n",
    "    \"\\n{review}\\n\"\n",
    "    \"Output in JSON.\"\n",
    ")\n",
    "\n",
    "review = \"I like this movie!\"\n",
    "llm_enum = (\n",
    "    Config()\n",
    "    .new_openai_like()\n",
    "    .with_structured_output(schema=model_json_schema, method=\"json_mode\")\n",
    ")\n",
    "result = (prompt | llm_enum).invoke(review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b6606",
   "metadata": {},
   "source": [
    "### ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e4b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numexpr as ne\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n",
    "\n",
    "    Always add * to operations, examples:\n",
    "      73i -> 73*i\n",
    "      7pi**2 -> 7*pi**2\n",
    "    \"\"\"\n",
    "    math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "llm_with_tools = Config().new_openai_like().bind_tools([search, calculator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "532d7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "\n",
    "def invoke_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"invoke_llm\", invoke_llm)\n",
    "builder.add_node(\"tools\", ToolNode([search, calculator]))\n",
    "\n",
    "builder.add_edge(START, \"invoke_llm\")\n",
    "builder.add_conditional_edges(\"invoke_llm\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"invoke_llm\")\n",
    "# builder.add_edge(\"tools\", END)\n",
    "builder.add_edge(\"invoke_llm\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "105cf623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e945d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'invoke_llm': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_db11bb1bee844c1f87044533', 'function': {'arguments': '{\"expression\": \"2+2\"}', 'name': 'calculator'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 389, 'total_tokens': 412, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-35d86d3e-5e21-4bef-bfbb-70cc641eaab2', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--df9ec622-2e73-400f-b1b8-8a03f008d303-0', tool_calls=[{'name': 'calculator', 'args': {'expression': '2+2'}, 'id': 'call_db11bb1bee844c1f87044533', 'type': 'tool_call'}], usage_metadata={'input_tokens': 389, 'output_tokens': 23, 'total_tokens': 412, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='4', name='calculator', id='aa67998d-8c41-4d21-bdae-22ffec414c5c', tool_call_id='call_db11bb1bee844c1f87044533')]}}\n",
      "{'invoke_llm': {'messages': [AIMessage(content='2 + 2 equals 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 428, 'total_tokens': 436, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-ce81fa8b-0e42-4fbf-90f1-535e35e3cb7e', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--47896f3e-a215-4b1c-915b-e9091e562a58-0', usage_metadata={'input_tokens': 428, 'output_tokens': 8, 'total_tokens': 436, 'input_token_details': {}, 'output_token_details': {}})]}}\n"
     ]
    }
   ],
   "source": [
    "for e in graph.stream({\"messages\": (\"human\", \"How much is 2+2\")}):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8931a19",
   "metadata": {},
   "source": [
    "### Tool-calling paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77f788db",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"I signed my contract 2 years ago\",\n",
    "    \"I started the deal with your company in February last year\",\n",
    "    \"Our contract started on March 24th two years ago\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce6f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_date(year: int, month: int = 1, day: int = 1) -> date:\n",
    "    \"\"\"Returns a date object given year, month and day.\n",
    "\n",
    "    Default month and day are 1 (January) and 1.\n",
    "    Examples in YYYY-MM-DD format:\n",
    "      2023-07-27 -> date(2023, 7, 27)\n",
    "      2022-12-15 -> date(2022, 12, 15)\n",
    "      March 2022 -> date(2022, 3)\n",
    "      2021 -> date(2021)\n",
    "    \"\"\"\n",
    "    return date(year, month, day).isoformat()\n",
    "\n",
    "\n",
    "@tool\n",
    "def time_difference(\n",
    "    days: int = 0, weeks: int = 0, months: int = 0, years: int = 0\n",
    ") -> date:\n",
    "    \"\"\"Returns a date given a difference in days, weeks, months and years relative to the current date.\n",
    "\n",
    "    By default, dayss, weeks, months and years are 0.\n",
    "    Examples:\n",
    "      two weeks ago -> time_difference(weeks=2)\n",
    "      last year -> time_difference(years=1)\n",
    "    \"\"\"\n",
    "    dt = date.today() - timedelta(days=days, weeks=weeks)\n",
    "    new_year = dt.year + (dt.month - months) // 12 - years\n",
    "    new_month = (dt.month - months) % 12\n",
    "    return dt.replace(year=new_year, month=new_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce08f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I signed my contract 2 years ago The starting date of your contract is November 1, 2023.\n",
      "\n",
      "I started the deal with your company in February last year The contract started in February 2024.\n",
      "\n",
      "Our contract started on March 24th two years ago The contract started on March 24, 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    [get_date, time_difference],\n",
    "    prompt=\"Extract the starting date of a contract. Current year is 2025.\",\n",
    ")\n",
    "\n",
    "\n",
    "for example in examples:\n",
    "    result = agent.invoke({\"messages\": [(\"user\", example)]})\n",
    "    print(example, result[\"messages\"][-1].content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5959957",
   "metadata": {},
   "source": [
    "## What are agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed094a3",
   "metadata": {},
   "source": [
    "### Plan-and-solve agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4379fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "\n",
    "system_prompt_template = (\n",
    "    \"For the given task, come up with a step by step plan.\\n\"\n",
    "    \"This plan should involve individual tasks, that if executed correctly will \"\n",
    "    \"yield the correct answer in JSON format and recorded in field 'steps'.\\n\"\n",
    "    \"Do not add any superfluous steps.\\n\"\n",
    "    \"The result of the final step should be the final answer. Make sure that each \"\n",
    "    \"step has all the information needed - do not skip steps.\"\n",
    ")\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like(temperature=1.0)\n",
    "\n",
    "planner = planner_prompt | llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456e72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "from langchain_core.tools import tool, convert_runnable_to_tool\n",
    "\n",
    "\n",
    "class CalculatorArgs(BaseModel):\n",
    "    expression: str = Field(description=\"Mathematical expression to be evaluated\")\n",
    "\n",
    "\n",
    "def calculator(state: CalculatorArgs, config: RunnableConfig) -> str:\n",
    "    expression = state[\"expression\"]\n",
    "    math_constants = config[\"configurable\"].get(\"math_constants\", {})\n",
    "    result = ne.evaluate(expression.strip(), local_dict=math_constants)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "calculator_with_retry = RunnableLambda(calculator).with_retry(\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=3,\n",
    ")\n",
    "\n",
    "calculator_tool = convert_runnable_to_tool(\n",
    "    calculator_with_retry,\n",
    "    name=\"calculator\",\n",
    "    description=(\n",
    "        \"Calculates a single mathematical expression, incl. complex numbers.\"\n",
    "        \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n",
    "        \"7pi**2 -> 7*pi**2\"\n",
    "    ),\n",
    "    args_schema=CalculatorArgs,\n",
    "    arg_types={\"expression\": \"str\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "462d00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 1.23s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                           \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m     0 B/103.90 KiB          \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m     0 B/103.90 KiB          \u001b[2A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m     0 B/103.90 KiB          \u001b[3A\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[3A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m 14.84 KiB/103.90 KiB        \u001b[3A\n",
      "\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[3A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m------\u001b[0m\u001b[0m 14.84 KiB/103.90 KiB        \u001b[3A\n",
      "\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.88 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)--------------\u001b[0m\u001b[0m 14.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.88 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)--------------\u001b[0m\u001b[0m 30.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 14.88 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)--------------\u001b[0m\u001b[0m 30.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 30.88 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)--------------\u001b[0m\u001b[0m 30.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 30.88 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)--------------\u001b[0m\u001b[0m 46.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 35.82 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)--------------\u001b[0m\u001b[0m 46.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2msoupsieve           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 35.82 KiB/35.82 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)--------------\u001b[0m\u001b[0m 46.84 KiB/103.90 KiB        \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)--------------\u001b[0m\u001b[0m 46.84 KiB/103.90 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/3)[2m-----------\u001b[0m\u001b[0m 62.84 KiB/103.90 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)---\u001b[2m-------\u001b[0m\u001b[0m 78.84 KiB/103.90 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)---\u001b[2m-------\u001b[0m\u001b[0m 78.84 KiB/103.90 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)--------\u001b[2m--\u001b[0m\u001b[0m 94.84 KiB/103.90 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)----------\u001b[2m\u001b[0m\u001b[0m 103.90 KiB/103.90 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 621ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/6] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.14.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msoupsieve\u001b[0m\u001b[2m==2.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install arxiv==2.2.0 wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0d3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm) + [\n",
    "    calculator_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6258599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a smart assistant that carefully helps to solve complex tasks.\\n\"\n",
    "    \" Given a general plan to solve a task and a specific step, work on this step. \"\n",
    "    \" Don't assume anything, keep in minds things might change and always try to \"\n",
    "    \"use tools to double-check yourself.\\n\"\n",
    "    \" Use a calculator for mathematical computations, use Search to gather\"\n",
    "    \"for information about common facts, fresh events and news, use Arxiv to get \"\n",
    "    \"ideas on recent research and use Wikipedia for common knowledge.\"\n",
    ")\n",
    "\n",
    "step_template = (\n",
    "    \"Given the task and the plan, try to execute on a specific step of the plan.\\n\"\n",
    "    \"TASK:\\n{task}\\n\\nPLAN:\\n{plan}\\n\\nSTEP TO EXECUTE:\\n{step}\\n\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", step_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class StepState(AgentState):\n",
    "    plan: str\n",
    "    step: str\n",
    "    task: str\n",
    "\n",
    "\n",
    "execution_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    state_schema=StepState,\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af5a0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "import operator\n",
    "\n",
    "\n",
    "class PlanState(TypedDict):\n",
    "    task: str\n",
    "    plan: Plan\n",
    "    past_steps: Annotated[list[str], operator.add]\n",
    "    final_response: str\n",
    "\n",
    "\n",
    "def get_current_step(state: PlanState) -> int:\n",
    "    \"\"\"Returns the number of current step to be executed.\"\"\"\n",
    "    return len(state.get(\"past_steps\", []))\n",
    "\n",
    "\n",
    "def get_full_plan(state: PlanState) -> str:\n",
    "    \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n",
    "    full_plan = []\n",
    "    for i, step in enumerate(state[\"plan\"].steps):\n",
    "        full_step = f\"# {i+1}. Planned step: {step}\\n\"\n",
    "        if i < get_current_step(state):\n",
    "            full_step += f\"Result: {state['past_steps'][i]}\\n\"\n",
    "        full_plan.append(full_step)\n",
    "    return \"\\n\".join(full_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "246ee35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "async def _build_initial_plan(state: PlanState) -> PlanState:\n",
    "    print(f\"task: {state['task']}\")\n",
    "    plan = await planner.ainvoke(state[\"task\"])\n",
    "    print(f\"plan: {plan}\")\n",
    "    print(f\"plan.steps: {len(plan.steps)}\")\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "async def _run_step(state: PlanState) -> PlanState:\n",
    "    print(f\"state: {state}\")\n",
    "    plan = state[\"plan\"]\n",
    "    current_step = get_current_step(state)\n",
    "    step = await execution_agent.ainvoke(\n",
    "        {\n",
    "            \"plan\": get_full_plan(state),\n",
    "            \"step\": plan.steps[current_step],\n",
    "            \"task\": state[\"task\"],\n",
    "        }\n",
    "    )\n",
    "    return {\"past_steps\": [step[\"messages\"][-1].content]}\n",
    "\n",
    "\n",
    "async def _get_final_response(state: PlanState) -> PlanState:\n",
    "    final_response = await (final_prompt | llm).ainvoke(\n",
    "        {\"task\": state[\"task\"], \"plan\": get_full_plan(state)}\n",
    "    )\n",
    "    return {\"final_response\": final_response}\n",
    "\n",
    "\n",
    "def _should_continue(state: PlanState) -> Literal[\"run\", \"response\"]:\n",
    "    if get_current_step(state) < len(state[\"plan\"].steps):\n",
    "        return \"run\"\n",
    "    return \"response\"\n",
    "\n",
    "\n",
    "builder = StateGraph(PlanState)\n",
    "builder.add_node(\"initial_plan\", _build_initial_plan)\n",
    "builder.add_node(\"run\", _run_step)\n",
    "builder.add_node(\"response\", _get_final_response)\n",
    "\n",
    "builder.add_edge(START, \"initial_plan\")\n",
    "builder.add_edge(\"initial_plan\", \"run\")\n",
    "builder.add_conditional_edges(\"run\", _should_continue)\n",
    "builder.add_edge(\"response\", END)\n",
    "\n",
    "graph = builder.compile().with_config({\"recursion_limit\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f20109f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50156116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: Write a strategic one-pager of building an AI startup\n",
      "plan: steps=['Define the core objective and unique value proposition of the AI startup, including the specific problem it aims to solve and the target market.', 'Outline the key components of the business model, such as revenue streams, pricing strategy, and customer acquisition approach.', 'Identify the core AI technology or innovation that differentiates the startup, including data strategy, model architecture, and IP considerations.', 'Summarize the go-to-market strategy, highlighting initial use cases, pilot customers, and partnership opportunities.', 'Describe the founding team’s relevant expertise and any current traction (e.g., prototypes, early customers, funding).', 'Highlight key milestones and a high-level roadmap for product development, market expansion, and scaling over the next 12–24 months.', 'Synthesize all elements into a concise, compelling one-page document structured with clear headings: Problem, Solution, Technology, Market Opportunity, Business Model, Go-to-Market Strategy, Team, and Roadmap.']\n",
      "plan.steps: 7\n",
      "state: {'task': 'Write a strategic one-pager of building an AI startup', 'plan': Plan(steps=['Define the core objective and unique value proposition of the AI startup, including the specific problem it aims to solve and the target market.', 'Outline the key components of the business model, such as revenue streams, pricing strategy, and customer acquisition approach.', 'Identify the core AI technology or innovation that differentiates the startup, including data strategy, model architecture, and IP considerations.', 'Summarize the go-to-market strategy, highlighting initial use cases, pilot customers, and partnership opportunities.', 'Describe the founding team’s relevant expertise and any current traction (e.g., prototypes, early customers, funding).', 'Highlight key milestones and a high-level roadmap for product development, market expansion, and scaling over the next 12–24 months.', 'Synthesize all elements into a concise, compelling one-page document structured with clear headings: Problem, Solution, Technology, Market Opportunity, Business Model, Go-to-Market Strategy, Team, and Roadmap.']), 'past_steps': []}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m task = \u001b[33m\"\u001b[39m\u001b[33mWrite a strategic one-pager of building an AI startup\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# FIXME: 生成计划后，无法正确执行，反复报 'Sorry, need more steps to process this request.'。\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m graph.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: task})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3171\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3168\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3169\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3171\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3173\u001b[39m     config,\n\u001b[32m   3174\u001b[39m     context=context,\n\u001b[32m   3175\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3178\u001b[39m     print_mode=print_mode,\n\u001b[32m   3179\u001b[39m     output_keys=output_keys,\n\u001b[32m   3180\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3181\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3182\u001b[39m     durability=durability,\n\u001b[32m   3183\u001b[39m     **kwargs,\n\u001b[32m   3184\u001b[39m ):\n\u001b[32m   3185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3186\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2993\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2991\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2992\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2993\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2994\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2995\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2996\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2997\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2998\u001b[39m ):\n\u001b[32m   2999\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3000\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3001\u001b[39m         stream_mode,\n\u001b[32m   3002\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3005\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3006\u001b[39m     ):\n\u001b[32m   3007\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:474\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36m_run_step\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     24\u001b[39m plan = state[\u001b[33m\"\u001b[39m\u001b[33mplan\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     25\u001b[39m current_step = get_current_step(state)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m step = \u001b[38;5;28;01mawait\u001b[39;00m execution_agent.ainvoke(\n\u001b[32m     27\u001b[39m     {\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplan\u001b[39m\u001b[33m\"\u001b[39m: get_full_plan(state),\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m: plan.steps[current_step],\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     31\u001b[39m     }\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mpast_steps\u001b[39m\u001b[33m\"\u001b[39m: [step[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3171\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3168\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3169\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3171\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3173\u001b[39m     config,\n\u001b[32m   3174\u001b[39m     context=context,\n\u001b[32m   3175\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3178\u001b[39m     print_mode=print_mode,\n\u001b[32m   3179\u001b[39m     output_keys=output_keys,\n\u001b[32m   3180\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3181\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3182\u001b[39m     durability=durability,\n\u001b[32m   3183\u001b[39m     **kwargs,\n\u001b[32m   3184\u001b[39m ):\n\u001b[32m   3185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3186\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2993\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2991\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2992\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2993\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2994\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2995\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2996\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2997\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2998\u001b[39m ):\n\u001b[32m   2999\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3000\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3001\u001b[39m         stream_mode,\n\u001b[32m   3002\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3005\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3006\u001b[39m     ):\n\u001b[32m   3007\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:465\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m         run = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m         ret = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(coro, context=context)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    467\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:663\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.acall_model\u001b[39m\u001b[34m(state, runtime, config)\u001b[39m\n\u001b[32m    661\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m dynamic_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m static_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    666\u001b[39m response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3291\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3289\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3290\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3291\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3292\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3293\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5724\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5717\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5718\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5719\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5722\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5723\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5724\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5725\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5726\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5727\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5728\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:417\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    409\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m     **kwargs: Any,\n\u001b[32m    415\u001b[39m ) -> BaseMessage:\n\u001b[32m    416\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    418\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    419\u001b[39m         stop=stop,\n\u001b[32m    420\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    421\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    422\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    423\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    424\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    425\u001b[39m         **kwargs,\n\u001b[32m    426\u001b[39m     )\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1036\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1027\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1033\u001b[39m     **kwargs: Any,\n\u001b[32m   1034\u001b[39m ) -> LLMResult:\n\u001b[32m   1035\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1037\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1038\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:956\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    943\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    944\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    945\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    950\u001b[39m     run_id=run_id,\n\u001b[32m    951\u001b[39m )\n\u001b[32m    953\u001b[39m input_messages = [\n\u001b[32m    954\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    955\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    957\u001b[39m     *[\n\u001b[32m    958\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    959\u001b[39m             m,\n\u001b[32m    960\u001b[39m             stop=stop,\n\u001b[32m    961\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    962\u001b[39m             **kwargs,\n\u001b[32m    963\u001b[39m         )\n\u001b[32m    964\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    965\u001b[39m     ],\n\u001b[32m    966\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    967\u001b[39m )\n\u001b[32m    968\u001b[39m exceptions = []\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1164\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1162\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1163\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1165\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1166\u001b[39m     )\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1449\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1442\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1443\u001b[39m             response,\n\u001b[32m   1444\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1445\u001b[39m             metadata=generation_info,\n\u001b[32m   1446\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1447\u001b[39m         )\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1449\u001b[39m         raw_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.with_raw_response.create(\n\u001b[32m   1450\u001b[39m             **payload\n\u001b[32m   1451\u001b[39m         )\n\u001b[32m   1452\u001b[39m         response = raw_response.parse()\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2603\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2557\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2559\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2600\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2601\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2602\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2604\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2605\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2606\u001b[39m             {\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2621\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2622\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2623\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2625\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2626\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2627\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2628\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2629\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2630\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2631\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2632\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2633\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2634\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2635\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2636\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2637\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2638\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2639\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2640\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2641\u001b[39m             },\n\u001b[32m   2642\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2643\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2644\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2645\u001b[39m         ),\n\u001b[32m   2646\u001b[39m         options=make_request_options(\n\u001b[32m   2647\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2648\u001b[39m         ),\n\u001b[32m   2649\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2650\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2651\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2652\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_base_client.py:1529\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1527\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1530\u001b[39m         request,\n\u001b[32m   1531\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1532\u001b[39m         **kwargs,\n\u001b[32m   1533\u001b[39m     )\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1535\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:237\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:180\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1263\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1258\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1261\u001b[39m ):\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "task = \"Write a strategic one-pager of building an AI startup\"\n",
    "\n",
    "# qwen3-max-2025-09-11 无法正确生成答案，qwen3-max-2025-09-23 可以。\n",
    "# 运行时间很长。\n",
    "# TODO: 跑完整。\n",
    "result = await graph.ainvoke({\"task\": task})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-with-lang-chain-2ed (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
