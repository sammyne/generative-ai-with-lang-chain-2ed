{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e4ee78",
   "metadata": {},
   "source": [
    "# 04. Building Intelligent RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6998a93",
   "metadata": {},
   "source": [
    "## 0. 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43596ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m118 packages\u001b[0m \u001b[2min 334ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/19] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m19 packages\u001b[0m \u001b[2min 51ms\u001b[0m\u001b[0m=0.3.30                        \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.12.15\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgreenlet\u001b[0m\u001b[2m==3.2.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjq\u001b[0m\u001b[2m==1.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==0.3.27\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.3.30\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==0.3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.6.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.43\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.20.1\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install jq~=1.10 langchain-chroma~=0.2 langchain-community~=0.3 langchain-core~=0.3 langchain-openai~=0.3.0 python-dotenv transformers~=4.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306cc65",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "        embeddings_model = os.getenv(\"OPENAI_EMBEDDINGS_MODEL\")\n",
    "        hf_pretrained_embeddings_model = os.getenv(\"HF_PRETRAINED_EMBEDDINGS_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.hf_pretrained_embeddings_model = hf_pretrained_embeddings_model if hf_pretrained_embeddings_model else 'Qwen/Qwen3-Embedding-8B'\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_embeddings(self, **kwargs) -> OpenAIEmbeddings:\n",
    "        if not self.embeddings_model:\n",
    "            raise ValueError(\"OPENAI_EMBEDDINGS_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            model=self.embeddings_model,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.tiktoken_enabled\n",
    "            # 对于非 OpenAI 的官方实现，将这个参数置为 False。\n",
    "            # 回退到用 huggingface transformers 库 AutoTokenizer 来处理 token。\n",
    "            tiktoken_enabled=False,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.model\n",
    "            # 元宝说 Jina 的 embedding 模型 https://huggingface.co/jinaai/jina-embeddings-v4 最接近\n",
    "            # text-embedding-ada-002\n",
    "            # 个人喜好，选了 Qwen/Qwen3-Embedding-8B\n",
    "            # tiktoken_model_name='Qwen/Qwen3-Embedding-8B',\n",
    "            tiktoken_model_name=self.hf_pretrained_embeddings_model,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_vl(self, **kwargs) -> ChatOpenAI:\n",
    "        if not self.vl_model:\n",
    "            raise ValueError(\"OPENAI_VL_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.vl_model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda46add",
   "metadata": {},
   "source": [
    "## From indexes to intelligent retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1946a4",
   "metadata": {},
   "source": [
    "## Components of a RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9b54a",
   "metadata": {},
   "source": [
    "### When to implement RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5df81",
   "metadata": {},
   "source": [
    "## From embeddings to search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d48d4",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a79e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embeddings model\n",
    "embeddings_model = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Create embeddings for example sentences\n",
    "text1 = \"The cat sat on the mat\"\n",
    "text2 = \"A feline rested on the carpet\"\n",
    "text3 = \"Python is a programming language\"\n",
    "\n",
    "# Get embeddings using LangChain\n",
    "embeddings = embeddings_model.embed_documents([text1, text2, text3])\n",
    "\n",
    "# These similar sentences will have similar embeddings\n",
    "embedding1 = embeddings[0]  # Embedding for \"The cat sat on the mat\"\n",
    "embedding2 = embeddings[1]  # Embedding for \"A feline rested on the carpet\"\n",
    "embedding3 = embeddings[2]  # Embedding for \"Python is a programming language\"\n",
    "\n",
    "# Output shows number of documents and embedding dimensions\n",
    "print(f\"Number of documents: {len(embeddings)}\")\n",
    "print(f\"Dimensions per embedding: {len(embeddings[0])}\")\n",
    "# Typically 1536 dimensions with OpenAI's embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e21096",
   "metadata": {},
   "source": [
    "### Vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e5a5d",
   "metadata": {},
   "source": [
    "#### Vector stores comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11c408",
   "metadata": {},
   "source": [
    "#### Hardware considerations for vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622af3e7",
   "metadata": {},
   "source": [
    "#### Vector store interface in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize with an embedding model\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Create some sample documents with explicit IDs\n",
    "docs = [\n",
    "    Document(page_content=\"Content about language models\", metadata={\"id\": \"doc_1\"}),\n",
    "    Document(page_content=\"Information about vector databases\", metadata={\"id\": \"doc_2\"}),\n",
    "    Document(page_content=\"Details about retrieval systems\", metadata={\"id\": \"doc_3\"})\n",
    "]\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# Add documents with explicit IDs\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "# Similarity Search with appropriate k value\n",
    "results = vector_store.similarity_search(\"How do language models work?\", k=2)\n",
    "\n",
    "# For MMR, adjust the parameters based on available documents\n",
    "found_docs = vector_store.similarity_search(\"retrieval\", k=1)\n",
    "print(f\"Found documents: {len(found_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dddd24",
   "metadata": {},
   "source": [
    "### Vector indexing strategies\n",
    "\n",
    "faiss 库不支持 python3.12。google 的 ScaNN 库没找到接口文档。\n",
    "TODO：用 ScaNN 复现书中代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c65ab6",
   "metadata": {},
   "source": [
    "## Breaking down the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ca7d6",
   "metadata": {},
   "source": [
    "1. Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c934641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cece6",
   "metadata": {},
   "source": [
    "2. Make embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d203e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Config().new_openai_like_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79be16",
   "metadata": {},
   "source": [
    "3. Store in vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import ScaNN\n",
    "\n",
    "vector_db = ScaNN.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb3a9a2",
   "metadata": {},
   "source": [
    "4. Retrieve similar docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the effects of climate change?\"\n",
    "\n",
    "vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800c0e8",
   "metadata": {},
   "source": [
    "### Document processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d214ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea71d6",
   "metadata": {},
   "source": [
    "#### Chunking strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88b2a6",
   "metadata": {},
   "source": [
    "##### Fixed-size chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-text-splitters~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \", # Split on spaces to avoid breaking words\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Generated {len(chunks)} chunks from document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf5f45",
   "metadata": {},
   "source": [
    "##### Recursive character chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f942af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "document = \"\"\"# Introduction to RAG\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.\n",
    "\n",
    "It helps address hallucinations by grounding responses in retrieved information.\n",
    "\n",
    "## Key Components\n",
    "RAG consists of several components:\n",
    "1. Document processing\n",
    "2. Vector embedding\n",
    "3. Retrieval\n",
    "4. Augmentation\n",
    "5. Generation\n",
    "\n",
    "### Document Processing\n",
    "This step involves loading and chunking documents appropriately.\n",
    "\"\"\"\n",
    "\n",
    "text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa1e5a",
   "metadata": {},
   "source": [
    "##### Document-specific chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd464ed",
   "metadata": {},
   "source": [
    "##### Semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-experimental~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    add_start_index=True  # Include position metadata\n",
    ")\n",
    "\n",
    "text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31191d3",
   "metadata": {},
   "source": [
    "##### Agent-based chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874330f",
   "metadata": {},
   "source": [
    "##### Multi-modal chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bee5e7",
   "metadata": {},
   "source": [
    "##### Choosing the right chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8df8fa",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051ef66",
   "metadata": {},
   "source": [
    "##### LangChain retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434b109",
   "metadata": {},
   "source": [
    "##### Vector store retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import KNNRetriever\n",
    "\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "retriever = KNNRetriever.from_documents(documents, embeddings)\n",
    "retriever.invoke(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf898b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install xmltodict~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers.pubmed import PubMedRetriever\n",
    "\n",
    "retriever = PubMedRetriever()\n",
    "# FIXME: 没有跑通\n",
    "results = retriever.invoke(\"chatgpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba012bb8",
   "metadata": {},
   "source": [
    "### Advanced RAG techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e47228",
   "metadata": {},
   "source": [
    "#### Hybrid retrieval: Combining semantic and keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9af5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install rank-bm25~=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d51824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Setup semantic retriever\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Setup lexical retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# Combine retrievers\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.7, 0.3],  # Weight semantic search higher than keyword search\n",
    ")\n",
    "\n",
    "hybrid_retriever.get_relevant_documents(\"climate change impacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11623d",
   "metadata": {},
   "source": [
    "#### Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99054385",
   "metadata": {},
   "source": [
    "#### Query transformation: Improving retrieval through better queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "expansion_template = \"\"\"Given the user question: {question}\n",
    "Generate three alternative versions that express the same information need but with different wording:\n",
    "1.\"\"\"\n",
    "\n",
    "expansion_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=expansion_template\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like(temperature=0.7)\n",
    "expansion_chain = expansion_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate expanded queries\n",
    "original_query = \"What are the effects of climate change?\"\n",
    "reply = expansion_chain.invoke(original_query)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d8c0f",
   "metadata": {},
   "source": [
    "##### Hypothetical Document Embeddings (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Create prompt for generating hypothetical document\n",
    "hyde_template = \"\"\"Based on the question: {question}\n",
    "Write a passage that could contain the answer to this question:\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=hyde_template\n",
    ")\n",
    "llm = Config().new_openai_like(temperature=0.2)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate hypothetical document\n",
    "query = \"What dietary changes can reduce carbon footprint?\"\n",
    "hypothetical_doc = hyde_chain.invoke(query)\n",
    "\n",
    "# Use the hypothetical document for retrieval\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "embedded_query = embeddings.embed_query(hypothetical_doc)\n",
    "vector_db.similarity_search_by_vector(embedded_query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f268dce",
   "metadata": {},
   "source": [
    "#### Context processing: maximizing retrieved information value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e76400",
   "metadata": {},
   "source": [
    "##### Contextual compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = Config().new_openai_like(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Create a basic retriever from the vector store\n",
    "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "compression_retriever.invoke(\"How do transformers work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbea1dc",
   "metadata": {},
   "source": [
    "##### Maximum marginal relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e8a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import ScaNN\n",
    "\n",
    "vector_store = ScaNN.from_documents(documents, embeddings)\n",
    "\n",
    "# FIXME: max_marginal_relevance_search is not implemented in ScaNN\n",
    "vector_store.max_marginal_relevance_search(\n",
    "    query=\"What are transformer models?\",\n",
    "    k=5, # Number of documents to return\n",
    "    fetch_k=20, # Number of documents to initially fetch\n",
    "    lambda_mult=0.5 # Diversity parameter (0 = max diversity, 1 = max relevance)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e32d00",
   "metadata": {},
   "source": [
    "#### Response enhancement: Improving generator output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n",
    "        metadata={\"source\": \"Neural Network Review 2021\", \"page\": 42}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.\",\n",
    "        metadata={\"source\": \"Introduction to NLP\", \"page\": 137}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"GPT models are autoregressive transformers that predict the next token based on previous tokens.\",\n",
    "        metadata={\"source\": \"Large Language Models Survey\", \"page\": 89}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e181d5",
   "metadata": {},
   "source": [
    "##### Source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import ScaNN\n",
    "\n",
    "\n",
    "# Create a vector store and retriever\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "vector_store = ScaNN.from_documents(documents, embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Source attribution prompt template\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a precise AI assistant that provides well-sourced information.\n",
    "Answer the following question based ONLY on the provided sources. For each fact or claim in your answer,\n",
    "include a citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "Your answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e993e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create a source-formatted string from documents\n",
    "def format_sources_with_citations(docs):\n",
    "    formatted_sources = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source_info = f\"[{i}] {doc.metadata.get('source', 'Unknown source')}\"\n",
    "        if doc.metadata.get('page'):\n",
    "            source_info += f\", page {doc.metadata['page']}\"\n",
    "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted_sources)\n",
    "\n",
    "# Build the RAG chain with source attribution\n",
    "def generate_attributed_response(question):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    # Format sources with citation numbers\n",
    "    sources_formatted = format_sources_with_citations(retrieved_docs)\n",
    "    \n",
    "    # Create the attribution chain using LCEL\n",
    "    attribution_chain = (\n",
    "        attribution_prompt\n",
    "        | Config().new_openai_like(temperature=0)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Generate the response with citations\n",
    "    response = attribution_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"sources\": sources_formatted\n",
    "    })\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce9591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "question = \"How do transformer models work and what are some examples?\"\n",
    "attributed_answer = generate_attributed_response(question)\n",
    "print(attributed_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c42ce7",
   "metadata": {},
   "source": [
    "##### Self-consistency checking: ensuring factual accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7029e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def verify_response_accuracy(\n",
    "    retrieved_docs: list[Document],\n",
    "    generated_answer: str,\n",
    "    llm: ChatOpenAI | None = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Verify if a generated answer is fully supported by the retrieved documents.\n",
    "    Args:\n",
    "        retrieved_docs: List of documents used to generate the answer\n",
    "        generated_answer: The answer produced by the RAG system\n",
    "        llm: Language model to use for verification\n",
    "    Returns:\n",
    "        Dictionary containing verification results and any identified issues\n",
    "    \"\"\"\n",
    "    if llm is None:\n",
    "        # llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        llm = Config().new_openai_like(temperature=0)\n",
    "        \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Define verification prompt - fixed to avoid JSON formatting issues in the template\n",
    "    verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    As a fact-checking assistant, verify whether the following answer is fully supported\n",
    "    by the provided context. Identify any statements that are not supported or contradict the context.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer to verify:\n",
    "    {answer}\n",
    "    \n",
    "    Perform a detailed analysis with the following structure:\n",
    "    1. List any factual claims in the answer\n",
    "    2. For each claim, indicate whether it is:\n",
    "       - Fully supported (provide the supporting text from context)\n",
    "       - Partially supported (explain what parts lack support)\n",
    "       - Contradicted (identify the contradiction)\n",
    "       - Not mentioned in context\n",
    "    3. Overall assessment: Is the answer fully grounded in the context?\n",
    "    \n",
    "    Return your analysis in JSON format with the following structure:\n",
    "    {{\n",
    "      \"claims\": [\n",
    "        {{\n",
    "          \"claim\": \"The factual claim\",\n",
    "          \"status\": \"fully_supported|partially_supported|contradicted|not_mentioned\",\n",
    "          \"evidence\": \"Supporting or contradicting text from context\",\n",
    "          \"explanation\": \"Your explanation\"\n",
    "        }}\n",
    "      ],\n",
    "      \"fully_grounded\": true|false,\n",
    "      \"issues_identified\": [\"List any specific issues\"]\n",
    "    }}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create verification chain using LCEL\n",
    "    verification_chain = (\n",
    "        verification_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Run verification\n",
    "    result = verification_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"answer\": generated_answer\n",
    "    })\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efe6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "retrieved_docs = [\n",
    "    Document(page_content=\"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\"),\n",
    "    Document(page_content=\"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\")\n",
    "]\n",
    "\n",
    "generated_answer = \"The transformer architecture was introduced by OpenAI in 2018 and uses recurrent neural networks. BERT is a transformer model developed by Google.\"\n",
    "\n",
    "verification_result = verify_response_accuracy(retrieved_docs, generated_answer)\n",
    "print(verification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c36a84",
   "metadata": {},
   "source": [
    "#### Corrective RAG\n",
    "缺失完整示例代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575734f9",
   "metadata": {},
   "source": [
    "#### Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ac846",
   "metadata": {},
   "source": [
    "#### Choosing the right techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389c4fb",
   "metadata": {},
   "source": [
    "## Developing a corporate documentation chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0108825",
   "metadata": {},
   "source": [
    "源码参见 src/chapter04/developing-a-corporate-documentation-chatbot 目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6e5e3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m60 packages\u001b[0m \u001b[2min 87ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/6] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==2.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==0.6.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langgraph~=0.6 streamlit~=1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c2661385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.17.0.8:8501\u001b[0m\n",
      "\u001b[0m\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain/embeddings/cache.py:58: UserWarning: Using default key encoder: SHA-1 is *not* collision-resistant. While acceptable for most cache scenarios, a motivated attacker can craft two different payloads that map to the same cache key. If that risk matters in your environment, supply a stronger encoder (e.g. SHA-256 or BLAKE2) via the `key_encoder` argument. If you change the key encoder, consider also creating a new cache, to avoid (the potential for) collisions with existing keys.\n",
      "  _warn_about_sha1_encoder()\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!.venv/bin/streamlit run src/chapter04/developing-a-corporate-documentation-chatbot/streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f015d",
   "metadata": {},
   "source": [
    "## Troubleshooting RAG systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
