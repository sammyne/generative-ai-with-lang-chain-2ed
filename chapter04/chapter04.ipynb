{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e4ee78",
   "metadata": {},
   "source": [
    "# 04. Building Intelligent RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6998a93",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7324f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=1.0 langchain-core~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df5e8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-chroma~=1.0 langchain-community~=0.4 langchain-openai~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43596ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install jq~=1.10 python-dotenv~=1.1 scann~=1.4 transformers~=4.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56cb2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装 cpu 版 PyTorch，避免后续依赖 PyTorch 的包安装庞大的 GPU 版浪费空间\n",
    "%uv pip install torch~=2.9 torchvision~=0.24 torchaudio~=2.9 --index-url https://download.pytorch.org/whl/cpu\n",
    "%uv pip install langchain-huggingface~=1.2 sentence-transformers~=5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0204c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306cc65",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3986693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import Embeddings\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "        embeddings_model = os.getenv(\"OPENAI_EMBEDDINGS_MODEL\")\n",
    "        hf_pretrained_embeddings_model = os.getenv(\"HF_PRETRAINED_EMBEDDINGS_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.hf_pretrained_embeddings_model = (\n",
    "            hf_pretrained_embeddings_model\n",
    "            if hf_pretrained_embeddings_model\n",
    "            else \"Qwen/Qwen3-Embedding-8B\"\n",
    "        )\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_embeddings(self, **kwargs) -> OpenAIEmbeddings:\n",
    "        if not self.embeddings_model:\n",
    "            raise ValueError(\"OPENAI_EMBEDDINGS_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            model=self.embeddings_model,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.tiktoken_enabled\n",
    "            # 对于非 OpenAI 的官方实现，将这个参数置为 False。\n",
    "            # 回退到用 huggingface transformers 库 AutoTokenizer 来处理 token。\n",
    "            tiktoken_enabled=False,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.model\n",
    "            # 元宝说 Jina 的 embedding 模型 https://huggingface.co/jinaai/jina-embeddings-v4 最接近\n",
    "            # text-embedding-ada-002\n",
    "            # 个人喜好，选了 Qwen/Qwen3-Embedding-8B\n",
    "            # tiktoken_model_name='Qwen/Qwen3-Embedding-8B',\n",
    "            tiktoken_model_name=self.hf_pretrained_embeddings_model,\n",
    "            **kwargs,\n",
    "        )\n",
    "    \n",
    "def new_hf_embeddings(**kwargs) -> Embeddings:\n",
    "# ref: https://reference.langchain.com/python/integrations/langchain_huggingface/#langchain_huggingface.HuggingFaceEmbeddings\n",
    "    model_name = kwargs.pop(\"model_name\", os.environ['HF_EMBEDDINGS_MODEL'])\n",
    "    out = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": False},\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda46add",
   "metadata": {},
   "source": [
    "## From indexes to intelligent retrieval\n",
    "\n",
    "- A fundamental limitation of traditional retrieval systems lies in their lexical approach to document retrieval.\n",
    "- The breakthrough came with advances in neural network models that could capture the meaning\n",
    "of words and documents as dense vector representations—known as embeddings.\n",
    "- As a “closed-book” generative systems, LLM faced limitations: hallucination\n",
    "risks, knowledge cutoffs limited to training data, inability to cite sources, and challenges with\n",
    "complex reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1946a4",
   "metadata": {},
   "source": [
    "## Components of a RAG system\n",
    "- RAG enables language models to ground their outputs in external knowledge, providing an elegant\n",
    "solution to the limitations that plague pure LLMs: hallucinations, outdated information, and\n",
    "restricted context windows.\n",
    "- Main components:\n",
    "    - Knowledge base: The storage layer for external information\n",
    "    - Retriever: The knowledge access layer that finds relevant information\n",
    "    - Augmenter: The integration layer that prepares retrieved content\n",
    "    - Generator: The response layer that produces the final output\n",
    "- RAG operates through two interconnected pipelines:\n",
    "    - An indexing pipeline that processes, chunks, and stores documents in the knowledge base\n",
    "    - A query pipeline that retrieves relevant information and generates responses using that information\n",
    "- Architecture and workflow offers several advantages for production systems:\n",
    "    1. modularity allows components to be developed independently;\n",
    "    1. scalability enables resources to be allocated based on specific needs;\n",
    "    1. maintainability is improved through the clear separation of concerns;\n",
    "    1. flexibility permits different implementation strategies to be swapped in as requirements evolve.\n",
    "\n",
    "<img src=\"static/rag-arch-and-workflow.png\" alt=\"RAG architecture and workflow\" style=\"width: 30%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9b54a",
   "metadata": {},
   "source": [
    "### When to implement RAG\n",
    "\n",
    "- Significant implementation considerations\n",
    "    1. The system requires efficient indexing and retrieval mechanisms to maintain reasonable response times.\n",
    "    1. Knowledge bases need regular updates and maintenance to remain valuable.\n",
    "    1. Infrastructure must be designed to handle errors and edge cases gracefully, especially where different components interact.\n",
    "    1. Development teams must be prepared to manage these ongoing operational requirements.\n",
    "- Experience from Chelsea AI Ventures\n",
    "    1. Clients in regulated industries particularly benefit from RAG's verifiability.\n",
    "    1. Creative applications often perform adequately with pure LLMs.\n",
    "- Development teams should consider RAG when their applications require:\n",
    "    1. Access to current information not available in LLM training data\n",
    "    1. Domain-specific knowledge integration\n",
    "    1. Verifiable responses with source attribution\n",
    "    1. Processing of specialized data formats\n",
    "    1. High precision in regulated industries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5df81",
   "metadata": {},
   "source": [
    "## From embeddings to search\n",
    "\n",
    "The core components of a RAG system:\n",
    "1. vector embeddings\n",
    "1. vector stores\n",
    "1. Indexing strategies to optimize retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d48d4",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204e87d",
   "metadata": {},
   "source": [
    "Embeddings are numerical representations of text that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a79e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "Dimensions per embedding: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embeddings model\n",
    "# embeddings_model = Config().new_openai_like_embeddings()\n",
    "embeddings_model = new_hf_embeddings()\n",
    "\n",
    "# Create embeddings for example sentences\n",
    "text1 = \"The cat sat on the mat\"\n",
    "text2 = \"A feline rested on the carpet\"\n",
    "text3 = \"Python is a programming language\"\n",
    "\n",
    "# Get embeddings using LangChain\n",
    "embeddings = embeddings_model.embed_documents([text1, text2, text3])\n",
    "\n",
    "# These similar sentences will have similar embeddings\n",
    "embedding1 = embeddings[0]  # Embedding for \"The cat sat on the mat\"\n",
    "embedding2 = embeddings[1]  # Embedding for \"A feline rested on the carpet\"\n",
    "embedding3 = embeddings[2]  # Embedding for \"Python is a programming language\"\n",
    "\n",
    "# Output shows number of documents and embedding dimensions\n",
    "print(f\"Number of documents: {len(embeddings)}\")\n",
    "print(f\"Dimensions per embedding: {len(embeddings[0])}\")\n",
    "# Typically 1536 dimensions with OpenAI's embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e21096",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "\n",
    "Vector stores are specialized databases designed to store, manage, and efficiently search vector\n",
    "embeddings.\n",
    "\n",
    "The vector database operates as an independent system that can be:\n",
    "- Scaled independently of the RAG components\n",
    "- Maintained and optimized separately\n",
    "- Potentially shared across multiple RAG applications\n",
    "- Hosted as a dedicated service\n",
    "\n",
    "When working with embeddings, several challenges arise:\n",
    "- **Scale**: Applications often need to store millions of embeddings\n",
    "- **Dimensionality**: Each embedding might have hundreds or thousands of dimensions\n",
    "- **Search performance**: Finding similar vectors quickly becomes computationally intensive\n",
    "- **Associated data**: We need to maintain connections between vectors and their source documents\n",
    "\n",
    "Vector stores combine two essential components:\n",
    "- **Vector storage**: The actual database that persists vectors and metadata\n",
    "- **Vector index**: A specialized data structure that enables efficient similarity search\n",
    "\n",
    "The curse of dimensionality: as vector dimensions increase, computing similarities becomes increasingly expensive,\n",
    "requiring `O(dN)` operations for `d` dimensions and `N` vectors.\n",
    "\n",
    "\n",
    "Traditional database:\n",
    "- Uses exact matching (equality, ranges)\n",
    "- Optimized for structured data (for example, “find all customers with age > 30”)\n",
    "- Usually utilizes B-trees or hash-based indexes\n",
    "\n",
    "Vector store search:\n",
    "- Uses similarity metrics (cosine similarity, Euclidean distance)\n",
    "- Optimized for high-dimensional vector spaces\n",
    "- Employs Approximate Nearest Neighbor (ANN) algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e5a5d",
   "metadata": {},
   "source": [
    "#### Vector stores comparison\n",
    "\n",
    "Database | Deployment options | License | Notable features\n",
    "---------|--------------------|---------|----------------------------\n",
    "Pinecone | Cloud-only | Commercial | Auto-scaling, enterprise security, monitoring\n",
    "Milvus | Cloud, Self-hosted | Apache 2.0 | HNSW/IVF indexing, multi-modal support, CRUD operations\n",
    "Weaviate | Cloud, Self-hosted | BSD 3-Clause | Graph-like structure, multi-modal support\n",
    "Qdrant | Cloud, Self-hosted | Apache 2.0 | HNSW indexing, filtering optimization, JSON metadata\n",
    "ChromaDB | Cloud, Self-hosted | Apache 2.0 | Lightweight, easy setup\n",
    "AnalyticDB-V | Cloud-only | Commercial | OLAP integration, SQL support, enterprise features\n",
    "pg_vector | Cloud, Self-hosted | OSS | SQL support, PostgreSQL integration\n",
    "Vertex Vector Search | Cloud-only | Commercial | Easy setup, low latency, high scalability\n",
    "\n",
    "\n",
    "Several search patterns:\n",
    "1. Exact search: Returns precise nearest neighbors but becomes computationally prohibitive with large vector collections\n",
    "1. Approximate search: Trades accuracy for speed using techniques like LSH, HNSW, or quantization; measured by recall \n",
    "  (the percentage of true nearest neighbors retrieved)\n",
    "1. Hybrid search: Combines vector similarity with text-based search (like keyword matching or BM25) in a single query\n",
    "1. Filtered vector search: Applies traditional database filters (for example, metadata constraints) alongside vector\n",
    "  similarity search\n",
    "\n",
    "Different types of embeddings:\n",
    "1. Dense vector search: Uses continuous embeddings where most dimensions have non-zero values, typically from neural\n",
    "  models (like BERT, OpenAI embeddings)\n",
    "1. Sparse vector search: Uses high-dimensional vectors where most values are zero, resembling traditional TF-IDF or\n",
    "  BM25 representations\n",
    "1. Sparse-dense hybrid: Combines both approaches to leverage semantic similarity (dense) and keyword precision (sparse)\n",
    "\n",
    "\n",
    "Choice of multiple similarity measures, for example:\n",
    "1. Inner product: Useful for comparing semantic directions\n",
    "1. Cosine similarity: Normalizes for vector magnitude\n",
    "1. Euclidean distance: Measures the L2 distance in vector space (note: with normalized embeddings, this becomes\n",
    "  functionally equivalent to the dot product)\n",
    "1. Hamming distance: For binary vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11c408",
   "metadata": {},
   "source": [
    "#### Hardware considerations for vector stores\n",
    "\n",
    "1. Memory requirements\n",
    "2. CPU vs. GPU\n",
    "3. Storage speed\n",
    "4. Network bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622af3e7",
   "metadata": {},
   "source": [
    "#### Vector store interface in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "776e9945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='c75eb16d-b896-4e1c-bf35-029220937607', metadata={'id': 'doc_1'}, page_content='Content about language models'), Document(id='b7e1ccfb-d60a-4a06-8ee7-6ea6cce982a9', metadata={'id': 'doc_3'}, page_content='Details about retrieval systems')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize with an embedding model\n",
    "# embeddings = Config().new_openai_like_embeddings()\n",
    "embeddings = new_hf_embeddings()\n",
    "\n",
    "# Create some sample documents with explicit IDs\n",
    "docs = [\n",
    "    Document(page_content=\"Content about language models\", metadata={\"id\": \"doc_1\"}),\n",
    "    Document(\n",
    "        page_content=\"Information about vector databases\", metadata={\"id\": \"doc_2\"}\n",
    "    ),\n",
    "    Document(page_content=\"Details about retrieval systems\", metadata={\"id\": \"doc_3\"}),\n",
    "]\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# Add documents with explicit IDs\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "# Similarity Search with appropriate k value\n",
    "results = vector_store.similarity_search(\"How do language models work?\", k=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c866a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='c75eb16d-b896-4e1c-bf35-029220937607', metadata={'id': 'doc_1'}, page_content='Content about language models'), Document(id='a5ed0e49-0714-4061-82f6-ad9dd47128ae', metadata={'id': 'doc_2'}, page_content='Information about vector databases'), Document(id='b7e1ccfb-d60a-4a06-8ee7-6ea6cce982a9', metadata={'id': 'doc_3'}, page_content='Details about retrieval systems')]\n"
     ]
    }
   ],
   "source": [
    "# For maximum marginal relevance search, adjust the parameters based on available documents\n",
    "# Find relevant BUT diverse documents (reduce redundancy)\n",
    "results = vector_store.max_marginal_relevance_search(\n",
    "    \"How does LangChain work?\",\n",
    "    k=3,\n",
    "    fetch_k=10,\n",
    "    lambda_mult=0.5,  # Controls diversity (0=max diversity, 1=max relevance)\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dddd24",
   "metadata": {},
   "source": [
    "### Vector indexing strategies\n",
    "\n",
    "Some common indexing approaches include:\n",
    "- **Tree-based structures** that hierarchically divide the vector space\n",
    "- **Graph-based methods** like **Hierarchical Navigable Small World (HNSW)** that create navigable networks of connected vectors\n",
    "- **Hashing techniques** that map similar vectors to the same “buckets”\n",
    "\n",
    "Trade-offs between:\n",
    "- Search speed\n",
    "- Accuracy of results\n",
    "- Memory usage\n",
    "- Update efficiency (how quickly new vectors can be added)\n",
    "\n",
    "\n",
    "Takeway: proper indexing transforms vector search from an `O(n)` operation (where `n` is the number of vectors) to\n",
    "something much more efficient (often closer to `O(log n)`), making it possible to search through millions of vectors in\n",
    "milliseconds rather than seconds or minutes.\n",
    "\n",
    "Vector store comparison by deployment options, licensing, and key features as\n",
    "| Strategy                     | Core algorithm                                                                 | Complexity             | Memory usage                          | Best for                                                       | Notes                                                                                     |\n",
    "|------------------------------|-------------------------------------------------------------------------------|------------------------|---------------------------------------|----------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
    "| Exact Search (Brute Force)  | Compares query vector with every vector in database                           | Search: O(DN); Build: O(1) | Low - only stores raw vectors         | Small datasets; When 100% recall needed; Testing/baseline        | Easiest to implement; Good baseline for testing                                            |\n",
    "| HNSW (Hierarchical Navigable Small World) | Creates layered graph with decreasing connectivity from bottom to top        | Search: O(log N); Build: O(N log N) | High - stores graph connections plus vectors | Production systems; When high accuracy needed; Large-scale search | Industry standard; Requires careful tuning of M (connections) and ef (search depth)        |\n",
    "| LSH (Locality Sensitive Hashing) | Uses hash functions that map similar vectors to the same buckets            | Search: O(Np); Build: O(N) | Medium - stores multiple hash tables | Streaming data; When updates frequent; Approximate search OK     | Good for dynamic data; Tunable accuracy vs speed                                          |\n",
    "| IVF (Inverted File Index)    | Clusters vectors and searches within relevant clusters                        | Search: O(DN/k); Build: O(kN) | Low - stores cluster assignments      | Limited memory; Balance of speed/accuracy; Simple implementation | k = number of clusters; Often combined with other methods                                |\n",
    "| Product Quantization (PQ)    | Compresses vectors by splitting into subspaces and quantizing                | Search: varies; Build: O(N) | Very Low - compressed vectors         | Memory-constrained systems; Massive datasets                    | Often combined with IVF; Requires training codebooks Complex implementation              |\n",
    "|  Tree-Based (KD-Tree, Ball Tree) | Recursively partitions space into regions | Search: O(D log N) best case; Build: O(N log N) | Medium – tree structure | Low dimensional data; Static datasets | Works well for D < 100; Expensive updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ddb6cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install faiss-cpu~=1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8173ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact search time: 0.000583 seconds\n",
      "HNSW search time: 0.000168 seconds\n",
      "Speed improvement: 3.48x faster\n",
      "Result overlap: 80.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# Create sample data - 10,000 vectors with 128 dimensions\n",
    "dimension = 128\n",
    "num_vectors = 10000\n",
    "vectors = np.random.random((num_vectors, dimension)).astype('float32')\n",
    "query = np.random.random((1, dimension)).astype('float32')\n",
    "\n",
    "# Exact search index\n",
    "exact_index = faiss.IndexFlatL2(dimension)\n",
    "exact_index.add(vectors)\n",
    "\n",
    "# HNSW index (approximate but faster)\n",
    "hnsw_index = faiss.IndexHNSWFlat(dimension, 32) # 32 connections per node\n",
    "hnsw_index.add(vectors)\n",
    "\n",
    "# Compare search times\n",
    "start_time = time.time()\n",
    "exact_D, exact_I = exact_index.search(query, k=10) # Search for 10 nearest neighbors\n",
    "exact_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "hnsw_D, hnsw_I = hnsw_index.search(query, k=10)\n",
    "hnsw_time = time.time() - start_time\n",
    "\n",
    "# Calculate overlap (how many of the same results were found)\n",
    "overlap = len(set(exact_I[0]).intersection(set(hnsw_I[0])))\n",
    "overlap_percentage = overlap * 100 / 10\n",
    "\n",
    "print(f\"Exact search time: {exact_time:.6f} seconds\")\n",
    "print(f\"HNSW search time: {hnsw_time:.6f} seconds\")\n",
    "print(f\"Speed improvement: {exact_time/hnsw_time:.2f}x faster\")\n",
    "print(f\"Result overlap: {overlap_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b64f71",
   "metadata": {},
   "source": [
    "Guide for choosing an indexing strategy\n",
    "\n",
    "<img src=\"static/choosing-an-indexing-strategy.png\" alt=\"Choosing an indexing strategy\" style=\"width: 30%;\" />\n",
    "\n",
    "\n",
    "Vector libraries provide functionality for working with vector data. Examples as\n",
    "1. Faiss by Meta: PQ, LSH, and HNSW;\n",
    "1. Annoy by Spotify implemented in C++;\n",
    "1. hnswlib in C++: HNSW;\n",
    "1. Non-Metric Space Library (nmslib): HNSW, SW-graph, and SPTAG;\n",
    "1. SPTAG by Microsoft implements a distributed ANN: SPTAG-KDT, SPTAG-BKT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c65ab6",
   "metadata": {},
   "source": [
    "## Breaking down the RAG pipeline\n",
    "\n",
    "4 steps:\n",
    "1. Document processing – like preparing books for a library\n",
    "1. Vector indexing – creating the card catalog\n",
    "1. Vector stores – the organized shelves\n",
    "1. Retrieval – finding the right books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c934641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load documents\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True,\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d203e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Make embedding model\n",
    "# embedder = Config().new_openai_like_embeddings()\n",
    "embedder = new_hf_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4686cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Store in vector database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_db = FAISS.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67fa6ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='05ee00d3-56ab-4424-a565-2bcf68ca89b9', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'),\n",
       " Document(id='3b62a1ac-f5aa-458a-b4ca-7976c7c99a9c', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(id='95059355-f3bd-41f5-b755-6bc510611ff5', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(id='93565fb0-9000-49d7-9121-0809f9924d2b', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Retrieve similar docs\n",
    "query = \"What is GPT?\"\n",
    "\n",
    "vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800c0e8",
   "metadata": {},
   "source": [
    "### Document processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e028873",
   "metadata": {},
   "source": [
    "A document loader is a component in LangChain that transforms various data sources into a standardized document format that can be used throughout the LangChain ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d214ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fcb37",
   "metadata": {},
   "source": [
    "Document loaders in LangChain\n",
    "\n",
    "| Category | Description | Notable Examples | Common Use Cases |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **File Systems** | Load from local files | TextLoader, CSVLoader, PDF-Loader | Processing local documents, data files |\n",
    "| **Web Content** | Extract from online sources | WebBaseLoader, RecursiveURL-Loader, SitemapLoader | Web scraping, content aggregation |\n",
    "| **Cloud Storage** | Access cloud-hosted files | S3DirectoryLoader, GCSFileLoader, DropboxLoader | Enterprise data integration |\n",
    "| **Databases** | Load from structured data stores | MongoDBLoader, SnowflakeLoader, BigQueryLoader | Business intelligence, data analysis |\n",
    "| **Social Media** | Import social platform content | TwitterTweetLoader, RedditPostsLoader, DiscordChatLoader | Social media analysis |\n",
    "| **Productivity Tools** | Access workspace documents | NotionDirectoryLoader, SlackDirectoryLoader, TrelloLoader | Knowledge base creation |\n",
    "| **Scientific Sources** | Load academic content | ArxivLoader, PubMedLoader | Research applications |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea71d6",
   "metadata": {},
   "source": [
    "#### Chunking strategies\n",
    "\n",
    "The way you chunk documents affects:\n",
    "1. Retrieval accuracy: Well-formed chunks maintain semantic coherence, making them easier to match with relevant queries\n",
    "1. Context preservation: Poor chunking can split related information, causing knowledge gaps\n",
    "1. Response quality: When the LLM receives fragmented or irrelevant chunks, it generates less accurate responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88b2a6",
   "metadata": {},
   "source": [
    "##### Fixed-size chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0151a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 71ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-text-splitters~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7d9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13 chunks from document\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",  # Split on spaces to avoid breaking words\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Generated {len(chunks)} chunks from document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf5f45",
   "metadata": {},
   "source": [
    "##### Recursive character chunking\n",
    "\n",
    "**The recommended default strategy for most applications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f942af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.',\n",
       " 'It helps address hallucinations by grounding responses in retrieved information.',\n",
       " '## Key Components\\nRAG consists of several components:\\n1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4. Augmentation\\n5. Generation',\n",
       " '### Document Processing\\nThis step involves loading and chunking documents appropriately.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], chunk_size=150, chunk_overlap=20\n",
    ")\n",
    "\n",
    "document = \"\"\"# Introduction to RAG\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.\n",
    "\n",
    "It helps address hallucinations by grounding responses in retrieved information.\n",
    "\n",
    "## Key Components\n",
    "RAG consists of several components:\n",
    "1. Document processing\n",
    "2. Vector embedding\n",
    "3. Retrieval\n",
    "4. Augmentation\n",
    "5. Generation\n",
    "\n",
    "### Document Processing\n",
    "This step involves loading and chunking documents appropriately.\n",
    "\"\"\"\n",
    "\n",
    "text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa1e5a",
   "metadata": {},
   "source": [
    "##### Document-specific chunking\n",
    "\n",
    "An implementation could involve using different specialized splitters based on document type using if statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd464ed",
   "metadata": {},
   "source": [
    "##### Semantic chunking\n",
    "\n",
    "Workflow:\n",
    "1. Splits text into sentences\n",
    "2. Creates embeddings for groups of sentences (determined by buffer_size)\n",
    "3. Measures semantic similarity between adjacent groups\n",
    "4. Identifies natural breakpoints where topics or concepts change\n",
    "5. Creates chunks that preserve semantic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f6d455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-experimental~=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493a08ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models. It helps address hallucinations by grounding responses in retrieved information. ## Key Components\\nRAG consists of several components:\\n1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4.',\n",
       " 'Augmentation\\n5. Generation\\n\\n### Document Processing\\nThis step involves loading and chunking documents appropriately. ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# embeddings = Config().new_openai_like_embeddings()\n",
    "embeddings = new_hf_embeddings()\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings, add_start_index=True  # Include position metadata\n",
    ")\n",
    "\n",
    "text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31191d3",
   "metadata": {},
   "source": [
    "##### Agent-based chunking\n",
    "Uses LLMs to intelligently divide text based on semantic analysis and content understanding.\n",
    "\n",
    "Workflow:\n",
    "1. Analyze the document’s structure and content\n",
    "2. Identify natural breakpoints based on topic shifts\n",
    "3. Determine optimal chunk boundaries that preserve meaning\n",
    "4. Return a list of starting positions for creating chunks\n",
    "\n",
    "Useful when\n",
    "1. Documents contain intricate logical flows that need to be preserved\n",
    "1. Content requires domain-specific understanding to chunk appropriately\n",
    "1. Maximum retrieval accuracy justifies the additional expense of LLM-based processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874330f",
   "metadata": {},
   "source": [
    "##### Multi-modal chunking\n",
    "\n",
    "Handle documents mixing text, tables, images and code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bee5e7",
   "metadata": {},
   "source": [
    "##### Choosing the right chunking strategy\n",
    "\n",
    "| Factor | Condition | Recommended Strategy |\n",
    "| :--- | :--- | :--- |\n",
    "| **Document Characteristics** | Highly structured documents (markdown, code) | Document-specific chunking |\n",
    "|  | Complex technical content | Semantic chunking |\n",
    "|  | Mixed media | Multi-modal approaches |\n",
    "| **Retrieval Needs** | Fact-based QA | Smaller chunks (100-300 tokens) |\n",
    "|  | Complex reasoning | Larger chunks (500-1000 tokens) |\n",
    "|                               | Context-heavy answers | Sliding window with significant overlap |\n",
    "| **Computational Resources**   | Limited API budget    | Basic recursive chunking                |\n",
    "|                               | Performance-critical  | Pre-computed semantic chunks            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8df8fa",
   "metadata": {},
   "source": [
    "#### Retrieval\n",
    "\n",
    "A retriever is fundamentally an interface that accepts natural language queries and returns relevant documents.\n",
    "\n",
    "Workflow\n",
    "1. **Input**: Takes a query as a string\n",
    "1. **Processing**: Applies retrieval logic specific to the implementation\n",
    "1. **Output**: Returns a list of document objects, each containing:\n",
    "    - `page_content`: The actual document content\n",
    "    - `metadata`: Associated information like document ID or source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051ef66",
   "metadata": {},
   "source": [
    "##### LangChain retrievers\n",
    "\n",
    "A few key groups\n",
    "\n",
    "Group | Example\n",
    "------|-----------\n",
    "Core infrastructure | self-hosted options like ElasticsearchRetriever, cloud-based from Amazon, Google, and Microsoft\n",
    "External knowledge | ArxivRetriever, WikipediaRetriever, and TavilySearchAPI\n",
    "Algorithmic | BM25 for keyword precision, TF-IDF for document classification, and kNN for similarity matching\n",
    "Advanced/Specialized | NeuralDB provides CPU-optimized retrieval, while LLMLingua focuses on document compression.\n",
    "Integration | connect with popular platforms and services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434b109",
   "metadata": {},
   "source": [
    "##### Vector store retrievers\n",
    "\n",
    "Any vector store can become a retriever through the `as_retriever()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb5c6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import KNNRetriever\n",
    "\n",
    "# embeddings = Config().new_openai_like_embeddings()\n",
    "embeddings = new_hf_embeddings()\n",
    "\n",
    "retriever = KNNRetriever.from_documents(documents, embeddings)\n",
    "retriever.invoke(\"query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9e638",
   "metadata": {},
   "source": [
    "Retrievers most relevant for RAG systems\n",
    "1. Search API retrievers: These retrievers interface with external search services without storing documents locally.\n",
    "1. Database retrievers: These connect to structured data sources, translating natural language queries into database queries.\n",
    "1. Lexical search retrievers: These implement traditional text-matching algorithms:\n",
    "    - BM25 for probabilistic ranking\n",
    "    - TF-IDF for term frequency analysis\n",
    "    - Elasticsearch integration for scalable text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf898b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install xmltodict~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1f696",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m retriever = PubMedRetriever(email=\u001b[33m\"\u001b[39m\u001b[33mxiangminli@outlook.com\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# FIXME: 没有跑通\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchatgpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_core/retrievers.py:216\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    220\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/retrievers/pubmed.py:20\u001b[39m, in \u001b[36mPubMedRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_relevant_documents\u001b[39m(\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, *, run_manager: CallbackManagerForRetrieverRun\n\u001b[32m     19\u001b[39m ) -> List[Document]:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/utilities/pubmed.py:132\u001b[39m, in \u001b[36mPubMedAPIWrapper.load_docs\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/utilities/pubmed.py:128\u001b[39m, in \u001b[36mPubMedAPIWrapper.lazy_load_docs\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> Iterator[Document]:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict2document\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/utilities/pubmed.py:110\u001b[39m, in \u001b[36mPubMedAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    108\u001b[39m result = urllib.request.urlopen(url)\n\u001b[32m    109\u001b[39m text = result.read().decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m json_text = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m webenv = json_text[\u001b[33m\"\u001b[39m\u001b[33mesearchresult\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mwebenv\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m json_text[\u001b[33m\"\u001b[39m\u001b[33mesearchresult\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33midlist\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/json/decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     end = _w(s, end).end()\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/json/decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import PubMedRetriever\n",
    "\n",
    "retriever = PubMedRetriever(email=\"xiangminli@outlook.com\")\n",
    "# 注意事项：国内访问 PubMed 会报错，估计是 PubMed 限制了国内访问\n",
    "results = retriever.invoke(\"chatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c889ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install arxiv~=2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75deb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/2405.09300v1', 'Published': datetime.date(2024, 5, 15), 'Title': 'Comparing the Efficacy of GPT-4 and Chat-GPT in Mental Health Care: A Blind Assessment of Large Language Models for Psychological Support', 'Authors': 'Birger Moell'}, page_content=\"Background: Rapid advancements in natural language processing have led to the development of large language models with the potential to revolutionize mental health care. These models have shown promise in assisting clinicians and providing support to individuals experiencing various psychological challenges.\\n  Objective: This study aims to compare the performance of two large language models, GPT-4 and Chat-GPT, in responding to a set of 18 psychological prompts, to assess their potential applicability in mental health care settings.\\n  Methods: A blind methodology was employed, with a clinical psychologist evaluating the models' responses without knowledge of their origins. The prompts encompassed a diverse range of mental health topics, including depression, anxiety, and trauma, to ensure a comprehensive assessment.\\n  Results: The results demonstrated a significant difference in performance between the two models (p > 0.05). GPT-4 achieved an average rating of 8.29 out of 10, while Chat-GPT received an average rating of 6.52. The clinical psychologist's evaluation suggested that GPT-4 was more effective at generating clinically relevant and empathetic responses, thereby providing better support and guidance to potential users.\\n  Conclusions: This study contributes to the growing body of literature on the applicability of large language models in mental health care settings. The findings underscore the importance of continued research and development in the field to optimize these models for clinical use. Further investigation is necessary to understand the specific factors underlying the performance differences between the two models and to explore their generalizability across various populations and mental health conditions.\"),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2310.19303v2', 'Published': datetime.date(2023, 12, 6), 'Title': 'Extracting user needs with Chat-GPT for dialogue recommendation', 'Authors': 'Yugen Sato, Taisei Nakajima, Tatsuki Kawamoto, Tomohiro Takagi'}, page_content=\"Large-scale language models (LLMs), such as ChatGPT, are becoming increasingly sophisticated and exhibit human-like capabilities, playing an essential role in assisting humans in a variety of everyday tasks. An important application of AI is interactive recommendation systems that respond to human inquiries and make recommendations tailored to the user. In most conventional interactive recommendation systems, the language model is used only as a dialogue model, and there is a separate recommendation system. This is due to the fact that the language model used as a dialogue system does not have the capability to serve as a recommendation system. Therefore, we will realize the construction of a dialogue system with recommendation capability by using OpenAI's Chat-GPT, which has a very high inference capability as a dialogue system and the ability to generate high-quality sentences, and verify the effectiveness of the system.\"),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2308.08769v1', 'Published': datetime.date(2023, 8, 17), 'Title': 'Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes', 'Authors': 'Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao'}, page_content='3D scene understanding has gained significant attention due to its wide range of applications. However, existing methods for 3D scene understanding are limited to specific downstream tasks, which hinders their practicality in real-world applications. This paper presents Chat-3D, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes. Specifically, we align 3D representations into the feature space of LLMs, thus enabling LLMs to perceive the 3D world. Given the scarcity of 3D scene-text data, we propose a three-stage training strategy to efficiently utilize the available data for better alignment. To enhance the reasoning ability and develop a user-friendly interaction scheme, we further construct a high-quality object-centric 3D instruction dataset and design an associated object-centric prompt. Our experiments show that Chat-3D achieves an impressive ability to comprehend diverse instructions for 3D scenes, engage in intricate spatial reasoning, and incorporate external knowledge into its responses. Chat-3D achieves a 75.6% relative score compared with GPT-4 on the constructed instruction dataset.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "retriever = ArxivRetriever(\n",
    "    load_max_docs=2,\n",
    "    # get_ful_documents=True,\n",
    ")\n",
    "\n",
    "retriever.invoke(\"chat-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8d279",
   "metadata": {},
   "source": [
    "Modern retrieval systems often combine multiple approaches for better results:\n",
    "1. Hybrid search\n",
    "    - Vector similarity for semantic understanding\n",
    "    - Keyword matching for precise terminology\n",
    "    - Weighted combinations for optimal results\n",
    "1. Maximal Marginal Relevance (MMR): Optimizes for both relevance and diversity by:\n",
    "    - Selecting documents similar to the query\n",
    "    - Ensuring retrieved documents are distinct from each other\n",
    "    - Balancing exploration and exploitation\n",
    "1. Custom retrieval logic: LangChain allows the creation of specialized retrievers by implementing the `BaseRetriever` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba012bb8",
   "metadata": {},
   "source": [
    "### Advanced RAG techniques\n",
    "\n",
    "A standard vector search has several limitations:\n",
    "- It might miss contextually relevant documents that use different terminology\n",
    "- It can’t distinguish between authoritative and less reliable sources\n",
    "- It might return redundant or contradictory information\n",
    "- It has no way to verify if generated responses accurately reflect the source material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e47228",
   "metadata": {},
   "source": [
    "#### Hybrid retrieval: Combining semantic and keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9af5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 1.84s\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/8.38 KiB            \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[2m\u001b[0m\u001b[0m 8.38 KiB/8.38 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 97ms\u001b[0m\u001b[0m                                                   \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrank-bm25\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install rank-bm25~=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35039b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize with an embedding model\n",
    "# embeddings = Config().new_openai_like_embeddings()\n",
    "embeddings = new_hf_embeddings()\n",
    "\n",
    "# Create some sample documents with explicit IDs\n",
    "docs = [\n",
    "    Document(page_content=\"Content about language models\", metadata={\"id\": \"doc_1\"}),\n",
    "    Document(\n",
    "        page_content=\"Information about vector databases\", metadata={\"id\": \"doc_2\"}\n",
    "    ),\n",
    "    Document(page_content=\"Details about retrieval systems\", metadata={\"id\": \"doc_3\"}),\n",
    "]\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bba4112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# https://docs.langchain.com/oss/python/migrate/langchain-v1#langchain-classic\n",
    "%uv pip install langchain-classic~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d51824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fdca971b-a214-4b72-87cb-0f6f5136167e', metadata={'id': 'doc_3'}, page_content='Details about retrieval systems'),\n",
       " Document(id='64cad61f-ea1e-4699-bdd2-ed982ba26802', metadata={'id': 'doc_1'}, page_content='Content about language models'),\n",
       " Document(metadata={'id': 'doc_2'}, page_content='Information about vector databases')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Setup semantic retriever\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "# print(vector_retriever.invoke(\"climate change impacts\"))\n",
    "\n",
    "# Setup lexical retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2\n",
    "# print(bm25_retriever.invoke(\"climate change impacts\"))\n",
    "\n",
    "# Combine retrievers\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.7, 0.3],  # Weight semantic search higher than keyword search\n",
    ")\n",
    "\n",
    "hybrid_retriever.invoke(\"climate change impacts\")\n",
    "\n",
    "# 注意事项：结果有 3 个。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11623d",
   "metadata": {},
   "source": [
    "#### Re-ranking\n",
    "\n",
    "Re-ranking is a post-processing step that can follow any retrieval method.\n",
    "\n",
    "Workflow\n",
    "1. retrieve a larger set of candidate documents\n",
    "1. Apply a more sophisticated model to re-score documents\n",
    "1. Reorder based on these more precise relevance scores\n",
    "\n",
    "3 main paradigms:\n",
    "- **Pointwise rerankers**: Score each document independently (for example, on a scale of 1-10) and sort the resulting array of documents accordingly\n",
    "- **Pairwise rerankers**: Compare document pairs to determine preferences, then construct a final ordering by ranking documents based on their win/loss record across all comparisons\n",
    "- **Listwise rerankers**: The re-ranking model processes the entire list of documents (and the original query) holistically to determine optimal order by optimizing NDCG or MAP\n",
    "\n",
    "\n",
    "LangChain's implementations\n",
    "1. Cohere rerank: Commercial API-based solution with excellent quality\n",
    "1. RankLLM: Library supporting open-source LLMs fine-tuned specifically for re-ranking\n",
    "1. LLM-based custom rerankers: Using any LLM to score document relevance\n",
    "\n",
    "Cross-encoder re-ranking typically improves these metrics by 10-20% over initial retrieval, especially for the top positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99054385",
   "metadata": {},
   "source": [
    "#### Query transformation: Improving retrieval through better queries\n",
    "\n",
    "Query expansion generates multiple variations of the original query to capture different aspects or phrasings.\n",
    "\n",
    "Particularly useful when dealing with ambiguous queries, questions formulated by non-experts, or situations where terminology mismatches between queries and documents are common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ca0d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. How is climate change impacting the environment and society?\n",
      "2. What are the consequences of global warming?\n",
      "3. In what ways is the changing climate affecting the planet?\n"
     ]
    }
   ],
   "source": [
    "# 用 langchain-classic 不用 langchain-core 的原理参见\n",
    "# https://docs.langchain.com/oss/python/migrate/langchain-v1#langchain-classic\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "expansion_template = \"\"\"Given the user question: {question}\n",
    "Generate three alternative versions that express the same information need but with different wording:\n",
    "1.\"\"\"\n",
    "\n",
    "expansion_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"], template=expansion_template\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like(temperature=0.7)\n",
    "expansion_chain = expansion_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate expanded queries\n",
    "original_query = \"What are the effects of climate change?\"\n",
    "reply = expansion_chain.invoke(original_query)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d8c0f",
   "metadata": {},
   "source": [
    "##### Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "HyDE uses an LLM to generate a hypothetical answer document based on the query, and then uses that document’s embedding for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997ef552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# embedder = Config().new_openai_like_embeddings()\n",
    "embedder = new_hf_embeddings()\n",
    "\n",
    "vector_db = FAISS.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa4bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 langchain-classic 不用 langchain-core 的原理参见\n",
    "# https://docs.langchain.com/oss/python/migrate/langchain-v1#langchain-classic\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Create prompt for generating hypothetical document\n",
    "hyde_template = \"\"\"Based on the question: {question}\n",
    "Write a passage that could contain the answer to this question:\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate(input_variables=[\"question\"], template=hyde_template)\n",
    "llm = Config().new_openai_like(temperature=0.2)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate hypothetical document\n",
    "query = \"What dietary changes can reduce carbon footprint?\"\n",
    "hypothetical_doc = hyde_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5e32f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='06789117-fad5-496b-82ec-c7cca664fb53', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(id='f3eeab14-faf7-49d7-9b6b-96ddb448c5d4', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(id='860a6a71-5bd9-483f-bda6-2e68d125da2d', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the hypothetical document for retrieval\n",
    "# embeddings = Config().new_openai_like_embeddings()\n",
    "embeddings = new_hf_embeddings()\n",
    "embedded_query = embeddings.embed_query(hypothetical_doc)\n",
    "vector_db.similarity_search_by_vector(embedded_query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f268dce",
   "metadata": {},
   "source": [
    "#### Context processing: maximizing retrieved information value\n",
    "\n",
    "Especially valuable when dealing with lengthy documents\n",
    "where only portions are relevant, or when providing comprehensive coverage of a topic requires\n",
    "diverse viewpoints. They help reduce noise in the generator’s input and ensure that the most\n",
    "valuable information is prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e76400",
   "metadata": {},
   "source": [
    "##### Contextual compression\n",
    "\n",
    "Extracts only the most relevant parts of retrieved documents, removing irrelevant content that might distract the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72db25dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Extracted relevant parts:\\n>>>\\nTransformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\\n>>>\"),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Create a basic retriever from the vector store\n",
    "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "compression_retriever.invoke(\"How do transformers work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbea1dc",
   "metadata": {},
   "source": [
    "##### Maximum marginal relevance\n",
    "\n",
    "Balances document relevance with diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4aa1b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='bf4e8670-55c6-40f9-96ca-70643d34d806', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(id='20ae0319-45dd-45c5-9d73-d693e8ab347f', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.'),\n",
       " Document(id='a6cd2884-9019-4ddc-a9bc-3b82f3f73936', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize with an embedding model\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# For maximum marginal relevance search, adjust the parameters based on available documents\n",
    "# Find relevant BUT diverse documents (reduce redundancy)\n",
    "vector_store.max_marginal_relevance_search(\n",
    "    query=\"What are transformer models?\",\n",
    "    k=3,  # Number of documents to return\n",
    "    fetch_k=20,  # Number of documents to initially fetch\n",
    "    lambda_mult=0.5,  # Diversity parameter (0 = max diversity, 1 = max relevance)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e32d00",
   "metadata": {},
   "source": [
    "#### Response enhancement: Improving generator output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ad7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n",
    "        metadata={\"source\": \"Neural Network Review 2021\", \"page\": 42},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.\",\n",
    "        metadata={\"source\": \"Introduction to NLP\", \"page\": 137},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"GPT models are autoregressive transformers that predict the next token based on previous tokens.\",\n",
    "        metadata={\"source\": \"Large Language Models Survey\", \"page\": 89},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e181d5",
   "metadata": {},
   "source": [
    "##### Source attribution\n",
    "\n",
    "Explicitly connects generated information to the retrieved sources, helping users verify facts and understand where information comes from.\n",
    "\n",
    "Example implementation\n",
    "1. Retrieving relevant documents for a query\n",
    "1. Formatting each document with a citation number\n",
    "1. Using a prompt that explicitly requests citations for each fact\n",
    "1. Generating a response that includes inline citations ([1], [2], etc.)\n",
    "1. Adding a references section that links each citation to its source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b6eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Create a vector store and retriever\n",
    "# embeddings = Config().new_openai_like_embeddings()\n",
    "embeddings = new_hf_embeddings()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Source attribution prompt template\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a precise AI assistant that provides well-sourced information.\n",
    "Answer the following question based ONLY on the provided sources. For each fact or claim in your answer,\n",
    "include a citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "Your answer:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e993e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create a source-formatted string from documents\n",
    "def format_sources_with_citations(docs):\n",
    "    formatted_sources = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source_info = f\"[{i}] {doc.metadata.get('source', 'Unknown source')}\"\n",
    "        if doc.metadata.get(\"page\"):\n",
    "            source_info += f\", page {doc.metadata['page']}\"\n",
    "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted_sources)\n",
    "\n",
    "\n",
    "# Build the RAG chain with source attribution\n",
    "def generate_attributed_response(question):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    # Format sources with citation numbers\n",
    "    sources_formatted = format_sources_with_citations(retrieved_docs)\n",
    "    # print(sources_formatted)\n",
    "\n",
    "    # Create the attribution chain using LCEL\n",
    "    attribution_chain = (\n",
    "        attribution_prompt | Config().new_openai_like(temperature=0) | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Generate the response with citations\n",
    "    response = attribution_chain.invoke(\n",
    "        {\"question\": question, \"sources\": sources_formatted}\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70ce9591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based solely on the provided sources:\n",
      "\n",
      "Transformer models are a neural network architecture that was introduced in a 2017 paper titled \"Attention is All You Need\" [3]. They work using an attention mechanism.\n",
      "\n",
      "Two specific examples of transformer models are:\n",
      "1.  **GPT models**, which are described as autoregressive transformers. They function by predicting the next token in a sequence based on the previous tokens [1].\n",
      "2.  **BERT**, which uses a bidirectional training approach based on the Transformer architecture. Its training involves masked language modeling and next sentence prediction tasks [2].\n",
      "\n",
      "**References:**\n",
      "[1] Large Language Models Survey, page 89\n",
      "[2] Introduction to NLP, page 137\n",
      "[3] Neural Network Review 2021, page 42\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"How do transformer models work and what are some examples?\"\n",
    "attributed_answer = generate_attributed_response(question)\n",
    "print(attributed_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c42ce7",
   "metadata": {},
   "source": [
    "##### Self-consistency checking: ensuring factual accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f7029e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def verify_response_accuracy(\n",
    "    retrieved_docs: list[Document], generated_answer: str, llm: ChatOpenAI | None = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Verify if a generated answer is fully supported by the retrieved documents.\n",
    "    Args:\n",
    "        retrieved_docs: List of documents used to generate the answer\n",
    "        generated_answer: The answer produced by the RAG system\n",
    "        llm: Language model to use for verification\n",
    "    Returns:\n",
    "        Dictionary containing verification results and any identified issues\n",
    "    \"\"\"\n",
    "    if llm is None:\n",
    "        llm = Config().new_openai_like(temperature=0)\n",
    "\n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Define verification prompt - fixed to avoid JSON formatting issues in the template\n",
    "    verification_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    As a fact-checking assistant, verify whether the following answer is fully supported\n",
    "    by the provided context. Identify any statements that are not supported or contradict the context.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer to verify:\n",
    "    {answer}\n",
    "    \n",
    "    Perform a detailed analysis with the following structure:\n",
    "    1. List any factual claims in the answer\n",
    "    2. For each claim, indicate whether it is:\n",
    "       - Fully supported (provide the supporting text from context)\n",
    "       - Partially supported (explain what parts lack support)\n",
    "       - Contradicted (identify the contradiction)\n",
    "       - Not mentioned in context\n",
    "    3. Overall assessment: Is the answer fully grounded in the context?\n",
    "    \n",
    "    Return your analysis in JSON format with the following structure:\n",
    "    {{\n",
    "      \"claims\": [\n",
    "        {{\n",
    "          \"claim\": \"The factual claim\",\n",
    "          \"status\": \"fully_supported|partially_supported|contradicted|not_mentioned\",\n",
    "          \"evidence\": \"Supporting or contradicting text from context\",\n",
    "          \"explanation\": \"Your explanation\"\n",
    "        }}\n",
    "      ],\n",
    "      \"fully_grounded\": true|false,\n",
    "      \"issues_identified\": [\"List any specific issues\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create verification chain using LCEL\n",
    "    verification_chain = verification_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run verification\n",
    "    result = verification_chain.invoke({\"context\": context, \"answer\": generated_answer})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9efe6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"The transformer architecture was introduced by OpenAI in 2018\",\n",
      "      \"status\": \"contradicted\",\n",
      "      \"evidence\": \"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\",\n",
      "      \"explanation\": \"The context clearly states that the transformer architecture was introduced in 2017 by Vaswani et al., not by OpenAI in 2018. This directly contradicts the claim.\"\n",
      "    },\n",
      "    {\n",
      "      \"claim\": \"The transformer architecture ... uses recurrent neural networks\",\n",
      "      \"status\": \"contradicted\",\n",
      "      \"evidence\": \"It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\",\n",
      "      \"explanation\": \"The context explicitly states that transformers use self-attention mechanisms instead of recurrent neural networks, making this claim directly false.\"\n",
      "    },\n",
      "    {\n",
      "      \"claim\": \"BERT is a transformer model developed by Google\",\n",
      "      \"status\": \"fully_supported\",\n",
      "      \"evidence\": \"BERT is a transformer-based model developed by Google\",\n",
      "      \"explanation\": \"This claim is directly supported by the context, which states that BERT is a transformer-based model developed by Google.\"\n",
      "    }\n",
      "  ],\n",
      "  \"fully_grounded\": false,\n",
      "  \"issues_identified\": [\n",
      "    \"Incorrect attribution of transformer introduction to OpenAI in 2018\",\n",
      "    \"False claim that transformers use recurrent neural networks\",\n",
      "    \"Correct statement about BERT being a Google-developed transformer model does not compensate for the two major contradictions\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "retrieved_docs = [\n",
    "    Document(\n",
    "        page_content=\"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "generated_answer = \"The transformer architecture was introduced by OpenAI in 2018 and uses recurrent neural networks. BERT is a transformer model developed by Google.\"\n",
    "\n",
    "verification_result = verify_response_accuracy(retrieved_docs, generated_answer)\n",
    "print(verification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9e91b",
   "metadata": {},
   "source": [
    "The verification can be further enhanced by:\n",
    "1. Granular claim extraction: Breaking down complex responses into atomic factual claims\n",
    "1. Evidence linking: Explicitly connecting each claim to specific supporting text\n",
    "1. Confidence scoring: Assigning numerical confidence scores to different parts of the response\n",
    "1. Selective regeneration: Regenerating only the unsupported portions of responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c36a84",
   "metadata": {},
   "source": [
    "#### Corrective RAG\n",
    "\n",
    "In real-world applications, retrieval systems often return irrelevant, insufficient, or even misleading content.\n",
    "\n",
    "Corrective Retrieval-Augmented Generation (CRAG) directly addresses this challenge by introducing explicit evaluation and correction mechanisms into the RAG pipeline.\n",
    "\n",
    "Workflow\n",
    "1. **Initial retrieval**: Standard document retrieval from the vector store based on the query.\n",
    "1. **Retrieval evaluation**: A retrieval evaluator component assesses each document’s relevance and quality.\n",
    "1. **Conditional correction**:\n",
    "    1. **Relevant documents**: Pass high-quality documents directly to the generator.\n",
    "    1. Irrelevant documents: Filter out low-quality documents to prevent noise.\n",
    "    1. Insufficient/Ambiguous results: Trigger alternative information-seeking strategies (like web search) when internal knowledge is inadequate.\n",
    "1. Generation: Produce the final response using the filtered or augmented context.\n",
    "\n",
    "<img src=\"static/corrective-rag.png\" style=\"width: 80%;\" alt=\"Corrective RAG workflow\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575734f9",
   "metadata": {},
   "source": [
    "#### Agentic RAG\n",
    "\n",
    "Agentic RAG uses agents to:\n",
    "1. Analyze queries and decompose complex questions into manageable sub-questions\n",
    "1. Plan information-gathering strategies based on the specific task requirements\n",
    "1. Select appropriate tools (retrievers, web search, calculators, APIs, etc.)\n",
    "1. Execute multi-step processes, potentially involving multiple rounds of retrieval and reasoning\n",
    "1. Reflect on intermediate results and adapt strategies accordingly\n",
    "\n",
    "CRAG primarily enhances data quality through evaluation and correction, while agentic RAG focuses on process intelligence through autonomous planning and orchestration.\n",
    "\n",
    "Particularly valuable for complex use cases that require:\n",
    "1. Multi-step reasoning across multiple information sources\n",
    "1. Dynamic tool selection based on query analysis\n",
    "1. Persistent task execution with intermediate reflection\n",
    "1. Integration with various external systems and APIs\n",
    "\n",
    "Agentic RAG introduces significant complexity in implementation, potentially higher\n",
    "latency due to multiple reasoning steps, and increased computational costs from multiple LLM\n",
    "calls for planning and reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ac846",
   "metadata": {},
   "source": [
    "#### Choosing the right techniques\n",
    "\n",
    "| RAG Approach | Chapter Section | Core Mechanism | Key Strengths | Key Weaknesses | Primary Use Cases | Relative Complexity |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Naive RAG | Breaking down the RAG pipeline | Basic index → retrieve → generate workflow with single retrieval step | Simple implementation Low initial resource usage Straightforward debugging | Limited retrieval quality Vulnerability to hallucinations No handling of retrieval failures | Simple Q&A systems Basic document lookup Prototyping | Low |\n",
    "| Hybrid Retrieval | Advanced RAG techniques - hybrid retrieval | Combines sparse (BM25) and dense (vector) retrieval methods | Balances keyword precision with semantic understanding Handles vocabulary mismatch Improves recall without sacrificing precision | Increased system complexity Challenge in optimizing fusion weights Higher computational overhead | Technical documentation Content with specialized terminology Multi-domain knowledge bases | Medium |\n",
    "| Re-ranking | Advanced RAG techniques - re-ranking | Post-processes initial retrieval results with more sophisticated relevance models | Improves result ordering Captures nuanced relevance signals Can be applied to any retrieval method | Additional computation layer May create bottlenecks for large result sets Requires training or configuring re-rankers | When retrieval quality is critical For handling ambiguous queries High-value information needs | Medium |\n",
    "| Query Transformation (HyDE) | Advanced RAG techniques - query transformation | Generates hypothetical document from query for improved retrieval | Bridges query-document semantic gap Improves retrieval for complex queries Handles implicit information needs | Additional LLM generation step Depends on hypothetical document quality Potential for query drift | Complex or ambiguous queries Users with unclear information needs Potential for query drift | Medium |\n",
    "| Context Processing | Advanced RAG techniques - context processing | Optimizes retrieved documents before sending to the generator (compression, MMR) | • Maximizes context window utilization<br>• Reduces redundancy<br>• Focuses on most relevant information | • Risk of removing important context<br>• Processing adds latency<br>• May lose document coherence | • Large documents<br>• When context window is limited<br>• Redundant information sources | Medium |\n",
    "| Response Enhancement | Advanced RAG techniques - response enhancement | Improves generated output with source attribution and consistency checking | • Increases output trustworthiness<br>• Provides verification mechanisms<br>• Enhances user confidence | • May reduce fluency or conciseness<br>• Additional post-processing overhead<br>• Complex implementation logic | • Educational or research content<br>• Legal or medical information<br>• When attribution is required | Medium-High |\n",
    "| Corrective RAG (CRAG) | Advanced RAG techniques - corrective RAG | Evaluates retrieved documents and takes corrective actions (filtering, web search) | • Explicitly handles poor retrieval results<br>• Improves robustness<br>• Can dynamically supplement knowledge | • Increased latency from evaluation<br>• Depends on evaluator accuracy<br>• More complex conditional logic | • High-reliability requirements<br>• Systems needing factual accuracy<br>• Applications with potential knowledge gaps | High |\n",
    "| Agentic RAG | Advanced RAG techniques - agentic RAG | Uses autonomous AI agents to orchestrate information gathering and synthesis | • Highly adaptable to complex tasks<br>• Can use diverse tools beyond retrieval<br>• Multi-step reasoning capabilities | • Significant implementation complexity<br>• Higher cost and latency<br>• Challenging to debug and control | • Complex multi-step information tasks<br>• Research applications<br>• Systems integrating multiple data sources | Very High |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389c4fb",
   "metadata": {},
   "source": [
    "## Developing a corporate documentation chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0108825",
   "metadata": {},
   "source": [
    "源码参见 src/chapter04/developing-a-corporate-documentation-chatbot 目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e5e3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m60 packages\u001b[0m \u001b[2min 549ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m     0 B/153.08 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m     0 B/153.08 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 16.00 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 32.00 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 48.00 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 76.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)[2m-----------\u001b[0m\u001b[0m 92.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--\u001b[2m--------\u001b[0m\u001b[0m 108.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)-----\u001b[2m-----\u001b[0m\u001b[0m 124.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 30.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)-----\u001b[2m-----\u001b[0m\u001b[0m 124.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 30.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------\u001b[2m--\u001b[0m\u001b[0m 140.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.48 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------\u001b[2m--\u001b[0m\u001b[0m 140.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.48 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 153.08 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.48 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 153.08 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 153.08 KiB/153.08 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 599ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/23] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m23 packages\u001b[0m \u001b[2min 143ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maltair\u001b[0m\u001b[2m==5.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblinker\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnarwhals\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==21.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydeck\u001b[0m\u001b[2m==0.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstreamlit\u001b[0m\u001b[2m==1.51.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtoml\u001b[0m\u001b[2m==0.10.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchdog\u001b[0m\u001b[2m==6.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langgraph~=1.0 streamlit~=1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe36111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain/embeddings/cache.py:58: UserWarning: Using default key encoder: SHA-1 is *not* collision-resistant. While acceptable for most cache scenarios, a motivated attacker can craft two different payloads that map to the same cache key. If that risk matters in your environment, supply a stronger encoder (e.g. SHA-256 or BLAKE2) via the `key_encoder` argument. If you change the key encoder, consider also creating a new cache, to avoid (the potential for) collisions with existing keys.\n",
      "  _warn_about_sha1_encoder()\n",
      "[]\n",
      "INFO:httpx:HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The square root of 10 is approximately 3.1623.\n",
      "\n",
      "None of the provided corporate document snippets are relevant to this mathematical question.\n",
      "INFO:httpx:HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "no issues detected\n",
      "The square root of 10 is approximately 3.1623.\n",
      "\n",
      "None of the provided corporate document snippets are relevant to this mathematical question.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv run src/chapter04/developing-a-corporate-documentation-chatbot/rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2661385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.17.0.4:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://43.132.141.4:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!.venv/bin/streamlit run src/chapter04/developing-a-corporate-documentation-chatbot/streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f520c79",
   "metadata": {},
   "source": [
    "## Evaluation and performance considerations\n",
    "\n",
    "Improvements for RAG pipeline\n",
    "1. Integrate a robust retrieval system such as FAISS, Pinecone, or Elasticsearch to fetch real-time sources.\n",
    "1. Scoring mechanisms like precision, recall, and mean reciprocal rank to evaluate retrieval quality.\n",
    "1. Assess answer accuracy by comparing generated responses against ground-truth data or curated references and incorporating human-in-the-loop validation to ensure the outputs are both correct and useful.\n",
    "\n",
    "Other considerations\n",
    "1. Error-handling.\n",
    "1. Building observability into the pipeline by logging API calls, node execution times, and retrieval performance is essential for scaling up and maintaining reliability in production.\n",
    "1. Optimizing API use by leveraging local models when possible, caching common queries, and managing memory efficiently when handling large-scale embeddings further supports cost optimization and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f015d",
   "metadata": {},
   "source": [
    "## Troubleshooting RAG systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ac1d0",
   "metadata": {},
   "source": [
    "Robust design and continuous system calibration:\n",
    "1. Foundational setup: Ensure comprehensive and high-quality document collections, clear prompt formulations, and effective retrieval techniques that enhance precision and relevance.\n",
    "1. Continuous calibration: Regular monitoring, user feedback, and updates to the knowledge base help identify emerging issues during operation.\n",
    "\n",
    "A few common failure points and their remedies are as follows:\n",
    "1. **Missing content**: Prevent this by validating content during ingestion and adding domain-specific resources. Use explicit\n",
    "signals to indicate when information is unavailable.\n",
    "1. **Missed top-ranked documents**: Improve this with advanced embedding models, hybrid semantic-lexical searches, and sentence-level retrieval.\n",
    "1. **Context window limitations**: Optimizing document chunking and extracting the most relevant sentences.\n",
    "1. **Information extraction failure**: LLM fails to synthesize the available context properly. This can be resolved by refining prompt design—using explicit instructions and contrastive examples enhances extraction accuracy.\n",
    "1. **Format compliance issues**: Enforce structured output with parsers, precise format examples, and post-processing validation.\n",
    "1. **Specificity mismatch**: The output may be too general or too detailed. Address this by using query expansion techniques and tailoring prompts based on the user’s expertise level.\n",
    "1. **Incomplete information**: Increase retrieval diversity (e.g., using maximum marginal relevance) and refine query transformation methods to cover all aspects of the query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter04 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
