{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e4ee78",
   "metadata": {},
   "source": [
    "# 04. Building Intelligent RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6998a93",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7324f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m25 packages\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m2                                \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==0.3.79\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.0.2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-core~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df5e8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m115 packages\u001b[0m \u001b[2min 29ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 18ms\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/2] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K██████████░░░░░░░░░░ [1/2] \u001b[2mlangchain-text-splitters==1.0.0                      \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 44ms\u001b[0m\u001b[0m1.0.0a1                         \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.3.31\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==1.0.0a1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==0.3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-chroma~=1.0 langchain-community==1.0.0a1 langchain-openai~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43596ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m22 packages\u001b[0m \u001b[2min 62ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/5] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 113ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjq\u001b[0m\u001b[2m==1.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscann\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install jq~=1.10 python-dotenv~=1.1 scann~=1.4 transformers~=4.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306cc65",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3986693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "        embeddings_model = os.getenv(\"OPENAI_EMBEDDINGS_MODEL\")\n",
    "        hf_pretrained_embeddings_model = os.getenv(\"HF_PRETRAINED_EMBEDDINGS_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.hf_pretrained_embeddings_model = (\n",
    "            hf_pretrained_embeddings_model\n",
    "            if hf_pretrained_embeddings_model\n",
    "            else \"Qwen/Qwen3-Embedding-8B\"\n",
    "        )\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_embeddings(self, **kwargs) -> OpenAIEmbeddings:\n",
    "        if not self.embeddings_model:\n",
    "            raise ValueError(\"OPENAI_EMBEDDINGS_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            model=self.embeddings_model,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.tiktoken_enabled\n",
    "            # 对于非 OpenAI 的官方实现，将这个参数置为 False。\n",
    "            # 回退到用 huggingface transformers 库 AutoTokenizer 来处理 token。\n",
    "            tiktoken_enabled=False,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.model\n",
    "            # 元宝说 Jina 的 embedding 模型 https://huggingface.co/jinaai/jina-embeddings-v4 最接近\n",
    "            # text-embedding-ada-002\n",
    "            # 个人喜好，选了 Qwen/Qwen3-Embedding-8B\n",
    "            # tiktoken_model_name='Qwen/Qwen3-Embedding-8B',\n",
    "            tiktoken_model_name=self.hf_pretrained_embeddings_model,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda46add",
   "metadata": {},
   "source": [
    "## From indexes to intelligent retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1946a4",
   "metadata": {},
   "source": [
    "## Components of a RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9b54a",
   "metadata": {},
   "source": [
    "### When to implement RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5df81",
   "metadata": {},
   "source": [
    "## From embeddings to search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d48d4",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a79e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "Dimensions per embedding: 1024\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embeddings model\n",
    "embeddings_model = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Create embeddings for example sentences\n",
    "text1 = \"The cat sat on the mat\"\n",
    "text2 = \"A feline rested on the carpet\"\n",
    "text3 = \"Python is a programming language\"\n",
    "\n",
    "# Get embeddings using LangChain\n",
    "embeddings = embeddings_model.embed_documents([text1, text2, text3])\n",
    "\n",
    "# These similar sentences will have similar embeddings\n",
    "embedding1 = embeddings[0]  # Embedding for \"The cat sat on the mat\"\n",
    "embedding2 = embeddings[1]  # Embedding for \"A feline rested on the carpet\"\n",
    "embedding3 = embeddings[2]  # Embedding for \"Python is a programming language\"\n",
    "\n",
    "# Output shows number of documents and embedding dimensions\n",
    "print(f\"Number of documents: {len(embeddings)}\")\n",
    "print(f\"Dimensions per embedding: {len(embeddings[0])}\")\n",
    "# Typically 1536 dimensions with OpenAI's embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e21096",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "\n",
    "The vector database operates as an independent system that can be:\n",
    "- Scaled independently of the RAG components\n",
    "- Maintained and optimized separately\n",
    "- Potentially shared across multiple RAG applications\n",
    "- Hosted as a dedicated service\n",
    "\n",
    "When working with embeddings, several challenges arise:\n",
    "- **Scale**: Applications often need to store millions of embeddings\n",
    "- **Dimensionality**: Each embedding might have hundreds or thousands of dimensions\n",
    "- **Search performance**: Finding similar vectors quickly becomes computationally intensive\n",
    "- **Associated data**: We need to maintain connections between vectors and their source documents\n",
    "\n",
    "Vector stores combine two essential components:\n",
    "- **Vector storage**: The actual database that persists vectors and metadata\n",
    "- **Vector index**: A specialized data structure that enables efficient similarity search\n",
    "\n",
    "The curse of dimensionality: as vector dimensions increase, computing similarities becomes increasingly expensive, requiring `O(dN)` operations for `d` dimensions and `N` vectors.\n",
    "\n",
    "\n",
    "Traditional database:\n",
    "- Uses exact matching (equality, ranges)\n",
    "- Optimized for structured data (for example, “find all customers with age > 30”)\n",
    "- Usually utilizes B-trees or hash-based indexes\n",
    "\n",
    "Vector store search:\n",
    "- Uses similarity metrics (cosine similarity, Euclidean distance)\n",
    "- Optimized for high-dimensional vector spaces\n",
    "- Employs Approximate Nearest Neighbor (ANN) algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e5a5d",
   "metadata": {},
   "source": [
    "#### Vector stores comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11c408",
   "metadata": {},
   "source": [
    "#### Hardware considerations for vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622af3e7",
   "metadata": {},
   "source": [
    "#### Vector store interface in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776e9945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='cdb87f66-2c1a-4f8f-b40b-ac1cc23bb874', metadata={'id': 'doc_1'}, page_content='Content about language models'), Document(id='c9fc393a-d6a1-454c-948f-108b4bd775e2', metadata={'id': 'doc_3'}, page_content='Details about retrieval systems'), Document(id='268a82a4-c387-4acd-ab79-54b180b3f95f', metadata={'id': 'doc_2'}, page_content='Information about vector databases')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize with an embedding model\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Create some sample documents with explicit IDs\n",
    "docs = [\n",
    "    Document(page_content=\"Content about language models\", metadata={\"id\": \"doc_1\"}),\n",
    "    Document(\n",
    "        page_content=\"Information about vector databases\", metadata={\"id\": \"doc_2\"}\n",
    "    ),\n",
    "    Document(page_content=\"Details about retrieval systems\", metadata={\"id\": \"doc_3\"}),\n",
    "]\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = Chroma(embedding_function=embeddings)\n",
    "\n",
    "# Add documents with explicit IDs\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "# Similarity Search with appropriate k value\n",
    "results = vector_store.similarity_search(\"How do language models work?\", k=2)\n",
    "\n",
    "# For maximum marginal relevance search, adjust the parameters based on available documents\n",
    "# Find relevant BUT diverse documents (reduce redundancy)\n",
    "results = vector_store.max_marginal_relevance_search(\n",
    "    \"How does LangChain work?\",\n",
    "    k=3,\n",
    "    fetch_k=10,\n",
    "    lambda_mult=0.5,  # Controls diversity (0=max diversity, 1=max relevance)\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dddd24",
   "metadata": {},
   "source": [
    "### Vector indexing strategies\n",
    "\n",
    "Some common indexing approaches include:\n",
    "- **Tree-based structures** that hierarchically divide the vector space\n",
    "- **Graph-based methods** like **Hierarchical Navigable Small World (HNSW)** that create navigable networks of connected vectors\n",
    "- **Hashing techniques** that map similar vectors to the same “buckets”\n",
    "\n",
    "faiss 库不支持 python3.12。google 的 ScaNN 库没找到接口文档。\n",
    "TODO：用 ScaNN 复现书中代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c65ab6",
   "metadata": {},
   "source": [
    "## Breaking down the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ca7d6",
   "metadata": {},
   "source": [
    "1. Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c934641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cece6",
   "metadata": {},
   "source": [
    "2. Make embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d203e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Config().new_openai_like_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79be16",
   "metadata": {},
   "source": [
    "3. Store in vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4686cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import ScaNN\n",
    "\n",
    "vector_db = ScaNN.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb3a9a2",
   "metadata": {},
   "source": [
    "4. Retrieve similar docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67fa6ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the effects of climate change?\"\n",
    "\n",
    "vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800c0e8",
   "metadata": {},
   "source": [
    "### Document processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e028873",
   "metadata": {},
   "source": [
    "A document loader is a component in LangChain that transforms various data sources into a standardized document format that can be used throughout the LangChain ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d214ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'), Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea71d6",
   "metadata": {},
   "source": [
    "#### Chunking strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88b2a6",
   "metadata": {},
   "source": [
    "##### Fixed-size chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0151a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-text-splitters~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f7d9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13 chunks from document\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",  # Split on spaces to avoid breaking words\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Generated {len(chunks)} chunks from document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf5f45",
   "metadata": {},
   "source": [
    "##### Recursive character chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f942af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.',\n",
       " 'It helps address hallucinations by grounding responses in retrieved information.',\n",
       " '## Key Components\\nRAG consists of several components:\\n1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4. Augmentation\\n5. Generation',\n",
       " '### Document Processing\\nThis step involves loading and chunking documents appropriately.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], chunk_size=150, chunk_overlap=20\n",
    ")\n",
    "\n",
    "document = \"\"\"# Introduction to RAG\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.\n",
    "\n",
    "It helps address hallucinations by grounding responses in retrieved information.\n",
    "\n",
    "## Key Components\n",
    "RAG consists of several components:\n",
    "1. Document processing\n",
    "2. Vector embedding\n",
    "3. Retrieval\n",
    "4. Augmentation\n",
    "5. Generation\n",
    "\n",
    "### Document Processing\n",
    "This step involves loading and chunking documents appropriately.\n",
    "\"\"\"\n",
    "\n",
    "text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa1e5a",
   "metadata": {},
   "source": [
    "##### Document-specific chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd464ed",
   "metadata": {},
   "source": [
    "##### Semantic chunking\n",
    "\n",
    "TODO: langchain-experimental 的最新版本 v0.3 依赖的 langchain 版本为 v0.3，待 langchain-experimental 升级后修复下述程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6d455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 551ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 22ms\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/5] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 80ms\u001b[0m\u001b[0m0.3.31                          \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==0.3.27\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==1.0.0a1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.3.31\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==0.3.79\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-experimental\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==0.3.11\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-experimental~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493a08ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models. It helps address hallucinations by grounding responses in retrieved information. ## Key Components\\nRAG consists of several components:\\n1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4.',\n",
       " 'Augmentation\\n5. Generation\\n\\n### Document Processing\\nThis step involves loading and chunking documents appropriately. ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings, add_start_index=True  # Include position metadata\n",
    ")\n",
    "\n",
    "text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31191d3",
   "metadata": {},
   "source": [
    "##### Agent-based chunking\n",
    "Uses LLMs to intelligently divide text based on semantic analysis and content understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874330f",
   "metadata": {},
   "source": [
    "##### Multi-modal chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bee5e7",
   "metadata": {},
   "source": [
    "##### Choosing the right chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8df8fa",
   "metadata": {},
   "source": [
    "#### Retrieval\n",
    "Retrieval integrates a vector store with other LangChain components for simplified querying and compatibility.\n",
    "\n",
    "A retriever in LangChain follows a pattern:\n",
    "- Input: Takes a query as a string\n",
    "- Processing: Applies retrieval logic specific to the implementation\n",
    "- Output: Returns a list of document objects, each containing:\n",
    "  - `page_content`: The actual document content\n",
    "  - `metadata`: Associated information like document ID or source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051ef66",
   "metadata": {},
   "source": [
    "##### LangChain retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434b109",
   "metadata": {},
   "source": [
    "##### Vector store retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acb5c6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import KNNRetriever\n",
    "\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "retriever = KNNRetriever.from_documents(documents, embeddings)\n",
    "retriever.invoke(\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcf898b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 44ms\u001b[0m\u001b[0m                                           \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxmltodict\u001b[0m\u001b[2m==1.0.2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install xmltodict~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d1f696",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m retriever = PubMedRetriever(email=\u001b[33m\"\u001b[39m\u001b[33mxiangminli@outlook.com\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# FIXME: 没有跑通\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchatgpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_core/retrievers.py:216\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    220\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/retrievers/pubmed.py:20\u001b[39m, in \u001b[36mPubMedRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_relevant_documents\u001b[39m(\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, *, run_manager: CallbackManagerForRetrieverRun\n\u001b[32m     19\u001b[39m ) -> List[Document]:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/utilities/pubmed.py:132\u001b[39m, in \u001b[36mPubMedAPIWrapper.load_docs\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/utilities/pubmed.py:128\u001b[39m, in \u001b[36mPubMedAPIWrapper.lazy_load_docs\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load_docs\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> Iterator[Document]:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict2document\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain_community/utilities/pubmed.py:110\u001b[39m, in \u001b[36mPubMedAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    108\u001b[39m result = urllib.request.urlopen(url)\n\u001b[32m    109\u001b[39m text = result.read().decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m json_text = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m webenv = json_text[\u001b[33m\"\u001b[39m\u001b[33mesearchresult\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mwebenv\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m json_text[\u001b[33m\"\u001b[39m\u001b[33mesearchresult\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33midlist\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/json/decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     end = _w(s, end).end()\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/json/decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import PubMedRetriever\n",
    "\n",
    "retriever = PubMedRetriever(email=\"xiangminli@outlook.com\")\n",
    "# FIXME: 没有跑通\n",
    "results = retriever.invoke(\"chatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c889ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m8 packages\u001b[0m \u001b[2min 282ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/3] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install arxiv~=2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75deb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/2503.04758v2', 'Published': datetime.date(2025, 3, 10), 'Title': 'Chat-GPT: An AI Based Educational Revolution', 'Authors': 'Sasa Maric, Sonja Maric, Lana Maric'}, page_content='The AI revolution is gathering momentum at an unprecedented rate. Over the\\npast decade, we have witnessed a seemingly inevitable integration of AI in\\nevery facet of our lives. Much has been written about the potential\\nrevolutionary impact of AI in education. AI has the potential to completely\\nrevolutionise the educational landscape as we could see entire courses and\\ndegrees developed by programs such as ChatGPT. AI has the potential to develop\\ncourses, set assignments, grade and provide feedback to students much faster\\nthan a team of teachers. In addition, because of its dynamic nature, it has the\\npotential to continuously improve its content. In certain fields such as\\ncomputer science, where technology is continuously evolving, AI based\\napplications can provide dynamically changing, relevant material to students.\\nAI has the potential to replace entire degrees and may challenge the concept of\\nhigher education institutions. We could also see entire new disciplines emerge\\nas a consequence of AI. This paper examines the practical impact of ChatGPT and\\nwhy it is believed that its implementation is a critical step towards a new era\\nof education. We investigate the impact that ChatGPT will have on learning,\\nproblem solving skills and cognitive ability of students. We examine the\\npositives, negatives and many other aspects of AI and its applications\\nthroughout this paper.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "retriever = ArxivRetriever(\n",
    "    load_max_docs=2,\n",
    "    # get_ful_documents=True,\n",
    ")\n",
    "\n",
    "retriever.invoke(\"chat-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba012bb8",
   "metadata": {},
   "source": [
    "### Advanced RAG techniques\n",
    "\n",
    "A standard vector search has several limitations:\n",
    "- It might miss contextually relevant documents that use different terminology\n",
    "- It can’t distinguish between authoritative and less reliable sources\n",
    "- It might return redundant or contradictory information\n",
    "- It has no way to verify if generated responses accurately reflect the source material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e47228",
   "metadata": {},
   "source": [
    "#### Hybrid retrieval: Combining semantic and keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb9af5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 51ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrank-bm25\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install rank-bm25~=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35039b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize with an embedding model\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Create some sample documents with explicit IDs\n",
    "docs = [\n",
    "    Document(page_content=\"Content about language models\", metadata={\"id\": \"doc_1\"}),\n",
    "    Document(\n",
    "        page_content=\"Information about vector databases\", metadata={\"id\": \"doc_2\"}\n",
    "    ),\n",
    "    Document(page_content=\"Details about retrieval systems\", metadata={\"id\": \"doc_3\"}),\n",
    "]\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = Chroma(embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bba4112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# https://docs.langchain.com/oss/python/migrate/langchain-v1#langchain-classic\n",
    "%uv pip install langchain-classic~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91d51824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='cdb87f66-2c1a-4f8f-b40b-ac1cc23bb874', metadata={'id': 'doc_1'}, page_content='Content about language models'),\n",
       " Document(id='268a82a4-c387-4acd-ab79-54b180b3f95f', metadata={'id': 'doc_2'}, page_content='Information about vector databases'),\n",
       " Document(id='c9fc393a-d6a1-454c-948f-108b4bd775e2', metadata={'id': 'doc_3'}, page_content='Details about retrieval systems'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\")]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Setup semantic retriever\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Setup lexical retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# Combine retrievers\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.7, 0.3],  # Weight semantic search higher than keyword search\n",
    ")\n",
    "\n",
    "hybrid_retriever.invoke(\"climate change impacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11623d",
   "metadata": {},
   "source": [
    "#### Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99054385",
   "metadata": {},
   "source": [
    "#### Query transformation: Improving retrieval through better queries\n",
    "\n",
    "Query expansion generates multiple variations of the original query to capture different aspects or phrasings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca0d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. How is climate change impacting the environment and human societies?  \n",
      "2. What are the consequences of global warming and changing climate patterns?  \n",
      "3. In what ways has climate change altered ecosystems, weather, and daily life?\n"
     ]
    }
   ],
   "source": [
    "# 用 langchain-classic 不用 langchain-core 的原理参见\n",
    "# https://docs.langchain.com/oss/python/migrate/langchain-v1#langchain-classic\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "expansion_template = \"\"\"Given the user question: {question}\n",
    "Generate three alternative versions that express the same information need but with different wording:\n",
    "1.\"\"\"\n",
    "\n",
    "expansion_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"], template=expansion_template\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like(temperature=0.7)\n",
    "expansion_chain = expansion_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate expanded queries\n",
    "original_query = \"What are the effects of climate change?\"\n",
    "reply = expansion_chain.invoke(original_query)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d8c0f",
   "metadata": {},
   "source": [
    "##### Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "HyDE uses an LLM to generate a hypothetical answer document based on the query, and then uses that document’s embedding for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "997ef552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import ScaNN\n",
    "\n",
    "# Load a json file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"static/knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "embedder = Config().new_openai_like_embeddings()\n",
    "\n",
    "vector_db = ScaNN.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfa4bd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用 langchain-classic 不用 langchain-core 的原理参见\n",
    "# https://docs.langchain.com/oss/python/migrate/langchain-v1#langchain-classic\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Create prompt for generating hypothetical document\n",
    "hyde_template = \"\"\"Based on the question: {question}\n",
    "Write a passage that could contain the answer to this question:\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate(input_variables=[\"question\"], template=hyde_template)\n",
    "llm = Config().new_openai_like(temperature=0.2)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate hypothetical document\n",
    "query = \"What dietary changes can reduce carbon footprint?\"\n",
    "hypothetical_doc = hyde_chain.invoke(query)\n",
    "\n",
    "# Use the hypothetical document for retrieval\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "embedded_query = embeddings.embed_query(hypothetical_doc)\n",
    "vector_db.similarity_search_by_vector(embedded_query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f268dce",
   "metadata": {},
   "source": [
    "#### Context processing: maximizing retrieved information value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e76400",
   "metadata": {},
   "source": [
    "##### Contextual compression\n",
    "\n",
    "Extracts only the most relevant parts of retrieved documents, removing irrelevant content that might distract the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72db25dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Create a basic retriever from the vector store\n",
    "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "compression_retriever.invoke(\"How do transformers work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbea1dc",
   "metadata": {},
   "source": [
    "##### Maximum marginal relevance\n",
    "\n",
    "FAISS 不支持 python 3.12，ScaNN 没实现 `max_marginal_relevance_search`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4aa1b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8de3bfa9-f133-4053-951b-0bcdaaea757f', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(id='52b4e8fa-49aa-4270-ac89-8b27e4227af3', metadata={'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'),\n",
       " Document(id='cdb87f66-2c1a-4f8f-b40b-ac1cc23bb874', metadata={'id': 'doc_1'}, page_content='Content about language models'),\n",
       " Document(id='06f71b44-92bf-411f-bb54-4fd607bbb185', metadata={'seq_num': 4, 'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json'}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(id='45433e41-1367-4130-8b7c-fcdb9d9cd1f8', metadata={'seq_num': 5, 'source': '/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/static/knowledge_base.json'}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize with an embedding model\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# For maximum marginal relevance search, adjust the parameters based on available documents\n",
    "# Find relevant BUT diverse documents (reduce redundancy)\n",
    "vector_store.max_marginal_relevance_search(\n",
    "    query=\"What are transformer models?\",\n",
    "    k=5,  # Number of documents to return\n",
    "    fetch_k=20,  # Number of documents to initially fetch\n",
    "    lambda_mult=0.5,  # Diversity parameter (0 = max diversity, 1 = max relevance)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e32d00",
   "metadata": {},
   "source": [
    "#### Response enhancement: Improving generator output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9ad7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n",
    "        metadata={\"source\": \"Neural Network Review 2021\", \"page\": 42},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.\",\n",
    "        metadata={\"source\": \"Introduction to NLP\", \"page\": 137},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"GPT models are autoregressive transformers that predict the next token based on previous tokens.\",\n",
    "        metadata={\"source\": \"Large Language Models Survey\", \"page\": 89},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e181d5",
   "metadata": {},
   "source": [
    "##### Source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03b6eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import ScaNN\n",
    "\n",
    "\n",
    "# Create a vector store and retriever\n",
    "embeddings = Config().new_openai_like_embeddings()\n",
    "vector_store = ScaNN.from_documents(documents, embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Source attribution prompt template\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a precise AI assistant that provides well-sourced information.\n",
    "Answer the following question based ONLY on the provided sources. For each fact or claim in your answer,\n",
    "include a citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "Your answer:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7e993e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create a source-formatted string from documents\n",
    "def format_sources_with_citations(docs):\n",
    "    formatted_sources = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source_info = f\"[{i}] {doc.metadata.get('source', 'Unknown source')}\"\n",
    "        if doc.metadata.get(\"page\"):\n",
    "            source_info += f\", page {doc.metadata['page']}\"\n",
    "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted_sources)\n",
    "\n",
    "\n",
    "# Build the RAG chain with source attribution\n",
    "def generate_attributed_response(question):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    # Format sources with citation numbers\n",
    "    sources_formatted = format_sources_with_citations(retrieved_docs)\n",
    "    # print(sources_formatted)\n",
    "\n",
    "    # Create the attribution chain using LCEL\n",
    "    attribution_chain = (\n",
    "        attribution_prompt | Config().new_openai_like(temperature=0) | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Generate the response with citations\n",
    "    response = attribution_chain.invoke(\n",
    "        {\"question\": question, \"sources\": sources_formatted}\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70ce9591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer models are a type of neural network architecture introduced in 2017 by Vaswani et al. in the paper \"Attention is All You Need\" [2]. They rely primarily on attention mechanisms to process input sequences, allowing them to weigh the importance of different words in a sequence when making predictions, rather than relying on sequential processing like recurrent networks [2]. \n",
      "\n",
      "One key characteristic of transformer models is their ability to handle long-range dependencies efficiently. GPT models are examples of autoregressive transformers that predict the next token based on previous tokens [1]. In contrast, BERT is a transformer-based model that uses bidirectional training, meaning it considers context from both left and right sides of a word simultaneously, and is trained using masked language modeling and next sentence prediction tasks [3].\n",
      "\n",
      "Examples of transformer models include GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) [1][3].\n",
      "\n",
      "References:\n",
      "[1] Large Language Models Survey, page 89  \n",
      "[2] Neural Network Review 2021, page 42  \n",
      "[3] Introduction to NLP, page 137\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"How do transformer models work and what are some examples?\"\n",
    "attributed_answer = generate_attributed_response(question)\n",
    "print(attributed_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c42ce7",
   "metadata": {},
   "source": [
    "##### Self-consistency checking: ensuring factual accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f7029e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def verify_response_accuracy(\n",
    "    retrieved_docs: list[Document], generated_answer: str, llm: ChatOpenAI | None = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Verify if a generated answer is fully supported by the retrieved documents.\n",
    "    Args:\n",
    "        retrieved_docs: List of documents used to generate the answer\n",
    "        generated_answer: The answer produced by the RAG system\n",
    "        llm: Language model to use for verification\n",
    "    Returns:\n",
    "        Dictionary containing verification results and any identified issues\n",
    "    \"\"\"\n",
    "    if llm is None:\n",
    "        llm = Config().new_openai_like(temperature=0)\n",
    "\n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Define verification prompt - fixed to avoid JSON formatting issues in the template\n",
    "    verification_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    As a fact-checking assistant, verify whether the following answer is fully supported\n",
    "    by the provided context. Identify any statements that are not supported or contradict the context.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer to verify:\n",
    "    {answer}\n",
    "    \n",
    "    Perform a detailed analysis with the following structure:\n",
    "    1. List any factual claims in the answer\n",
    "    2. For each claim, indicate whether it is:\n",
    "       - Fully supported (provide the supporting text from context)\n",
    "       - Partially supported (explain what parts lack support)\n",
    "       - Contradicted (identify the contradiction)\n",
    "       - Not mentioned in context\n",
    "    3. Overall assessment: Is the answer fully grounded in the context?\n",
    "    \n",
    "    Return your analysis in JSON format with the following structure:\n",
    "    {{\n",
    "      \"claims\": [\n",
    "        {{\n",
    "          \"claim\": \"The factual claim\",\n",
    "          \"status\": \"fully_supported|partially_supported|contradicted|not_mentioned\",\n",
    "          \"evidence\": \"Supporting or contradicting text from context\",\n",
    "          \"explanation\": \"Your explanation\"\n",
    "        }}\n",
    "      ],\n",
    "      \"fully_grounded\": true|false,\n",
    "      \"issues_identified\": [\"List any specific issues\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create verification chain using LCEL\n",
    "    verification_chain = verification_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run verification\n",
    "    result = verification_chain.invoke({\"context\": context, \"answer\": generated_answer})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9efe6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"The transformer architecture was introduced by OpenAI in 2018\",\n",
      "      \"status\": \"contradicted\",\n",
      "      \"evidence\": \"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\",\n",
      "      \"explanation\": \"The context clearly states that the transformer architecture was introduced in 2017 by Vaswani et al., not by OpenAI in 2018. This directly contradicts the claim.\"\n",
      "    },\n",
      "    {\n",
      "      \"claim\": \"The transformer architecture ... uses recurrent neural networks\",\n",
      "      \"status\": \"contradicted\",\n",
      "      \"evidence\": \"It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\",\n",
      "      \"explanation\": \"The context explicitly states that transformers use self-attention mechanisms instead of recurrent neural networks, making this claim directly false.\"\n",
      "    },\n",
      "    {\n",
      "      \"claim\": \"BERT is a transformer model developed by Google\",\n",
      "      \"status\": \"fully_supported\",\n",
      "      \"evidence\": \"BERT is a transformer-based model developed by Google\",\n",
      "      \"explanation\": \"This claim is directly supported by the context, which states that BERT is a transformer-based model developed by Google.\"\n",
      "    }\n",
      "  ],\n",
      "  \"fully_grounded\": false,\n",
      "  \"issues_identified\": [\n",
      "    \"Incorrect attribution of transformer introduction to OpenAI in 2018\",\n",
      "    \"False claim that transformers use recurrent neural networks\",\n",
      "    \"Correct statement about BERT being a Google-developed transformer model does not compensate for the two major contradictions\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "retrieved_docs = [\n",
    "    Document(\n",
    "        page_content=\"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\"\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "generated_answer = \"The transformer architecture was introduced by OpenAI in 2018 and uses recurrent neural networks. BERT is a transformer model developed by Google.\"\n",
    "\n",
    "verification_result = verify_response_accuracy(retrieved_docs, generated_answer)\n",
    "print(verification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c36a84",
   "metadata": {},
   "source": [
    "#### Corrective RAG\n",
    "\n",
    "In real-world applications, retrieval systems often return irrelevant, insufficient, or even misleading content.\n",
    "\n",
    "Corrective Retrieval-Augmented Generation (CRAG) directly addresses this challenge by introducing explicit evaluation and correction mechanisms into the RAG pipeline.\n",
    "\n",
    "缺失完整示例代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575734f9",
   "metadata": {},
   "source": [
    "#### Agentic RAG\n",
    "\n",
    "CRAG primarily enhances data quality through evaluation and correction, while agentic RAG focuses on process intelligence through autonomous planning and orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ac846",
   "metadata": {},
   "source": [
    "#### Choosing the right techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389c4fb",
   "metadata": {},
   "source": [
    "## Developing a corporate documentation chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0108825",
   "metadata": {},
   "source": [
    "源码参见 src/chapter04/developing-a-corporate-documentation-chatbot 目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e5e3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m60 packages\u001b[0m \u001b[2min 549ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m     0 B/153.08 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m     0 B/153.08 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 16.00 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 32.00 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 48.00 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[1A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 60.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------------\u001b[0m\u001b[0m 76.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)[2m-----------\u001b[0m\u001b[0m 92.25 KiB/153.08 KiB        \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--\u001b[2m--------\u001b[0m\u001b[0m 108.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 14.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)-----\u001b[2m-----\u001b[0m\u001b[0m 124.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 30.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)-----\u001b[2m-----\u001b[0m\u001b[0m 124.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 30.87 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------\u001b[2m--\u001b[0m\u001b[0m 140.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.48 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)--------\u001b[2m--\u001b[0m\u001b[0m 140.25 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.48 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 153.08 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2mlanggraph-prebuilt  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 33.48 KiB/33.48 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 153.08 KiB/153.08 KiB       \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)----------\u001b[2m\u001b[0m\u001b[0m 153.08 KiB/153.08 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 599ms\u001b[0m\u001b[0m                                                 \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/23] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m23 packages\u001b[0m \u001b[2min 143ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maltair\u001b[0m\u001b[2m==5.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblinker\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnarwhals\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==21.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydeck\u001b[0m\u001b[2m==0.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstreamlit\u001b[0m\u001b[2m==1.51.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtoml\u001b[0m\u001b[2m==0.10.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchdog\u001b[0m\u001b[2m==6.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langgraph~=1.0 streamlit~=1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe36111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter04/.venv/lib/python3.12/site-packages/langchain/embeddings/cache.py:58: UserWarning: Using default key encoder: SHA-1 is *not* collision-resistant. While acceptable for most cache scenarios, a motivated attacker can craft two different payloads that map to the same cache key. If that risk matters in your environment, supply a stronger encoder (e.g. SHA-256 or BLAKE2) via the `key_encoder` argument. If you change the key encoder, consider also creating a new cache, to avoid (the potential for) collisions with existing keys.\n",
      "  _warn_about_sha1_encoder()\n",
      "[]\n",
      "INFO:httpx:HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The square root of 10 is approximately 3.1623.\n",
      "\n",
      "None of the provided corporate document snippets are relevant to this mathematical question.\n",
      "INFO:httpx:HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "no issues detected\n",
      "The square root of 10 is approximately 3.1623.\n",
      "\n",
      "None of the provided corporate document snippets are relevant to this mathematical question.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv run src/chapter04/developing-a-corporate-documentation-chatbot/rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2661385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.17.0.4:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://43.132.141.4:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!.venv/bin/streamlit run src/chapter04/developing-a-corporate-documentation-chatbot/streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f015d",
   "metadata": {},
   "source": [
    "## Troubleshooting RAG systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter04 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
