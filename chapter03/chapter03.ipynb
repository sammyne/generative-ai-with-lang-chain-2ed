{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65bd6c65",
   "metadata": {},
   "source": [
    "# 03. Building Workflows with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe4139",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langgraph==0.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b752b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-core~=0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e915003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-openai~=0.3.0 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf49e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain~=0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa567f43",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_vl(self, **kwargs) -> ChatOpenAI:\n",
    "        if not self.vl_model:\n",
    "            raise ValueError(\"OPENAI_VL_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.vl_model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cedeac",
   "metadata": {},
   "source": [
    "## LangGraph fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00a60e",
   "metadata": {},
   "source": [
    "### State management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff864fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class JobApplicationState(TypedDict):\n",
    "    job_description: str\n",
    "    is_suitable: bool\n",
    "    application: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "def analyze_job_description(state):\n",
    "    print(\"...Analyzing a provided job description ...\")\n",
    "    return {\"is_suitable\": len(state[\"job_description\"]) > 100}\n",
    "\n",
    "def generate_application(state):\n",
    "    print(\"...generating application...\")\n",
    "    return {\"application\": \"some_fake_application\"}\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_edge(\"analyze_job_description\", \"generate_application\")\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"job_description\":\"fake_jd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "\n",
    "\n",
    "def is_suitable_condition(\n",
    "    state: JobApplicationState,\n",
    ") -> Literal[\"generate_application\", END]:\n",
    "    if state.get(\"is_suitable\"):\n",
    "        return \"generate_application\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\"analyze_job_description\", is_suitable_condition)\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59442174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe88165",
   "metadata": {},
   "source": [
    "### Reducers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0873c7",
   "metadata": {},
   "source": [
    "#### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f953ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobApplicationState(TypedDict):\n",
    "    job_description: str\n",
    "    is_suitable: bool\n",
    "    application: str\n",
    "    actions: list[str]\n",
    "\n",
    "\n",
    "def analyze_job_description(state):\n",
    "    print(\"...Analyzing a provided job description ...\")\n",
    "    result = {\n",
    "        \"is_suitable\": len(state[\"job_description\"]) < 100,\n",
    "        \"actions\": [\"action1\"],\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_application(state):\n",
    "    print(\"...generating application...\")\n",
    "    return {\"application\": \"some_fake_application\", \"actions\": [\"action2\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\"analyze_job_description\", is_suitable_condition)\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in graph.astream(\n",
    "    input={\"job_description\":\"fake_jd\"},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cea487",
   "metadata": {},
   "source": [
    "#### Option 2 - use `add` method as a reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83beed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional\n",
    "from operator import add\n",
    "\n",
    "\n",
    "class JobApplicationState(TypedDict):\n",
    "    job_description: str\n",
    "    is_suitable: bool\n",
    "    application: str\n",
    "    actions: Annotated[list[str], add]\n",
    "\n",
    "\n",
    "def analyze_job_description(state):\n",
    "    print(\"...Analyzing a provided job description ...\")\n",
    "    result = {\n",
    "        \"is_suitable\": len(state[\"job_description\"]) < 100,\n",
    "        \"actions\": [\"action1\"],\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_application(state):\n",
    "    print(\"...generating application...\")\n",
    "    return {\"application\": \"some_fake_application\", \"actions\": [\"action2\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\"analyze_job_description\", is_suitable_condition)\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b003cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45370c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in graph.astream(\n",
    "    input={\"job_description\":\"fake_jd\"},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93205dbf",
   "metadata": {},
   "source": [
    "#### Option 3 - customize reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4593b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional, Union\n",
    "\n",
    "\n",
    "def my_reducer(left: list[str], right: Optional[Union[str, list[str]]]) -> list[str]:\n",
    "    if right:\n",
    "        return left + [right] if isinstance(right, str) else left + right\n",
    "    return left\n",
    "\n",
    "\n",
    "class JobApplicationState(TypedDict):\n",
    "    job_description: str\n",
    "    is_suitable: bool\n",
    "    application: str\n",
    "    actions: Annotated[list[str], my_reducer]\n",
    "\n",
    "\n",
    "def analyze_job_description(state):\n",
    "    print(\"...Analyzing a provided job description ...\")\n",
    "    result = {\"is_suitable\": len(state[\"job_description\"]) < 100, \"actions\": \"action1\"}\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_application(state):\n",
    "    print(\"...generating application...\")\n",
    "    return {\"application\": \"some_fake_application\", \"actions\": [\"action2\", \"action3\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\"analyze_job_description\", is_suitable_condition)\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96812ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in graph.astream(\n",
    "    input={\"job_description\":\"fake_jd\"},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6728282",
   "metadata": {},
   "source": [
    "### Making graphs configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "\n",
    "def generate_application(state: JobApplicationState, config: RunnableConfig):\n",
    "    model_provider = config[\"configurable\"].get(\"model_provider\", \"Google\")\n",
    "    model_name = config[\"configurable\"].get(\"model_name\", \"gemini-2.0-flash\")\n",
    "    print(f\"...generating application with {model_provider} and {model_name} ...\")\n",
    "    return {\"application\": \"some_fake_application\", \"actions\": [\"action2\", \"action3\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\"analyze_job_description\", is_suitable_condition)\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"job_description\":\"fake_jd\"}, config={\"configurable\": {\"model_provider\": \"OpenAI\", \"model_name\": \"gpt-4o\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fffddc",
   "metadata": {},
   "source": [
    "### Controlled output generation\n",
    "#### Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    "SPS-Software Engineer (m/w/d) im Maschinenbau\n",
    "Glaston Germany GmbH\n",
    "Neuhausen-Hamberg\n",
    "Feste Anstellung\n",
    "Homeoffice möglich, Vollzeit\n",
    "Erschienen: vor 1 Tag\n",
    "Glaston Germany GmbH logo\n",
    "SPS-Software Engineer (m/w/d) im Maschinenbau\n",
    "Glaston Germany GmbH\n",
    "slide number 1slide number 2slide number 3\n",
    "Glaston ist eine internationale Marke mit weltweit führenden Unternehmen, die für zukunftsweisende Maschinen, Anlagen, Systeme und Dienstleistungen in der Bearbeitung von Architektur-, Fahrzeug- und Displayglas steht.\n",
    "\n",
    "Mit unserer über 50-jährigen Erfahrung am Standort Glaston Germany GmbH in Neuhausen bei Pforzheim verbessern und sichern wir nachhaltig die Produktivität unserer Kunden bei der Fertigung von Architekturglas. Diesen Erfolg verdanken wir unseren motivierten und engagierten Mitarbeitenden und wurden so zu einem der führenden Anbieter von automatisierten und kundenspezifischen Anlagen.\n",
    "\n",
    "Der Umgang mit Software liegt dir im Blut und du möchtest bei einem Hidden Champion durchstarten?\n",
    "Dein Faible für Softwarelösungen und dein Herz für unterschiedliche Technologien sind ideale Voraussetzungen, um Maschinen wieder zu alter Stärke zu verhelfen?\n",
    "Du hast einen ausgeprägten Servicegedanken und Spaß an der Arbeit mit Kunden?\n",
    "\n",
    "Dann komm zu Glaston! Wir suchen ab sofort für unseren Bereich Service Upgrades Verstärkung!\n",
    "\n",
    "SPS-SOFTWARE ENGINEER (M/W/D) IM MASCHINENBAU\n",
    "\n",
    "Als SPS-Software Engineer (m/w/d) im Maschinenbau sind deine Aufgabengebiete:\n",
    "Ausarbeitung und Weiterentwicklung von Kundenaufträgen und Upgrade-Konzepten\n",
    "Selbstständige und termingerechte Bearbeitung von Kundenprojekten und Bereitstellung der notwendigen Dokumente\n",
    "Unterstützung des Inbetriebnahme- und Servicepersonals im Haus und beim Kunden vor Ort\n",
    "Diese Anforderungen musst du mitbringen:\n",
    "Qualifizierte technische Ausbildung: Techniker, Studium oder vergleichbare Qualifikation\n",
    "Mehrjährige Berufserfahrung im Serviceumfeld, idealerweise im Maschinen- und Anlagenbau\n",
    "Umfangreiche Kenntnisse in verschiedenen SPS-Programmiersprachen (z.B. S7Classic, TIA, Simotion)\n",
    "Bei uns profitierst du von folgenden Benefits:\n",
    "Exzellente Rahmenbedingungen (z.B. attraktives Gehaltsmodell, flexible Arbeitszeiten mit Gleitzeit und Homeoffice-Möglichkeiten)\n",
    "Attraktives Arbeitsumfeld in idyllisch-ländlicher Lage\n",
    "Umfangreiche Mobilitätsförderung (z.B. Ladestation für Elektroautos)\n",
    "Wellbeing am Arbeitsplatz\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Config().new_openai_like()\n",
    "\n",
    "prompt_template = (\n",
    "    \"Given a job description, decide whether it suites a junior Java developer.\"\n",
    "    \"\\nJOB DESCRIPTION:\\n{job_description}\\n\"\n",
    ")\n",
    "\n",
    "llm.invoke(prompt_template.format(job_description=job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d927bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_enum = (\n",
    "    \"Given a job description, decide whether it suites a junior Java developer.\"\n",
    "    \"\\nJOB DESCRIPTION:\\n{job_description}\\n\\nAnswer only YES or NO.\"\n",
    ")\n",
    "result = llm.invoke(prompt_template_enum.format(job_description=job_description))\n",
    "\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617af28",
   "metadata": {},
   "source": [
    "Out-of-box parser from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ec6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from langchain.output_parsers import EnumOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "class IsSuitableJobEnum(Enum):\n",
    "    YES = \"YES\"\n",
    "    NO = \"NO\"\n",
    "\n",
    "parser = EnumOutputParser(enum=IsSuitableJobEnum)\n",
    "\n",
    "assert parser.invoke(\"NO\") == IsSuitableJobEnum.NO\n",
    "assert parser.invoke(\"YES\\n\") == IsSuitableJobEnum.YES\n",
    "assert parser.invoke(\" YES \\n\") == IsSuitableJobEnum.YES\n",
    "assert parser.invoke(HumanMessage(content=\" YES \\n\")) == IsSuitableJobEnum.YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | parser\n",
    "\n",
    "chain.invoke(prompt_template_enum.format(job_description=job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4769f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class JobApplicationState(TypedDict):\n",
    "    job_description: str\n",
    "    is_suitable: IsSuitableJobEnum\n",
    "    application: str\n",
    "\n",
    "analyze_chain = llm | parser\n",
    "\n",
    "\n",
    "def analyze_job_description(state):\n",
    "    job_description = state[\"job_description\"]\n",
    "    prompt = prompt_template_enum.format(job_description=job_description)\n",
    "    result = analyze_chain.invoke(prompt)\n",
    "    return {\"is_suitable\": result}\n",
    "\n",
    "\n",
    "def is_suitable_condition(state: JobApplicationState):\n",
    "    return state[\"is_suitable\"] == IsSuitableJobEnum.YES\n",
    "\n",
    "\n",
    "def generate_application(state):\n",
    "    print(\"...generating application...\")\n",
    "    return {\"application\": \"some_fake_application\", \"actions\": [\"action2\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\n",
    "    \"analyze_job_description\", is_suitable_condition,\n",
    "     {True: \"generate_application\", False: END})\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b662e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2b4af",
   "metadata": {},
   "source": [
    "#### Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa560498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import GenericFakeChatModel\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "class MessagesIterator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self._count += 1\n",
    "        if self._count % 2 == 1:\n",
    "            raise ValueError(\"Something went wrong\")\n",
    "        return AIMessage(content=\"YES\")\n",
    "\n",
    "fake_llm = GenericFakeChatModel(messages=MessagesIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ac43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "openai_like_llm = Config().new_openai_like()\n",
    "\n",
    "llms = {\"fake\": fake_llm, \"openai-like\": openai_like_llm }\n",
    "\n",
    "\n",
    "def analyze_job_description(state, config: RunnableConfig):\n",
    "    try:\n",
    "        model_provider = config[\"configurable\"].get(\"model_provider\", \"openai-like\")\n",
    "        llm = llms[model_provider]\n",
    "        analyze_chain = llm | parser\n",
    "        prompt = prompt_template_enum.format(job_description=job_description)\n",
    "        result = analyze_chain.invoke(prompt)\n",
    "        return {\"is_suitable\": result}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception {e} occurred while executing analyze_job_description\")\n",
    "    return {\"is_suitable\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, Literal\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class JobApplicationState(TypedDict):\n",
    "    job_description: str\n",
    "    is_suitable: bool\n",
    "    application: str\n",
    "    actions: Annotated[list[str], add]\n",
    "\n",
    "def generate_application(state):\n",
    "    print(\"...generating application...\")\n",
    "    return {\"application\": \"some_fake_application\", \"actions\": [\"action2\"]}\n",
    "\n",
    "def is_suitable_condition(state: JobApplicationState) -> Literal[\"generate_application\", END]:\n",
    "    if state.get(\"is_suitable\"):\n",
    "        return \"generate_application\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description)\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\"analyze_job_description\", is_suitable_condition)\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"job_description\":\"fake_jd\"}, config={\"configurable\": {\"model_provider\": \"fake\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc677d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"job_description\": job_description}, config={\"configurable\": {\"model_provider\": \"openai-like\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101da01",
   "metadata": {},
   "source": [
    "##### Retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_llm_retry = fake_llm.with_retry(\n",
    "    retry_if_exception_type=(ValueError,),\n",
    "    wait_exponential_jitter=True,\n",
    "    stop_after_attempt=2,\n",
    ")\n",
    "\n",
    "analyze_chain_fake_retries = fake_llm_retry | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.pregel import RetryPolicy\n",
    "\n",
    "def analyze_job_description(state, config: RunnableConfig):\n",
    "    model_provider = config[\"configurable\"].get(\"model_provider\", \"openai-like\")\n",
    "    llm = llms[model_provider]\n",
    "    analyze_chain = llm | parser\n",
    "    prompt = prompt_template_enum.format(job_description=job_description)\n",
    "    result = analyze_chain.invoke(prompt)\n",
    "    return {\"is_suitable\": result}\n",
    "\n",
    "builder = StateGraph(JobApplicationState)\n",
    "builder.add_node(\"analyze_job_description\", analyze_job_description, retry=RetryPolicy(retry_on=ValueError, max_attempts=2))\n",
    "builder.add_node(\"generate_application\", generate_application)\n",
    "builder.add_edge(START, \"analyze_job_description\")\n",
    "builder.add_conditional_edges(\n",
    "    \"analyze_job_description\", is_suitable_condition)\n",
    "builder.add_edge(\"generate_application\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a503721",
   "metadata": {},
   "source": [
    "##### Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dae8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain_fallback = RunnableLambda(lambda _: print(\"running fallback\"))\n",
    "chain = fake_llm | RunnableLambda(lambda _: print(\"running main chain\"))\n",
    "chain_with_fb = chain.with_fallbacks([chain_fallback])\n",
    "\n",
    "chain_with_fb.invoke(\"test\")\n",
    "chain_with_fb.invoke(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e42bf",
   "metadata": {},
   "source": [
    "## Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d64db3",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    \"Given a job description, decide whether it suites a junior Java developer.\"\n",
    "    \"\\nJOB DESCRIPTION:\\n{job_description}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bcbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "lc_prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "chain = lc_prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"job_description\": job_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c844bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "msg_template = HumanMessagePromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([SystemMessage(content=\"You are a helpful assistant.\"), msg_template])\n",
    "chain = chat_prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"job_description\": job_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e008120",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful assistant.\"),\n",
    "     (\"human\", prompt_template)])\n",
    "chain = chat_prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"job_description\": job_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c42d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful assistant.\"),\n",
    "     # same as MessagesPlaceholder(\"history\"),\n",
    "     (\"placeholder\", \"{history}\"),\n",
    "     (\"human\", prompt_template)])\n",
    "\n",
    "chat_prompt_template.invoke(\"fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_history = chat_prompt_template.invoke({\"job_description\": \"fake\", \"history\": [(\"human\", \"hi!\"), (\"ai\", \"hi!\")]}).messages\n",
    "\n",
    "print(with_history)\n",
    "len(with_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12d759",
   "metadata": {},
   "source": [
    "### Zero-shot vs. few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669213e",
   "metadata": {},
   "source": [
    "#### Chaining prompts together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = PromptTemplate.from_template(\"a: {a} b: {b}\")\n",
    "system_template_part = system_template.partial(\n",
    "    a=\"a\" # you also can provide a function here\n",
    ")\n",
    "print(system_template_part.invoke({\"b\": \"b\"}).text)\n",
    "\n",
    "system_template_part.invoke({\"b\": \"b\"}).text == system_template_part.format(b=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template_part1 = PromptTemplate.from_template(\"a: {a}\")\n",
    "system_template_part2 = PromptTemplate.from_template(\"b: {b}\")\n",
    "system_template = system_template_part1 + system_template_part2\n",
    "print(system_template_part.invoke({\"a\": \"a\", \"b\": \"b\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95068b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = PromptTemplate.from_template(\"a: {a} b: {b}\")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_template.template),\n",
    "     (\"human\", \"hi\"),\n",
    "     (\"ai\", \"{c}\")])\n",
    "\n",
    "messages = chat_prompt_template.invoke({\"a\": \"a\", \"b\": \"b\", \"c\": \"c\"}).messages\n",
    "print(len(messages))\n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef639584",
   "metadata": {},
   "source": [
    "#### Dynamic few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529c583",
   "metadata": {},
   "source": [
    "### Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b751a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "math_cot_prompt = hub.pull(\"arietem/math_cot\")\n",
    "cot_chain = math_cot_prompt | llm | StrOutputParser()\n",
    "print(cot_chain.invoke(\"Solve equation 2*x+5=15\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "parse_prompt_template = (\n",
    "    \"Given the initial question and a full answer, \"\n",
    "    \"extract the concise answer. Do not assume anything and \"\n",
    "    \"only use a provided full answer.\\n\\nQUESTION:\\n{question}\\n\"\n",
    "    \"FULL ANSWER:\\n{full_answer}\\n\\nCONCISE ANSWER:\\n\"\n",
    ")\n",
    "parse_prompt = PromptTemplate.from_template(\n",
    "    parse_prompt_template\n",
    ")\n",
    "final_chain = (\n",
    "  {\"full_answer\": itemgetter(\"question\") | cot_chain,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "  }\n",
    "  | parse_prompt\n",
    "  | llm\n",
    "  | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain.invoke({\"question\": \"Solve equation 2*x**2-96*x+1152\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee917975",
   "metadata": {},
   "source": [
    "### Self-consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = []\n",
    "for _ in range(20):\n",
    "  generations.append(final_chain.invoke({\"question\": \"Solve equation 2*x**2-96*x+1152\"}, temperature=2.0).strip())\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(generations).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbb78a",
   "metadata": {},
   "source": [
    "## Working with short context windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612a666",
   "metadata": {},
   "source": [
    "## Understanding memory mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebff86",
   "metadata": {},
   "source": [
    "### Trimming chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989fa27",
   "metadata": {},
   "source": [
    "### Saving history to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.language_models import FakeListChatModel\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.messages import trim_messages, HumanMessage\n",
    "\n",
    "\n",
    "class PrintOutputCallback(BaseCallbackHandler):\n",
    "    def on_chat_model_start(self, serialized, messages, **kwargs):\n",
    "        print(f\"Amount of input messages: {len(messages)}\")\n",
    "\n",
    "\n",
    "sessions = {}\n",
    "handler = PrintOutputCallback()\n",
    "llm = FakeListChatModel(responses=[\"ai1\", \"ai2\", \"ai3\"])\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in sessions:\n",
    "        sessions[session_id] = InMemoryChatMessageHistory()\n",
    "    return sessions[session_id]\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=1,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len,\n",
    "    include_system=True,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "raw_chain = trimmer | llm\n",
    "chain = RunnableWithMessageHistory(raw_chain, get_session_history)\n",
    "\n",
    "config = {\"callbacks\": [PrintOutputCallback()], \"configurable\": {\"session_id\": \"1\"}}\n",
    "_ = chain.invoke(\n",
    "    [HumanMessage(\"Hi!\")],\n",
    "    config=config,\n",
    ")\n",
    "# print(f\"History: {sessions['1'].messages}\")\n",
    "print(f\"History length: {len(sessions['1'].messages)}\")\n",
    "\n",
    "_ = chain.invoke(\n",
    "    [HumanMessage(\"How are you?\")],\n",
    "    config=config,\n",
    ")\n",
    "# print(f\"History: {sessions['1'].messages}\")\n",
    "print(f\"History length: {len(sessions['1'].messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ada7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions[\"1\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba7404",
   "metadata": {},
   "source": [
    "### LangGraph checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31063ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# 1. 定义状态模式，必须包含一个'messages'键\n",
    "class AppState(TypedDict):\n",
    "    # 使用 Annotated 和 add_messages 函数来定义消息列表的合并规则\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def test_node(state):\n",
    "    # ignore the last message since it's an input one\n",
    "    # 注意别用 state.messages 访问，否则报错。\n",
    "    print(f\"History length = {len(state['messages'][:-1])}\")\n",
    "    # return [AIMessage(content=\"Hello!\")]\n",
    "    return {\"messages\": [AIMessage(content=\"Hello!\")]}\n",
    "\n",
    "\n",
    "# builder = MessageGraph()\n",
    "builder = StateGraph(AppState)\n",
    "builder.add_node(\"test_node\", test_node)\n",
    "builder.add_edge(START, \"test_node\")\n",
    "builder.add_edge(\"test_node\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = graph.invoke({\"messages\": [HumanMessage(content=\"test\")]}, config={\"configurable\": {\"thread_id\": \"thread-a\"}})\n",
    "_ = graph.invoke({\"messages\": [HumanMessage(content=\"test\")]}, config={\"configurable\": {\"thread_id\": \"thread-b\"}})\n",
    "_ = graph.invoke({\"messages\": [HumanMessage(content=\"test\")]}, config={\"configurable\": {\"thread_id\": \"thread-a\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d84558",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = list(memory.list(config={\"configurable\": {\"thread_id\": \"thread-a\"}}))\n",
    "\n",
    "for check_point in checkpoints:\n",
    "  print(check_point.config[\"configurable\"][\"checkpoint_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_id = checkpoints[-1].config[\"configurable\"][\"checkpoint_id\"]\n",
    "_ = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"test\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"thread-a\", \"checkpoint_id\": checkpoint_id}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_id = checkpoints[-3].config[\"configurable\"][\"checkpoint_id\"]\n",
    "_ = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"test\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"thread-a\", \"checkpoint_id\": checkpoint_id}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
