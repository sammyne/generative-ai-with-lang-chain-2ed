{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de63e87a",
   "metadata": {},
   "source": [
    "# 07. Software Development and Data Analysis Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee204eaf",
   "metadata": {},
   "source": [
    "# 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain~=0.3 langchain-core~=0.3 langchain-community~=0.3 langchain-openai~=0.3 langgraph~=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-anthropic~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cc43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdaabc7",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cde1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        coder_model = os.getenv(\"OPENAI_CODER_MODEL\")\n",
    "        if not coder_model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.coder_model = coder_model\n",
    "\n",
    "        self.anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        self.anthropic_base_url = os.getenv(\"ANTHROPIC_BASE_URL\")\n",
    "        self.anthropic_model = os.getenv(\"ANTHROPIC_MODEL\")\n",
    "\n",
    "        self.embeddings_model = os.getenv(\"OPENAI_EMBEDDINGS_MODEL\")\n",
    "\n",
    "        hf_pretrained_embeddings_model = os.getenv(\"HF_PRETRAINED_EMBEDDINGS_MODEL\")\n",
    "        self.hf_pretrained_embeddings_model = (\n",
    "            hf_pretrained_embeddings_model\n",
    "            if hf_pretrained_embeddings_model\n",
    "            else \"Qwen/Qwen3-Embedding-8B\"\n",
    "        )\n",
    "\n",
    "    def new_anthropic(self, **kwargs) -> ChatAnthropic:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatAnthropic(\n",
    "            api_key=self.anthropic_api_key,\n",
    "            base_url=self.anthropic_base_url,\n",
    "            model=self.anthropic_model,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_coder(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            model=self.coder_model,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_embeddings(self, **kwargs) -> OpenAIEmbeddings:\n",
    "        if not self.embeddings_model:\n",
    "            raise ValueError(\"OPENAI_EMBEDDINGS_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            model=self.embeddings_model,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.tiktoken_enabled\n",
    "            # 对于非 OpenAI 的官方实现，将这个参数置为 False。\n",
    "            # 回退到用 huggingface transformers 库 AutoTokenizer 来处理 token。\n",
    "            tiktoken_enabled=False,\n",
    "            # https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html#langchain_openai.embeddings.base.OpenAIEmbeddings.model\n",
    "            # 元宝说 Jina 的 embedding 模型 https://huggingface.co/jinaai/jina-embeddings-v4 最接近\n",
    "            # text-embedding-ada-002\n",
    "            # 个人喜好，选了 Qwen/Qwen3-Embedding-8B\n",
    "            # tiktoken_model_name='Qwen/Qwen3-Embedding-8B',\n",
    "            tiktoken_model_name=self.hf_pretrained_embeddings_model,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "def must_get_hfh_api_token() -> str:\n",
    "    \"\"\"\n",
    "    从环境变量或者 .env 环境获取 HuggingFaceHub 的 API 令牌\n",
    "    \"\"\"\n",
    "    # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "    dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "    if not dotenv_path:\n",
    "        raise ValueError(\"No .env file found\")\n",
    "    dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "    out = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    if not out:\n",
    "        raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15e022",
   "metadata": {},
   "source": [
    "## LLMs in software development\n",
    "### The future of development\n",
    "### Implementation considerations\n",
    "### Evolution of code LLMs\n",
    "### Benchmarks for code LLMs\n",
    "### LLM-based software engineering approaches\n",
    "### Security and risk mitigation\n",
    "### Validation framework for LLM-generated code\n",
    "### LangChain integrations\n",
    "## Writing code with LLMs\n",
    "### Google generative AI\n",
    "replaced by Qwen as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Given an integer n, return a string array answer (1-indexed) where:\n",
    "\n",
    "answer[i] == \"FizzBuzz\" if i is divisible by 3 and 5.\n",
    "answer[i] == \"Fizz\" if i is divisible by 3.\n",
    "answer[i] == \"Buzz\" if i is divisible by 5.\n",
    "answer[i] == i (as a string) if none of the above conditions are true.\n",
    "\"\"\"\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# 调用 print 格式化输出\n",
    "print(llm.invoke(question).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7cecd1",
   "metadata": {},
   "source": [
    "### Hugging Face [没跑通]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9415eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text\n",
    "text = \"\"\"\n",
    "def calculate_primes(n):\n",
    "    \\\"\\\"\\\"Create a list of consecutive integers from 2 up to N.\n",
    "\n",
    "    For example:\n",
    "    >>> calculate_primes(20)\n",
    "    Output: [2, 3, 5, 7, 11, 13, 17, 19]\n",
    "    \\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install huggingface-hub~=0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80021b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Choose a lightweight model good for code generation\n",
    "repo_id = \"bigcode/starcoder\"\n",
    "\n",
    "# Initialize the HuggingFaceHub LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 1000},\n",
    "    huggingfacehub_api_token=must_get_hfh_api_token(),\n",
    ")\n",
    "\n",
    "# Use the LangChain LLM to generate text\n",
    "output = llm.invoke(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-huggingface~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Choose a lightweight model good for code generation\n",
    "repo_id = \"bigcode/starcoder\"\n",
    "\n",
    "# Initialize the HuggingFaceHub LLM\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=repo_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\"temperature\": 0.5, \"max_length\": 1000},\n",
    "#     huggingfacehub_api_token=must_get_hfh_api_token(),\n",
    "# )\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    # max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=must_get_hfh_api_token(),\n",
    "    provider=\"huggingface\",  # set your provider here hf.co/settings/inference-providers\n",
    "    # provider=\"hyperbolic\",\n",
    "    # provider=\"nebius\",\n",
    "    # provider=\"together\",\n",
    ")\n",
    "\n",
    "# Use the LangChain LLM to generate text\n",
    "output = llm.invoke(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4caa5",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ae489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Let's think step by step.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = Config().new_anthropic()\n",
    "\n",
    "llm_chain = prompt | llm\n",
    "print(llm_chain.invoke(text).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f73fb",
   "metadata": {},
   "source": [
    "### Agentic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-experimental~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af347e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "responses = [\"Action: Python_REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "tools = [PythonREPLTool()]\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "result = agent(\"What is 2 + 2?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85830ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "result = agent(\"What are the prime numbers until 20?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8c99b",
   "metadata": {},
   "source": [
    "### Documentation RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install beautifulsoup4~=4.14 lxml~=6.0 nest-asyncio~=1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DocusaurusLoader\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "loader = DocusaurusLoader(\"https://python.langchain.com\")\n",
    "# TODO: 解决内存不足导致加载很慢的问题\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd790a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a47b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "\n",
    "store = LocalFileStore(\"./_cache/\")\n",
    "\n",
    "underlying_embeddings = Config().new_openai_like_embeddings()\n",
    "\n",
    "# Avoiding unnecessary costs by caching the embeddings.\n",
    "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-text-splitters~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bed5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5568a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-chroma~=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Config().new_openai_like(model=\"qwen-plus-2025-09-11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318513a8",
   "metadata": {},
   "source": [
    "### Repository RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install GitPython~=3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "\n",
    "\n",
    "# Clone the book repository from GitHub\n",
    "repo_path = os.path.expanduser(\"_generative_ai_with_langchain\")\n",
    "# this directory should not exist yet!\n",
    "repo = Repo.clone_from(\n",
    "    \"https://github.com/benman1/generative_ai_with_langchain\", to_path=repo_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=\"python\", parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "# Split the Document into chunks for embedding and vector storage\n",
    "texts = python_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install transformers~=4.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8642d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 解决执行失败的问题\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# Store the documents in a vector store\n",
    "db = Chroma.from_documents(texts, Config().new_openai_like_embeddings())\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})\n",
    "\n",
    "# Create a retrieval chain for Q&A over code\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "qa = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9752ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question from the book\n",
    "question = \"What examples are in the code related to software development?\"\n",
    "result = qa.invoke({\"input\": question})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1fa52",
   "metadata": {},
   "source": [
    "## Applying LLM agents for data science\n",
    "### Training an ML model\n",
    "#### Setting up a Python-capable agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits.python.base import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like_coder()\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee32ce4",
   "metadata": {},
   "source": [
    "#### Asking the agent to build a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_executor.run(\n",
    "    \"\"\"Understand, write a single neuron neural network in PyTorch.\n",
    "Take synthetic data for y=2x. Train for 1000 epochs and print every 100 epochs.\n",
    "Return prediction for x = 5\"\"\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f16ee",
   "metadata": {},
   "source": [
    "#### Agent execution and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7826d9",
   "metadata": {},
   "source": [
    "### Analyzing a dataset\n",
    "#### Creating a pandas DataFrame agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638af9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install pandas~=2.3 scikit-learn~=1.7 tabulate~=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "df = load_iris(as_frame=True)[\"data\"]\n",
    "\n",
    "df.to_csv(\"iris.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bde905",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eecbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits.pandas.base import (\n",
    "    create_pandas_dataframe_agent,\n",
    ")\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "PROMPT = (\n",
    "    \"If you do not know the answer, say you don't know.\\n\"\n",
    "    \"Think step by step.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Below is the query.\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    ")\n",
    "prompt = PromptTemplate(template=PROMPT, input_variables=[\"query\"])\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "# llm = Config().new_openai_like_coder()\n",
    "\n",
    "agent = create_pandas_dataframe_agent(llm, df, verbose=True, allow_dangerous_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ef31f",
   "metadata": {},
   "source": [
    "#### Asking questions about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(prompt.format(query=\"What's this dataset about?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    prompt.format(\n",
    "        query=\"Which row has the biggest difference between petal length and petal width?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ccda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install matplotlib~=3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748614a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(prompt.format(query=\"Show the distributions for each column visually!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter07",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
