{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a726ef33",
   "metadata": {},
   "source": [
    "# 08. Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d9e56",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain~=0.3 langchain-core~=0.3 langchain-community~=0.3 langchain-openai~=0.3 langgraph~=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c2342",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import langsmith\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "\n",
    "        self.langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_langsmith_client(self, **kwargs) -> langsmith.Client:\n",
    "        if not self.langsmith_api_key:\n",
    "            raise ValueError(\"LANGSMITH_API_KEY is not set\")\n",
    "\n",
    "        return langsmith.Client(api_key=self.langsmith_api_key, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8fee8",
   "metadata": {},
   "source": [
    "## Why evaluation matters\n",
    "### Safety and alignment\n",
    "### Performance and efficiency\n",
    "### User and stakeholder value\n",
    "### Building consensus for LLM evaluation\n",
    "## What we evaluate: core agent capabilities\n",
    "### Task performance evaluation\n",
    "### Tool usage evaluation\n",
    "### RAG evaluation\n",
    "### Planning and reasoning evaluation\n",
    "## How we evaluate: methodologies and approaches\n",
    "### Automated evaluation approaches\n",
    "### Human-in-the-loop evaluation\n",
    "### System-level evaluation\n",
    "## Evaluating LLM agents in practice\n",
    "### Evaluating the correctness of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import ExactMatchStringEvaluator\n",
    "\n",
    "prompt = \"What is the current Federal Reserve interest rate?\"\n",
    "reference_answer = \"0.25%\"  # Suppose this is the correct answer.\n",
    "\n",
    "# Example predictions:\n",
    "prediction_correct = \"0.25%\"\n",
    "prediction_incorrect = \"0.50%\"\n",
    "\n",
    "# Initialize an Exact Match evaluator that ignores case differences.\n",
    "exact_evaluator = ExactMatchStringEvaluator(ignore_case=True)\n",
    "\n",
    "# Evaluate the correct prediction.\n",
    "exact_result_correct = exact_evaluator.evaluate_strings(\n",
    "    prediction=prediction_correct, reference=reference_answer\n",
    ")\n",
    "print(\"Exact match result (correct answer):\", exact_result_correct)\n",
    "\n",
    "# Evaluate an incorrect prediction.\n",
    "exact_result_incorrect = exact_evaluator.evaluate_strings(\n",
    "    prediction=prediction_incorrect, reference=reference_answer\n",
    ")\n",
    "print(\"Exact match result (incorrect answer):\", exact_result_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.scoring import ScoreStringEvalChain\n",
    "\n",
    "# Initialize the evaluator LLM\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Create the ScoreStringEvalChain from the LLM\n",
    "chain = ScoreStringEvalChain.from_llm(llm=llm)\n",
    "\n",
    "# Define the finance-related input, prediction, and reference answer\n",
    "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
    "finance_prediction = \"The current interest rate is 0.25%.\"\n",
    "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
    "\n",
    "# Evaluate the prediction using the scoring chain\n",
    "result_finance = chain.evaluate_strings(\n",
    "    input=finance_input,\n",
    "    prediction=finance_prediction,\n",
    ")\n",
    "\n",
    "print(\"Finance Evaluation Result:\")\n",
    "print(result_finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.scoring import LabeledScoreStringEvalChain\n",
    "\n",
    "# Initialize the evaluator LLM\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Create the evaluation chain that can use reference answers\n",
    "labeled_chain = LabeledScoreStringEvalChain.from_llm(llm=llm)\n",
    "\n",
    "# Define the finance-related input, prediction, and reference answer\n",
    "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
    "finance_prediction = \"The current interest rate is 0.25%.\"\n",
    "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
    "\n",
    "# Evaluate the prediction against the reference\n",
    "labeled_result = labeled_chain.evaluate_strings(\n",
    "    input=finance_input,\n",
    "    prediction=finance_prediction,\n",
    "    reference=finance_reference,\n",
    ")\n",
    "\n",
    "print(\"Finance Evaluation Result (with reference):\")\n",
    "print(labeled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2fdff",
   "metadata": {},
   "source": [
    "### Evaluating tone and conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e133b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm = Config().new_openai_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f44e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_health = \"What is a healthy blood pressure range for adults?\"\n",
    "\n",
    "# A sample LLM output from your healthcare assistant:\n",
    "prediction_health = (\n",
    "    \"A normal blood pressure reading is typically around 120/80 mmHg. \"\n",
    "    \"It's important to follow your doctor's advice for personal health management!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType, load_evaluator\n",
    "\n",
    "conciseness_evaluator = load_evaluator(\n",
    "    EvaluatorType.CRITERIA, criteria=\"conciseness\", llm=evaluation_llm\n",
    ")\n",
    "conciseness_result = conciseness_evaluator.evaluate_strings(\n",
    "    prediction=prediction_health, input=prompt_health\n",
    ")\n",
    "print(\"Conciseness evaluation result:\", conciseness_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate friendliness with custom criterion\n",
    "custom_friendliness = {\n",
    "    \"friendliness\": \"Is the response written in a friendly and approachable tone?\"\n",
    "}\n",
    "friendliness_evaluator = load_evaluator(\n",
    "    EvaluatorType.CRITERIA, criteria=custom_friendliness, llm=evaluation_llm\n",
    ")\n",
    "friendliness_result = friendliness_evaluator.evaluate_strings(\n",
    "    prediction=prediction_health, input=prompt_health\n",
    ")\n",
    "print(\"Friendliness evaluation result:\", friendliness_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e8a77",
   "metadata": {},
   "source": [
    "### Evaluating the output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import JsonValidityEvaluator\n",
    "\n",
    "# Initialize the JSON validity evaluator.\n",
    "json_validator = JsonValidityEvaluator()\n",
    "\n",
    "valid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000}'\n",
    "invalid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000,}'\n",
    "\n",
    "# Evaluate the valid JSON.\n",
    "valid_result = json_validator.evaluate_strings(prediction=valid_json_output)\n",
    "print(\"JSON validity result (valid):\", valid_result)\n",
    "\n",
    "# Evaluate the invalid JSON.\n",
    "invalid_result = json_validator.evaluate_strings(prediction=invalid_json_output)\n",
    "print(\"JSON validity result (invalid):\", invalid_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56682bb9",
   "metadata": {},
   "source": [
    "### Evaluating agent trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec95a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langsmith~=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:\n",
    "    \"\"\"Check how many of the desired steps the agent took.\"\"\"\n",
    "    if len(reference_outputs[\"trajectory\"]) > len(outputs[\"trajectory\"]):\n",
    "        return 0.0\n",
    "\n",
    "    i = j = 0\n",
    "    while i < len(reference_outputs[\"trajectory\"]) and j < len(outputs[\"trajectory\"]):\n",
    "        if reference_outputs[\"trajectory\"][i] == outputs[\"trajectory\"][j]:\n",
    "            i += 1\n",
    "        j += 1\n",
    "\n",
    "    return i / len(reference_outputs[\"trajectory\"])\n",
    "\n",
    "\n",
    "# Create example dataset with expected trajectories\n",
    "client = Config().new_langsmith_client()\n",
    "trajectory_dataset = client.create_dataset(\n",
    "    \"Healthcare Agent Trajectory Evaluation\",\n",
    "    description=\"Evaluates agent trajectory for medication queries\",\n",
    ")\n",
    "\n",
    "# Add example with expected trajectory\n",
    "client.create_example(\n",
    "    inputs={\"question\": \"What is the recommended dosage of ibuprofen for an adult?\"},\n",
    "    outputs={\n",
    "        \"trajectory\": [\n",
    "            \"intent_classifier\",\n",
    "            \"healthcare_agent\",\n",
    "            \"MedicalDatabaseSearch\",\n",
    "            \"format_response\",\n",
    "        ],\n",
    "        \"response\": \"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day.\",\n",
    "    },\n",
    "    dataset_id=trajectory_dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c86437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run graph with trajectory tracking (example implementation)\n",
    "async def run_graph_with_trajectory(inputs: dict) -> dict:\n",
    "    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n",
    "    trajectory = []\n",
    "    final_response = \"\"\n",
    "    # Here you would implement your actual graph execution\n",
    "    # For the example, we'll just return a sample result\n",
    "    trajectory = [\n",
    "        \"intent_classifier\",\n",
    "        \"healthcare_agent\",\n",
    "        \"MedicalDatabaseSearch\",\n",
    "        \"format_response\",\n",
    "    ]\n",
    "    final_response = (\n",
    "        \"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day.\"\n",
    "    )\n",
    "    return {\"trajectory\": trajectory, \"response\": final_response}\n",
    "\n",
    "\n",
    "# Note: This is an async function, so in a notebook you'd need to use await\n",
    "experiment_results = await client.aevaluate(\n",
    "    run_graph_with_trajectory,\n",
    "    data=trajectory_dataset.id,\n",
    "    evaluators=[trajectory_subsequence],\n",
    "    experiment_prefix=\"healthcare-agent-trajectory\",\n",
    "    num_repetitions=1,\n",
    "    max_concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0501d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install pandas~=2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration without async:\n",
    "results_df = experiment_results.to_pandas()\n",
    "print(\n",
    "    f\"Average trajectory match score: {results_df['feedback.trajectory_subsequence'].mean()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde8de6",
   "metadata": {},
   "source": [
    "### Evaluating CoT reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f46cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "# Simulated chain-of-thought reasoning provided by the agent:\n",
    "agent_reasoning = (\n",
    "    \"The current interest rate is 0.25%. I determined this by recalling that recent monetary policies have aimed \"\n",
    "    \"to stimulate economic growth by keeping borrowing costs low. A rate of 0.25% is consistent with the ongoing \"\n",
    "    \"trend of low rates, which encourages consumer spending and business investment.\"\n",
    ")\n",
    "\n",
    "# Expected reasoning reference:\n",
    "expected_reasoning = (\n",
    "    \"An ideal reasoning should mention that the Federal Reserve has maintained a low interest rate—around 0.25%—to \"\n",
    "    \"support economic growth, and it should briefly explain the implications for borrowing costs and consumer spending.\"\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Load the chain-of-thought evaluator.\n",
    "cot_evaluator = load_evaluator(EvaluatorType.COT_QA, llm=llm)\n",
    "\n",
    "result_reasoning = cot_evaluator.evaluate_strings(\n",
    "    input=\"What is the current Federal Reserve interest rate and why does it matter?\",\n",
    "    prediction=agent_reasoning,\n",
    "    reference=expected_reasoning,\n",
    ")\n",
    "\n",
    "print(\"\\nChain-of-Thought Reasoning Evaluation:\")\n",
    "print(result_reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b5bcd",
   "metadata": {},
   "source": [
    "## Offline evaluation\n",
    "### Evaluating RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Config().new_langsmith_client()\n",
    "\n",
    "# Sample financial examples\n",
    "financial_examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"What are the tax implications of early 401(k) withdrawal?\",\n",
    "            \"context_needed\": [\"retirement\", \"taxation\", \"penalties\"],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"Early withdrawals from a 401(k) typically incur a 10% penalty if you're under 59½ years old, in addition to regular income taxes. However, certain hardship withdrawals may qualify for penalty exemptions.\",\n",
    "            \"key_points\": [\"10% penalty\", \"income tax\", \"hardship exemptions\"],\n",
    "            \"documents\": [\"IRS publication 575\", \"Retirement plan guidelines\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"How does dollar-cost averaging compare to lump-sum investing?\",\n",
    "            \"context_needed\": [\n",
    "                \"investment strategy\",\n",
    "                \"risk management\",\n",
    "                \"market timing\",\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"Dollar-cost averaging spreads investments over time to reduce timing risk, while lump-sum investing typically outperforms in rising markets due to longer market exposure. DCA may provide psychological benefits through reduced volatility exposure.\",\n",
    "            \"key_points\": [\"timing risk\", \"market exposure\", \"psychological benefits\"],\n",
    "            \"documents\": [\"Investment strategy comparisons\", \"Market timing research\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create dataset in LangSmith\n",
    "dataset_name = \"Financial Advisory RAG Evaluation\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Evaluation dataset for financial advisory RAG systems covering retirement, investments, and tax planning.\",\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "for example in financial_examples:\n",
    "    client.create_example(\n",
    "        inputs=example[\"inputs\"], outputs=example[\"outputs\"], dataset_id=dataset.id\n",
    "    )\n",
    "print(f\"Created evaluation dataset with {len(financial_examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02732091",
   "metadata": {},
   "source": [
    "### Evaluating a benchmark in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ad999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration for LangSmith:\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"My Project\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = \"lsv2_pt_xxxxxxx\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 没在 langsmith 看到追踪信息\n",
    "\n",
    "# Create a simple LLM call that will be traced in LangSmith\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(f\"Model response: {response.content}\")\n",
    "print(\"\\nThis run has been logged to LangSmith.\")\n",
    "print(\"You can view it in the LangSmith UI: https://smith.langchain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation configuration\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "# Define evaluation criteria specific to RAG systems\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        {\n",
    "            \"criteria\": {\n",
    "                \"factual_accuracy\": \"Does the response contain only factually correct information consistent with the reference answer?\"\n",
    "            },\n",
    "            \"evaluator_type\": \"criteria\",\n",
    "        },\n",
    "        {\n",
    "            \"criteria\": {\n",
    "                \"groundedness\": \"Is the response fully supported by the retrieved documents without introducing unsupported information?\"\n",
    "            },\n",
    "            \"evaluator_type\": \"criteria\",\n",
    "        },\n",
    "        {\n",
    "            \"criteria\": {\n",
    "                \"retrieval_relevance\": \"Are the retrieved documents relevant to answering the question?\"\n",
    "            },\n",
    "            \"evaluator_type\": \"criteria\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55062efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 补充实现\n",
    "def construct_chain():\n",
    "    # This would be your actual RAG implementation\n",
    "    # For example: return RAGChain(...)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ab19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import run_on_dataset\n",
    "\n",
    "results = run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=construct_chain,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc68f2",
   "metadata": {},
   "source": [
    "### Evaluating a benchmark with HF datasets and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install datasets~=3.4 evaluate~=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da1228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "# from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "human_eval = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "code_eval_metric = load(\"code_eval\")\n",
    "\n",
    "test_cases = [\"assert add(2,3)==5\"]\n",
    "candidates = [[\"def add(a,b): return a*b\", \"def add(a, b): return a+b\"]]\n",
    "\n",
    "pass_at_k, results = code_eval_metric.compute(\n",
    "    references=test_cases, predictions=candidates, k=[1, 2]\n",
    ")\n",
    "print(pass_at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb302a9e",
   "metadata": {},
   "source": [
    "### Evaluating email extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of synthetic insurance claim examples\n",
    "example_inputs = [\n",
    "    (\n",
    "        \"I was involved in a car accident on 2023-08-15. My name is Jane Smith, Claim ID INS78910, \"\n",
    "        \"Policy Number POL12345, and the damage is estimated at $3500.\",\n",
    "        {\n",
    "            \"claimant_name\": \"Jane Smith\",\n",
    "            \"claim_id\": \"INS78910\",\n",
    "            \"policy_number\": \"POL12345\",\n",
    "            \"claim_amount\": \"$3500\",\n",
    "            \"accident_date\": \"2023-08-15\",\n",
    "            \"accident_description\": \"Car accident causing damage\",\n",
    "            \"status\": \"pending\",\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"My motorcycle was hit in a minor collision on 2023-07-20. I am John Doe, with Claim ID INS112233 \"\n",
    "        \"and Policy Number POL99887. The estimated damage is $1500.\",\n",
    "        {\n",
    "            \"claimant_name\": \"John Doe\",\n",
    "            \"claim_id\": \"INS112233\",\n",
    "            \"policy_number\": \"POL99887\",\n",
    "            \"claim_amount\": \"$1500\",\n",
    "            \"accident_date\": \"2023-07-20\",\n",
    "            \"accident_description\": \"Minor motorcycle collision\",\n",
    "            \"status\": \"pending\",\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Config().new_langsmith_client()\n",
    "\n",
    "dataset_name = \"Insurance Claims\"\n",
    "\n",
    "# Create the dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Synthetic dataset for insurance claim extraction tasks\",\n",
    ")\n",
    "\n",
    "# Store examples in the dataset\n",
    "for input_text, expected_output in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"input\": input_text},\n",
    "        outputs={\"output\": expected_output},\n",
    "        metadata={\"source\": \"Synthetic\"},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7282ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extraction schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class InsuranceClaim(BaseModel):\n",
    "    claimant_name: str = Field(..., description=\"The name of the claimant\")\n",
    "    claim_id: str = Field(..., description=\"The unique insurance claim identifier\")\n",
    "    policy_number: str = Field(\n",
    "        ..., description=\"The policy number associated with the claim\"\n",
    "    )\n",
    "    claim_amount: str = Field(..., description=\"The claimed amount (e.g., '$5000')\")\n",
    "    accident_date: str = Field(..., description=\"The date of the accident (YYYY-MM-DD)\")\n",
    "    accident_description: str = Field(\n",
    "        ..., description=\"A brief description of the accident\"\n",
    "    )\n",
    "    status: str = Field(\"pending\", description=\"The current status of the claim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extraction chain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=InsuranceClaim)\n",
    "\n",
    "instructions = PromptTemplate(\n",
    "    template=(\n",
    "        \"Extract the following structured information from the insurance claim text: \"\n",
    "        \"claimant_name, claim_id, policy_number, claim_amount, accident_date, \"\n",
    "        \"accident_description, and status.\\n\"\n",
    "        \"{format_instructions}\\n\"\n",
    "        \"{input}\\n\"\n",
    "    ),\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "extraction_chain = instructions | llm | output_parser\n",
    "\n",
    "# Test the extraction chain\n",
    "sample_claim_text = (\n",
    "    \"I was involved in a car accident on 2023-08-15. My name is Jane Smith, \"\n",
    "    \"Claim ID INS78910, Policy Number POL12345, and the damage is estimated at $3500. \"\n",
    "    \"Please process my claim.\"\n",
    ")\n",
    "\n",
    "result = extraction_chain.invoke({\"input\": sample_claim_text})\n",
    "print(\"Extraction Result:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
