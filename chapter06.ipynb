{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77fa30a",
   "metadata": {},
   "source": [
    "# 06. Advanced Applications and Multi-Agent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83c2d9",
   "metadata": {},
   "source": [
    "# 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221f858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m60 packages\u001b[0m \u001b[2min 328ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/59] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m59 packages\u001b[0m \u001b[2min 120ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.10.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgreenlet\u001b[0m\u001b[2m==3.2.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.11.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonpatch\u001b[0m\u001b[2m==1.33\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonpointer\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==1.0.0a1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.4.39\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.10.23\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.44\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzstandard\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=1.0 langchain-core~=1.0 langchain-community==1.0.0a1 langchain-openai~=1.0 langgraph~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4073b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23405207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m26 packages\u001b[0m \u001b[2min 102ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/15] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m15 packages\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.14.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbrotli\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mddgs\u001b[0m\u001b[2m==9.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh2\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhpack\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhyperframe\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprimp\u001b[0m\u001b[2m==0.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msocksio\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msoupsieve\u001b[0m\u001b[2m==2.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install arxiv~=2.2 ddgs~=9.6 wikipedia~=1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b10359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_small(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        model = os.environ[\"OPENAI_MODEL_SMALL\"]\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2760b2d",
   "metadata": {},
   "source": [
    "## Agentic architectures\n",
    "### Agentic RAG\n",
    "RAG becomes agentic RAG when you have shared partial control with the LLM over the execution flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c45c1",
   "metadata": {},
   "source": [
    "## Multi-agent architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5affe26",
   "metadata": {},
   "source": [
    "### Agent roles and specialization\n",
    "\n",
    "1. Specialization improves performance on specific tasks. This allows you to:\n",
    "    - Select the optimal set of tools for each task type.\n",
    "    - Craft tailored prompts and workflows.\n",
    "    - Fine-tune hyper-parameters such as temperature for specific contexts.\n",
    "2. Specialized agents help manage complexity\n",
    "\n",
    "### Consensus mechanism\n",
    "\n",
    "Let multiple agents work on the same tasks in parallel.\n",
    "\n",
    "One important note on parallelization – when you let LangGraph execute nodes in parallel, updates\n",
    "are applied to the main state in the same order as you’ve added nodes to your graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c99dd",
   "metadata": {},
   "source": [
    "### Communication protocols\n",
    "Let agents communicate and work collaboratively on a task.\n",
    "\n",
    "Agents can work collaboratively on a task by providing critique and reflection.\n",
    "\n",
    "Reflection patterns \n",
    "1. **self-reflection**, when the agent analyzes its own steps and identifies areas for improvements\n",
    "1. **cross-reflection**, when you use another agent\n",
    "1. **even reflection**, which includes Human-in-the-Loop (HIL) on critical checkpoints \n",
    "\n",
    "2 practical mechanisms to structure and facilitate agent interactions:\n",
    "1. semantic routing, which directs tasks intelligently based on their content.\n",
    "1. organizing interaction, detailing the specific formats and structures that agents can use to effectively exchange information.\n",
    "\n",
    "#### Semantic router\n",
    "\n",
    "2 ways for multi-class tasks, \n",
    "1. Educate the user by replying with an explanation that they should task your application with a single problem per turn.\n",
    "1. Use tool calling or other controlled generation techniques we’ve learned about to extract both goals and route the \n",
    "execution to two specialized agents with different tasks\n",
    "\n",
    "#### Organizing interactions\n",
    "\n",
    "2 ways to organize communication\n",
    "1. Communicate via specific structures that force them to put their thoughts and reasoning traces in a specific form.\n",
    "1. Apply messages from different agents to the shared list of messages\n",
    "    - Share all messages via a so-called scratchpad – a shared list of messages. As list grows, trimming the chat history is necessary.\n",
    "    - Share only the final results of each execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af53382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m40 packages\u001b[0m \u001b[2min 107ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/13] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 160ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install datasets~=4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc50eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main factor preventing subsistence economies from advancing economically is the lack of'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(\"cais/mmlu\", \"high_school_geography\")\n",
    "\n",
    "ds_dict = ds[\"test\"].take(100).to_dict()\n",
    "\n",
    "ds_dict[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f231e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a currency.',\n",
       " 'a well-connected transportation infrastructure.',\n",
       " 'government activity.',\n",
       " 'a banking service.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict[\"choices\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1399342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_small = Config().new_openai_like_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2b3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import load_tools\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "research_tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a hard-working, curious and creative student. \"\n",
    "    \"You're working on exam question. Think step by step.\"\n",
    "    \"Always provide an argumentation for your answer. \"\n",
    "    \"Do not assume anything, use available tools to search \"\n",
    "    \"for evidence and supporting statements.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dadfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import AgentState, create_agent\n",
    "\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"Answer the following multiple-choice question. \"\n",
    "    \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", raw_prompt_template),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ResearchState(AgentState):\n",
    "    question: str\n",
    "    options: str\n",
    "\n",
    "\n",
    "research_agent = create_agent(\n",
    "    model=llm_small,\n",
    "    tools=research_tools,\n",
    "    state_schema=ResearchState,\n",
    "    system_prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6b276fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# 'Output in JSON format' 的必要性参见\n",
    "# - Qwen 的 https://help.aliyun.com/zh/model-studio/json-mode?spm=0.0.0.i2#6f7bb9cd64o7o\n",
    "# - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format#supported-schemas\n",
    "reflection_prompt = (\n",
    "    \"You are a university professor and you're supervising a student who is \"\n",
    "    \"working on multiple-choice exam question. \"\n",
    "    \"nQUESTION: {question}.\\nANSWER OPTIONS:\\n{options}\\n.\"\n",
    "    \"STUDENT'S ANSWER:\\n{answer}\\n\"\n",
    "    \"Reflect on the answer and provide a feedback whether the answer \"\n",
    "    \"is right or wrong. If you think the final answer is correct, reply with \"\n",
    "    \"the final answer. Only provide critique if you think the answer might \"\n",
    "    \"be incorrect or there are reasoning flaws. Do not assume anything, \"\n",
    "    \"evaluate only the reasoning the student provided and whether there is \"\n",
    "    \"enough evidence for their answer.\\n\"\n",
    "    # \"Output in JSON format, where the correct answer is put in the 'answer' field, \"\n",
    "    # \"and critique is put in the 'critique' field.\"\n",
    "    \"{format_instructions}\"\n",
    ")\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"A final response to the user.\"\"\"\n",
    "\n",
    "    answer: str | None = Field(\n",
    "        description=\"The final answer. It should be empty if critique has been provided.\",\n",
    "        default=None,\n",
    "    )\n",
    "    critique: str | None = Field(\n",
    "        description=\"A critique of the initial answer. If you think it might be incorrect, provide an actionable feedback\",\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Response)\n",
    "reflection_prompt_tmpl = PromptTemplate.from_template(reflection_prompt).partial(\n",
    "    format_instructions=parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "reflection_chain = reflection_prompt_tmpl | llm.with_structured_output(Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60f2e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prompt_template_with_critique = (\n",
    "    \"You tried to answer the exam question and you get feedback from your \"\n",
    "    \"professor. Work on improving your answer and incorporating the feedback. \"\n",
    "    \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n",
    "    \"INITIAL ANSWER:\\n{answer}\\n\\nFEEDBACK:\\n{feedback}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", raw_prompt_template_with_critique),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ReflectionState(ResearchState):\n",
    "    answer: str\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "research_agent_with_critique = create_agent(\n",
    "    model=llm_small,\n",
    "    tools=research_tools,\n",
    "    state_schema=ReflectionState,\n",
    "    system_prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3024bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "class ReflectionAgentState(TypedDict):\n",
    "    question: str\n",
    "    options: str\n",
    "    answer: str\n",
    "    steps: Annotated[int, add]\n",
    "    response: Response\n",
    "\n",
    "\n",
    "def _should_end(\n",
    "    state: ReflectionAgentState, config: RunnableConfig\n",
    ") -> Literal[\"research\", END]:\n",
    "    max_reasoning_steps = config[\"configurable\"].get(\"max_reasoning_steps\", 10)\n",
    "    if state.get(\"response\") and state[\"response\"].answer:\n",
    "        return END\n",
    "    if state.get(\"steps\", 1) > max_reasoning_steps:\n",
    "        return END\n",
    "    return \"research\"\n",
    "\n",
    "\n",
    "def _reflection_step(state):\n",
    "    result = reflection_chain.invoke(state)\n",
    "    return {\"response\": result, \"steps\": 1}\n",
    "\n",
    "\n",
    "def _research_start(state):\n",
    "    answer = research_agent.invoke(state)\n",
    "    return {\"answer\": answer[\"messages\"][-1].content}\n",
    "\n",
    "\n",
    "def _research(state):\n",
    "    agent_state = {\n",
    "        \"answer\": state[\"answer\"],\n",
    "        \"question\": state[\"question\"],\n",
    "        \"options\": state[\"options\"],\n",
    "        \"feedback\": state[\"response\"].critique,\n",
    "    }\n",
    "    answer = research_agent_with_critique.invoke(agent_state)\n",
    "    return {\"answer\": answer[\"messages\"][-1].content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ReflectionAgentState)\n",
    "\n",
    "builder.add_node(\"research_start\", _research_start)\n",
    "builder.add_node(\"research\", _research)\n",
    "builder.add_node(\"reflect\", _reflection_step)\n",
    "\n",
    "builder.add_edge(START, \"research_start\")\n",
    "builder.add_edge(\"research_start\", \"reflect\")\n",
    "builder.add_edge(\"research\", \"reflect\")\n",
    "builder.add_conditional_edges(\"reflect\", _should_end)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a4b589b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de393b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "question = ds_dict[\"question\"][i]\n",
    "options = \"\\n\".join([f\"{i}. {a}\" for i, a in enumerate(ds_dict[\"choices\"][i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95e424bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, event \u001b[38;5;129;01min\u001b[39;00m graph.astream(\n\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question, \u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m: options}, stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m ):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:603\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(executor_or_config, func, *args, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    602\u001b[39m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(\n\u001b[32m    604\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    605\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mCallable[..., T]\u001b[39m\u001b[33m\"\u001b[39m, partial(copy_context().run, wrapper)),\n\u001b[32m    606\u001b[39m     )\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:594\u001b[39m, in \u001b[36mrun_in_executor.<locals>.wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    592\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m() -> T:\n\u001b[32m    593\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    596\u001b[39m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[32m    597\u001b[39m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[32m    598\u001b[39m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[32m    599\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36m_research_start\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_research_start\u001b[39m(state):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     answer = \u001b[43mresearch_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: answer[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1065\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1052\u001b[39m request = ModelRequest(\n\u001b[32m   1053\u001b[39m     model=model,\n\u001b[32m   1054\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1060\u001b[39m     runtime=runtime,\n\u001b[32m   1061\u001b[39m )\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1064\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1068\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1036\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1034\u001b[39m messages = request.messages\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_prompt:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     messages = [\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m, *messages]\n\u001b[32m   1038\u001b[39m output = model_.invoke(messages)\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/system.py:60\u001b[39m, in \u001b[36mSystemMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     56\u001b[39m         content=cast(\u001b[33m\"\u001b[39m\u001b[33mstr | list[str | dict]\u001b[39m\u001b[33m\"\u001b[39m, content_blocks),\n\u001b[32m     57\u001b[39m         **kwargs,\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/base.py:178\u001b[39m, in \u001b[36mBaseMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(content=content_blocks, **kwargs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
      "During task with name 'model' and id '43bdd75d-a2f2-8c9b-fab9-d811e9d96934'",
      "During task with name 'research_start' and id '93dc92dc-d3c9-c214-60d6-00df2dbb429a'"
     ]
    }
   ],
   "source": [
    "async for _, event in graph.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"updates\"]\n",
    "):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee93be1",
   "metadata": {},
   "source": [
    "### LangGraph streaming\n",
    "\n",
    "The `stream` method allows you to stream changes to the graph’s state after each super-step.\n",
    "\n",
    "A super-step is a single iteration over the graph where\n",
    "- Parallel nodes belong to a single super-step\n",
    "- Sequential nodes belong to different super-steps\n",
    "\n",
    "5 stream mode as\n",
    "Mode | Description | Output\n",
    "-----|-------------|----------\n",
    "updates | Streams only updates to the graph produced by the node | A dictionary where each node name maps to its corresponding state update)\n",
    "values | Streams the full state of the graph after each super-step | A dictionary with the entire graph’s state \n",
    "debug | Attempts to stream as much information as possible in the debug mode | A dictionary with a timestamp, task_type, and all the corresponding information for every event\n",
    "custom | Streams events emitted by the node using a StreamWriter | A dictionary that was written from the node to a custom writer\n",
    "messages | Streams full events (for example, ToolMessages) or its chunks in a streaming node if possible (e.g., AI Messages) | A tuple with token or message segment and a dictionary containing metadata from the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d87bd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, event \u001b[38;5;129;01min\u001b[39;00m research_agent.astream(\n\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question, \u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m: options}, stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m ):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(event[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1118\u001b[39m, in \u001b[36mcreate_agent.<locals>.amodel_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1105\u001b[39m request = ModelRequest(\n\u001b[32m   1106\u001b[39m     model=model,\n\u001b[32m   1107\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1113\u001b[39m     runtime=runtime,\n\u001b[32m   1114\u001b[39m )\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m awrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1117\u001b[39m     \u001b[38;5;66;03m# No async handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m _execute_model_async(request)\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;66;03m# Call composed async handler with base handler\u001b[39;00m\n\u001b[32m   1121\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m awrap_model_call_handler(request, _execute_model_async)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1089\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_async\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1087\u001b[39m messages = request.messages\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_prompt:\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     messages = [\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m, *messages]\n\u001b[32m   1091\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m model_.ainvoke(messages)\n\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/system.py:60\u001b[39m, in \u001b[36mSystemMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     56\u001b[39m         content=cast(\u001b[33m\"\u001b[39m\u001b[33mstr | list[str | dict]\u001b[39m\u001b[33m\"\u001b[39m, content_blocks),\n\u001b[32m     57\u001b[39m         **kwargs,\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/base.py:178\u001b[39m, in \u001b[36mBaseMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(content=content_blocks, **kwargs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
      "During task with name 'model' and id 'bb047e68-4c5d-51be-edd7-81f62d886c5e'"
     ]
    }
   ],
   "source": [
    "async for _, event in research_agent.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"values\"]\n",
    "):\n",
    "    print(len(event[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75891d67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, event \u001b[38;5;129;01min\u001b[39;00m research_agent.astream(\n\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question, \u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m: options}, stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m ):\n\u001b[32m      4\u001b[39m     node = \u001b[38;5;28mlist\u001b[39m(event.keys())[\u001b[32m0\u001b[39m]\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(node, \u001b[38;5;28mlen\u001b[39m(event[node].get(\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, [])))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1118\u001b[39m, in \u001b[36mcreate_agent.<locals>.amodel_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1105\u001b[39m request = ModelRequest(\n\u001b[32m   1106\u001b[39m     model=model,\n\u001b[32m   1107\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1113\u001b[39m     runtime=runtime,\n\u001b[32m   1114\u001b[39m )\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m awrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1117\u001b[39m     \u001b[38;5;66;03m# No async handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m _execute_model_async(request)\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;66;03m# Call composed async handler with base handler\u001b[39;00m\n\u001b[32m   1121\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m awrap_model_call_handler(request, _execute_model_async)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1089\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_async\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1087\u001b[39m messages = request.messages\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_prompt:\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     messages = [\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m, *messages]\n\u001b[32m   1091\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m model_.ainvoke(messages)\n\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/system.py:60\u001b[39m, in \u001b[36mSystemMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     56\u001b[39m         content=cast(\u001b[33m\"\u001b[39m\u001b[33mstr | list[str | dict]\u001b[39m\u001b[33m\"\u001b[39m, content_blocks),\n\u001b[32m     57\u001b[39m         **kwargs,\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/base.py:178\u001b[39m, in \u001b[36mBaseMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(content=content_blocks, **kwargs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
      "During task with name 'model' and id 'a8d62f07-0c76-1df9-1022-21bbbed3534c'"
     ]
    }
   ],
   "source": [
    "async for _, event in research_agent.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"updates\"]\n",
    "):\n",
    "    node = list(event.keys())[0]\n",
    "    print(node, len(event[node].get(\"messages\", [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d19719c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m seen_events = \u001b[38;5;28mset\u001b[39m([])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m research_agent.astream_events(\n\u001b[32m      3\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question, \u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m: options}, version=\u001b[33m\"\u001b[39m\u001b[33mv1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m ):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event[\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen_events:\n\u001b[32m      6\u001b[39m         seen_events.add(event[\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1487\u001b[39m, in \u001b[36mRunnable.astream_events\u001b[39m\u001b[34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1484\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[32m   1488\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py:849\u001b[39m, in \u001b[36m_astream_events_implementation_v1\u001b[39m\u001b[34m(runnable, value, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m    846\u001b[39m root_metadata = config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    847\u001b[39m root_name = config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m, runnable.get_name())\n\u001b[32m--> \u001b[39m\u001b[32m849\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m _astream_log_implementation(\n\u001b[32m    850\u001b[39m     runnable,\n\u001b[32m    851\u001b[39m     value,\n\u001b[32m    852\u001b[39m     config=config,\n\u001b[32m    853\u001b[39m     stream=stream,\n\u001b[32m    854\u001b[39m     diff=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    855\u001b[39m     with_streamed_output_list=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    856\u001b[39m     **kwargs,\n\u001b[32m    857\u001b[39m ):\n\u001b[32m    858\u001b[39m     run_log += log\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encountered_start_event:\n\u001b[32m    861\u001b[39m         \u001b[38;5;66;03m# Yield the start event for the root runnable.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:747\u001b[39m, in \u001b[36m_astream_log_implementation\u001b[39m\u001b[34m(runnable, value, config, stream, diff, with_streamed_output_list, **kwargs)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib.suppress(asyncio.CancelledError):\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:699\u001b[39m, in \u001b[36m_astream_log_implementation.<locals>.consume_astream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    696\u001b[39m prev_final_output: Output | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    697\u001b[39m final_output: Output | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m runnable.astream(value, config, **kwargs):\n\u001b[32m    700\u001b[39m     prev_final_output = final_output\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:132\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m task.proc.astream(task.input, config):\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:839\u001b[39m, in \u001b[36mRunnableSeq.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m             aiterator = h.tap_output_aiter(\n\u001b[32m    836\u001b[39m                 run_manager.run_id, aiterator\n\u001b[32m    837\u001b[39m             )\n\u001b[32m    838\u001b[39m \u001b[38;5;66;03m# consume into final output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    840\u001b[39m     _consume_aiter(aiterator), context=context\n\u001b[32m    841\u001b[39m )\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# sequence doesn't emit output, yield to mark as generator\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:904\u001b[39m, in \u001b[36m_consume_aiter\u001b[39m\u001b[34m(it)\u001b[39m\n\u001b[32m    902\u001b[39m output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    903\u001b[39m add_supported = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# collect final output\u001b[39;00m\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m add_supported:\n\u001b[32m    907\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:315\u001b[39m, in \u001b[36mLogStreamCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtap_output_aiter\u001b[39m(\n\u001b[32m    304\u001b[39m     \u001b[38;5;28mself\u001b[39m, run_id: UUID, output: AsyncIterator[T]\n\u001b[32m    305\u001b[39m ) -> AsyncIterator[T]:\n\u001b[32m    306\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Tap an output async iterator to stream its values to the log.\u001b[39;00m\n\u001b[32m    307\u001b[39m \n\u001b[32m    308\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m \u001b[33;03m        The output value.\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[32m    316\u001b[39m         \u001b[38;5;66;03m# root run is handled in .astream_log()\u001b[39;00m\n\u001b[32m    317\u001b[39m         \u001b[38;5;66;03m# if we can't find the run silently ignore\u001b[39;00m\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# eg. because this run wasn't included in the log\u001b[39;00m\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    320\u001b[39m             run_id != \u001b[38;5;28mself\u001b[39m.root_id\n\u001b[32m    321\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (key := \u001b[38;5;28mself\u001b[39m._key_map_by_run_id.get(run_id))\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m             )\n\u001b[32m    331\u001b[39m         ):\n\u001b[32m    332\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1560\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m final: Input\n\u001b[32m   1558\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1561\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1562\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1565\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1566\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1568\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1569\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1560\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m final: Input\n\u001b[32m   1558\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1561\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1562\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1565\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1566\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1568\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1569\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1147\u001b[39m, in \u001b[36mRunnable.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mastream\u001b[39m(\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   1131\u001b[39m     config: RunnableConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1132\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1133\u001b[39m ) -> AsyncIterator[Output]:\n\u001b[32m   1134\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Default implementation of `astream`, which calls `ainvoke`.\u001b[39;00m\n\u001b[32m   1135\u001b[39m \n\u001b[32m   1136\u001b[39m \u001b[33;03m    Subclasses must override this method if they support streaming output.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m \n\u001b[32m   1146\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1118\u001b[39m, in \u001b[36mcreate_agent.<locals>.amodel_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1105\u001b[39m request = ModelRequest(\n\u001b[32m   1106\u001b[39m     model=model,\n\u001b[32m   1107\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1113\u001b[39m     runtime=runtime,\n\u001b[32m   1114\u001b[39m )\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m awrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1117\u001b[39m     \u001b[38;5;66;03m# No async handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m _execute_model_async(request)\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;66;03m# Call composed async handler with base handler\u001b[39;00m\n\u001b[32m   1121\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m awrap_model_call_handler(request, _execute_model_async)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1089\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_async\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1087\u001b[39m messages = request.messages\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_prompt:\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     messages = [\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m, *messages]\n\u001b[32m   1091\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m model_.ainvoke(messages)\n\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/system.py:60\u001b[39m, in \u001b[36mSystemMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     56\u001b[39m         content=cast(\u001b[33m\"\u001b[39m\u001b[33mstr | list[str | dict]\u001b[39m\u001b[33m\"\u001b[39m, content_blocks),\n\u001b[32m     57\u001b[39m         **kwargs,\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/messages/base.py:178\u001b[39m, in \u001b[36mBaseMessage.__init__\u001b[39m\u001b[34m(self, content, content_blocks, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(content=content_blocks, **kwargs)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 21 validation errors for SystemMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=ChatPromptTemplate(input_...sages', optional=True)]), input_type=ChatPromptTemplate]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_variables', ['options', 'question']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('optional_variables', ['messages']), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('input_types', {'message...error_context=None))]]}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('output_parser', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('partial_variables', {'messages': []}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('metadata', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tags', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('messages', [SystemMessa...sages', optional=True)]), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('validate_template', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.12/v/dict_type",
      "During task with name 'model' and id '68b29ee3-4c8e-b2c2-ae3a-0124aea35bda'"
     ]
    }
   ],
   "source": [
    "seen_events = set([])\n",
    "async for event in research_agent.astream_events(\n",
    "    {\"question\": question, \"options\": options}, version=\"v1\"\n",
    "):\n",
    "    if event[\"event\"] not in seen_events:\n",
    "        seen_events.add(event[\"event\"])\n",
    "\n",
    "print(seen_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc00604",
   "metadata": {},
   "source": [
    "### Handoffs\n",
    "\n",
    "Handoffs: update your graph’s state and at the same time invoke another agent by passing a custom state to it using the\n",
    "`Command` API with 2 fields\n",
    "- Pass an `update` – a dictionary with an update of the current state to be sent to your graph\n",
    "- `goto` – a name (or list of names) of the nodes to hand off control. A destination agent can be a node from the current or a parent (`Command.PARENT`) graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff20e073",
   "metadata": {},
   "source": [
    "#### Communication via a shared messages list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d997345",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Config().new_openai_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "612e8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import load_tools\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "research_tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a hard-working, curious and creative student. \"\n",
    "    \"You're working on exam question. Think step by step.\"\n",
    "    \"Always provide an argumentation for your answer. \"\n",
    "    \"Do not assume anything, use available tools to search \"\n",
    "    \"for evidence and supporting statements.\"\n",
    ")\n",
    "\n",
    "research_agent = create_agent(\n",
    "    model=llm, tools=research_tools, system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2799f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = (\n",
    "    \"You are a university professor and you're supervising a student who is \"\n",
    "    \"working on multiple-choice exam question. \"\n",
    "    \"Given the dialogue above, reflect on the answer provided and give a feedback \"\n",
    "    \" if needed. If you think the final answer is correct, reply with \"\n",
    "    \"an empty message. Only provide critique if you think the last answer might \"\n",
    "    \"be incorrect or there are reasoning flaws. Do not assume anything, \"\n",
    "    \"evaluate only the reasoning the student provided and whether there is \"\n",
    "    \"enough evidence for their answer.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaddc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.types import Command\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "question_template = PromptTemplate.from_template(\n",
    "    \"QUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def _ask_question(state):\n",
    "    return {\"messages\": [(\"human\", question_template.invoke(state).text)]}\n",
    "\n",
    "\n",
    "def _give_feedback(state, config: RunnableConfig):\n",
    "    # print(state.keys())\n",
    "    messages = state[\"messages\"] + [(\"human\", reflection_prompt)]\n",
    "    max_messages = config[\"configurable\"].get(\"max_messages\", 20)\n",
    "\n",
    "    if len(messages) > max_messages:\n",
    "        return Command(update={}, goto=END)\n",
    "\n",
    "    result = llm.invoke(messages)\n",
    "\n",
    "    if result.content:\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [\n",
    "                    (\"assistant\", result.content),\n",
    "                    (\"human\", \"Please, address the feedback above and give an answer.\"),\n",
    "                ]\n",
    "            },\n",
    "            goto=\"research\",\n",
    "        )\n",
    "    return Command(update={}, goto=END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c91faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "\n",
    "\n",
    "class ReflectionAgentState(MessagesState):\n",
    "    question: str\n",
    "    options: str\n",
    "\n",
    "\n",
    "builder = StateGraph(ReflectionAgentState)\n",
    "\n",
    "builder.add_node(\"ask_question\", _ask_question)\n",
    "builder.add_node(\"research\", research_agent)\n",
    "builder.add_node(\"reflect\", _give_feedback)\n",
    "\n",
    "builder.add_edge(START, \"ask_question\")\n",
    "builder.add_edge(\"ask_question\", \"research\")\n",
    "builder.add_edge(\"research\", \"reflect\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5445e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c82a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "8\n",
      "10\n",
      "11\n",
      "13\n",
      "14\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "async for _, event in graph.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"values\"]\n",
    "):\n",
    "    print(len(event[\"messages\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db072431",
   "metadata": {},
   "source": [
    "## LangGraph platform\n",
    "## Building adaptive systems\n",
    "### Dynamic behavior adjustment\n",
    "### Human-in-the-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "416575f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__interrupt__': (Interrupt(value='What is your address?', id='7116c6a05f5c258f3cd630aa05c146fd'),)}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    home_address: str | None\n",
    "\n",
    "\n",
    "def _human_input(state: State):\n",
    "    address = interrupt(\"What is your address?\")\n",
    "    return {\"home_address\": address}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"human_input\", _human_input)\n",
    "builder.add_edge(START, \"human_input\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for chunk in graph.stream({\"messages\": [(\"human\", \"What is weather today?\")]}, config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ed40368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human_input': {'home_address': 'Munich'}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "for chunk in graph.stream(Command(resume=\"Munich\"), config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa69e1a",
   "metadata": {},
   "source": [
    "## Exploring reasoning paths\n",
    "### Tree of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dc43d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "system_prompt_template = (\n",
    "    \"For the given task, come up with a step by step plan.\\n\"\n",
    "    \"This plan should involve individual tasks, that if executed correctly will \"\n",
    "    \"yield the correct answer. Do not add any superfluous steps.\\n\"\n",
    "    \"The result of the final step should be the final answer. Make sure that each \"\n",
    "    \"step has all the information needed - do not skip steps.\\n\"\n",
    "    \"Output in JSON format described as follows.\\n\"\n",
    "    \"{formatting_instructions}\"\n",
    ")\n",
    "# 注意 system 消息中的插值变量 `formatting_instructions` 需要和 partial 函数的入参完全匹配。\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\"),\n",
    "    ]\n",
    ").partial(formatting_instructions=parser.get_format_instructions())\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "planner = planner_prompt | llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45f9dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import load_tools\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a smart assistant that carefully helps to solve complex tasks.\\n\"\n",
    "    \" Given a general plan to solve a task and a specific step, work on this step. \"\n",
    "    \" Don't assume anything, keep in minds things might change and always try to \"\n",
    "    \"use tools to double-check yourself.\\nUse Search to gather \"\n",
    "    \"information about common facts, fresh events and news, use Arxiv to get \"\n",
    "    \"ideas on recent research and use Wikipedia for common knowledge.\"\n",
    ")\n",
    "\n",
    "step_template = (\n",
    "    \"Given the task and the plan, try to execute on a specific step of the plan.\\n\"\n",
    "    \"TASK:\\n{task}\\n\\nPLAN:\\n{previous_steps}\\n\\nSTEP TO EXECUTE:\\n{step}\\n\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", step_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "execution_agent = prompt_template | create_agent(model=llm, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82d4e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class ReplanStep(BaseModel):\n",
    "    \"\"\"Replanned next step in the plan.\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(description=\"different options of the proposed next step\")\n",
    "\n",
    "\n",
    "llm_replanner = llm.with_structured_output(ReplanStep)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "replanner_prompt_template = (\n",
    "    \"Suggest next action in the plan. Do not add any superfluous steps.\\n\"\n",
    "    \"If you think no actions are needed, just return an empty list of steps. \"\n",
    "    \"TASK: {task}\\n PREVIOUS STEPS WITH OUTPUTS: {current_plan}\\n\"\n",
    "    \"Output in JSON format described as follows.\\n\"\n",
    "    \"{formatting_instructions}\"\n",
    ")\n",
    "# 参考 https://zhuanlan.zhihu.com/p/1901678624639255242\n",
    "replanner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a helpful assistant. You goal is to help with planning actions to solve the task. Do not solve the task itself.\",\n",
    "        ),\n",
    "        (\"user\", replanner_prompt_template),\n",
    "    ]\n",
    ").partial(formatting_instructions=parser.get_format_instructions())\n",
    "\n",
    "replanner = replanner_prompt | llm_replanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e677a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_id: int,\n",
    "        step: str,\n",
    "        step_output: str | None = None,\n",
    "        parent: Optional[\"TreeNode\"] = None,\n",
    "    ):\n",
    "        self.node_id = node_id\n",
    "        self.step = step\n",
    "        self.step_output = step_output\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.final_response = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        parent_id = self.parent.node_id if self.parent else \"None\"\n",
    "        return f\"Node_id: {self.node_id}, parent: {parent_id}, {len(self.children)} children.\"\n",
    "\n",
    "    def get_full_plan(self) -> str:\n",
    "        \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n",
    "        steps = []\n",
    "        node = self\n",
    "        while node.parent:\n",
    "            steps.append((node.step, node.step_output))\n",
    "            node = node.parent\n",
    "\n",
    "        full_plan = []\n",
    "        for i, (step, result) in enumerate(steps[::-1]):\n",
    "            if result:\n",
    "                full_plan.append(f\"# {i+1}. Planned step: {step}\\nResult: {result}\\n\")\n",
    "        return \"\\n\".join(full_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38c0f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import deque\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class PlanState(TypedDict):\n",
    "    task: str\n",
    "    root: TreeNode\n",
    "    queue: deque[TreeNode]\n",
    "    current_node: TreeNode\n",
    "    next_node: TreeNode\n",
    "    is_current_node_final: bool\n",
    "    paths_explored: Annotated[int, operator.add]\n",
    "    visited_ids: set[int]\n",
    "    max_id: int\n",
    "    candidates: Annotated[list[str], operator.add]\n",
    "    best_candidate: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "549609cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.types import Command\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "responder = final_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# 这里用的应该是广度有限算法，而不是书中说的深度优先算法。\n",
    "async def _run_node(state: PlanState, config: RunnableConfig):\n",
    "    node = state.get(\"next_node\")\n",
    "    visited_ids = state.get(\"visited_ids\", set())\n",
    "    queue = state[\"queue\"]\n",
    "    if node is None:\n",
    "        while queue and not node:\n",
    "            node = state[\"queue\"].popleft()\n",
    "            if node.node_id in visited_ids:\n",
    "                node = None\n",
    "        if not node:\n",
    "            return Command(goto=\"vote\", update={})\n",
    "\n",
    "    step = await execution_agent.ainvoke(\n",
    "        {\n",
    "            \"previous_steps\": node.get_full_plan(),\n",
    "            \"step\": node.step,\n",
    "            \"task\": state[\"task\"],\n",
    "        }\n",
    "    )\n",
    "    node.step_output = step[\"messages\"][-1].content\n",
    "    visited_ids.add(node.node_id)\n",
    "    return {\n",
    "        \"current_node\": node,\n",
    "        \"queue\": queue,\n",
    "        \"visited_ids\": visited_ids,\n",
    "        \"next_node\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "async def _plan_next(state: PlanState, config: RunnableConfig) -> PlanState:\n",
    "    max_candidates = config[\"configurable\"].get(\"max_candidates\", 1)\n",
    "    node = state[\"current_node\"]\n",
    "    next_step = await replanner.ainvoke(\n",
    "        {\"task\": state[\"task\"], \"current_plan\": node.get_full_plan()}\n",
    "    )\n",
    "    if not next_step.steps:\n",
    "        return {\"is_current_node_final\": True}\n",
    "    max_id = state[\"max_id\"]\n",
    "    for step in next_step.steps[:max_candidates]:\n",
    "        child = TreeNode(node_id=max_id + 1, step=step, parent=node)\n",
    "        max_id += 1\n",
    "        node.children.append(child)\n",
    "        state[\"queue\"].append(child)\n",
    "    return {\"is_current_node_final\": False, \"next_node\": child, \"max_id\": max_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcf667d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_voting = PromptTemplate.from_template(\n",
    "    \"Pick the best solution for a given task. \"\n",
    "    \"\\nTASK:{task}\\n\\nSOLUTIONS:\\n{candidates}\\n\"\n",
    "    \"Output 1-based index of the best solution.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def _vote_for_the_best_option(state):\n",
    "    candidates = state.get(\"candidates\", [])\n",
    "    if not candidates:\n",
    "        return {\"best_response\": None}\n",
    "    all_candidates = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        all_candidates.append(f\"OPTION {i+1}: {candidate}\")\n",
    "\n",
    "    llm_enum = Config().new_openai_like()\n",
    "\n",
    "    result = (prompt_voting | llm_enum | StrOutputParser()).invoke(\n",
    "        {\"candidates\": \"\\n\".join(all_candidates), \"task\": state[\"task\"]}\n",
    "    )\n",
    "    return {\"best_candidate\": candidates[int(result) - 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10ae9f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_candidate': '4'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_vote_for_the_best_option({\"candidates\": [\"1\", \"5\", \"4\"], \"task\": \"How much is 2+2?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb709f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "responder = final_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "async def _build_initial_plan(state: PlanState) -> PlanState:\n",
    "    plan = await planner.ainvoke(state[\"task\"])\n",
    "    queue = deque()\n",
    "    root = TreeNode(step=plan.steps[0], node_id=1)\n",
    "    queue.append(root)\n",
    "    current_root = root\n",
    "    for i, step in enumerate(plan.steps[1:]):\n",
    "        child = TreeNode(node_id=i + 2, step=step, parent=current_root)\n",
    "        current_root.children.append(child)\n",
    "        queue.append(child)\n",
    "        current_root = child\n",
    "    return {\"root\": root, \"queue\": queue, \"max_id\": i + 2}\n",
    "\n",
    "\n",
    "async def _get_final_response(state: PlanState) -> PlanState:\n",
    "    node = state[\"current_node\"]\n",
    "    final_response = await responder.ainvoke(\n",
    "        {\"task\": state[\"task\"], \"plan\": node.get_full_plan()}\n",
    "    )\n",
    "    node.final_response = final_response\n",
    "    return {\"paths_explored\": 1, \"candidates\": [final_response]}\n",
    "\n",
    "\n",
    "def _should_create_final_response(\n",
    "    state: PlanState,\n",
    ") -> Literal[\"run\", \"generate_response\"]:\n",
    "    return \"generate_response\" if state[\"is_current_node_final\"] else \"run\"\n",
    "\n",
    "\n",
    "def _should_continue(\n",
    "    state: PlanState, config: RunnableConfig\n",
    ") -> Literal[\"run\", \"vote\"]:\n",
    "    max_paths = config[\"configurable\"].get(\"max_paths\", 30)\n",
    "    if state.get(\"paths_explored\", 1) >= max_paths:\n",
    "        return \"vote\"\n",
    "    if state[\"queue\"] or state.get(\"next_node\"):\n",
    "        return \"run\"\n",
    "    return \"vote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b6afe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "builder = StateGraph(PlanState)\n",
    "\n",
    "builder.add_node(\"initial_plan\", _build_initial_plan)\n",
    "builder.add_node(\"run\", _run_node)\n",
    "builder.add_node(\"plan_next\", _plan_next)\n",
    "builder.add_node(\"generate_response\", _get_final_response)\n",
    "builder.add_node(\"vote\", _vote_for_the_best_option)\n",
    "\n",
    "builder.add_edge(START, \"initial_plan\")\n",
    "builder.add_edge(\"initial_plan\", \"run\")\n",
    "builder.add_edge(\"run\", \"plan_next\")\n",
    "builder.add_conditional_edges(\"plan_next\", _should_create_final_response)\n",
    "builder.add_conditional_edges(\"generate_response\", _should_continue)\n",
    "builder.add_edge(\"vote\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "155e41fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAAJ2CAIAAACGshoRAAAQAElEQVR4nOydB3wT5RvH30vSdE8opbvsvctGZkFkLxUE2bJEkKGiyJS/bERBQEBBluy996zI3iACLYVO6N4Zd/8nuTajTWlGk94lz5d+wuW990be+937Pu98RAzDEAQxFhFBEBNAASEmgQJCTAIFhJgECggxCRQQYhI8E9Cz25nP7qenJ8kkObQst2ADBCUgDPyjKa0QWrkhZBi5KhwOpLTiCwihVcfANkMoihIwjOpUlPKgog6HbYqw7SGUkDByrbsSOwnE9gInN1GFmi7VmzgT64LiRTvQ1aNJT66nZaXL4cHZiQUiMWVnR9HywgKiGHiaGuGKEFrxVeu55qtBGZ8hcnU0di/IQnEiTVVpXUZ1uEJmebeRr6DCArJzEMokNChemgsXYZxcRZXrurTqVYZYBVwXUPjBxPt/p8IDKx/i0KxzWZ8QMeEzSbGyvw+/jY7IgnyxWiPXth96E57DaQFtmB0pzaXrtfFs2tmTWBd3zqVdP/1WKKSGz61A+AxHBZSZJN/4v8jAas49RpUn1suxjfERDzO6DPMLqelI+AkXBSSXkNXTnvUZF+RXmd8Flj6kJzOb5j0fPruCo6uQ8BDOCSgxVrJ96avPl1QitsTqr593+sSvUn3+5UMCwjF2LH018KsQYmOMXVTp+OZowkO4JaDfZ0ZUrO3i4cPLzNxEajX3WDf9BeEbHBLQ2R0JcinTeagPsUna9isrFAmObYgjvIJDAnpyPb1pl7LEhgkbUP7FgwzCK7gioAt73orEgnrvuREbJqi6g509zzIhrgjo2e2MgMpOxLJ07NgxOtpg0/X58+fdunUj5qFqA9eop1mEP3BFQDnZsrD+5YgFiY2NTU5OJobz6NEjYjagc0MqobMzCV/ghICunUwWCgViJ4qYAWjo2rZt2yeffNKyZctBgwatXLlSLpffuHGje/fusLdnz55Tpkwhynxl4cKF/fr1a9GiBUTbvXs3e/izZ89CQ0MvX77cuXPnAQMGrFmzZs6cOXFxcRC4detWYgbsHYTXjr0lPIETwzliX+Q4OJur6r59+/Y//vjjyy+/BAGdP3/+119/dXZ2HjZs2PLlyyHwwIED/v7+EG3p0qUxMTHTp0+HDvbIyEgQk6+vLxxiZ2cHe9evX//pp5/Wr1+/Vq1aEonk5MmThw8fJubByVUYF5VDeAInBJSZKnV0MZeAbt26VbNmTdZq6d27d+PGjbOydBgZ8+fPz8zM9PPzg23IXQ4ePBgeHg4CAj1BSLNmzQYOHEgsgouHXcpbCeEJnBCQVMI4upil/ALq1au3YsWKuXPnNmjQoHXr1gEBATqjQUkHedWVK1devnzJhrA5E0uNGjWIpbCzpwqPleMsnBAQTdPsGD9zANYPlFkXLlwA20UkEkHNa8KECd7e3gVuYOLEiVA2jR8/HrIfV1fXESNGaEawt7cnFoMitO6RbFyEEwIS2wshEyLmQSAQ9Fby4sWLa9eurV27NiMj46efftKM8+TJk4cPH65atapJkyZsSHp6erlyFq0VqpDlMHZCzvVRFgUnBOToLExLlhLzANYuFECVKlWqqASUsW/fvgJxUlJS4FOlmBdK4BBSGmSkyh2ceTNWnRNK9wlyzM6UE/Nw/Pjxr7766uLFi6mpqVAbP3v2LFhFEB4SEgKfp06devDgAQgLSrfNmzenpaVBFWzx4sVgNUNDkc4TBgUFvX37Fip0KmupZElPknh62xGewAkBteziJTdbEfb999+DPiZPntyhQ4cffvihTZs2UFeHcLCmoSkI2nXAxC5fvvy8efPu37/fvn37SZMmff7559AgBMKCz8InbNWqFdTnp06deuLECWIGpFK6UXveDOHlyoCy1V8/r9LANWxA6Zgd3OHq0aRbZ5PH8Wc8HVfK2pAazi8f86cB32w8/CfNJ9iB8AeuCOiDYeVXTn4W/V+OfxXdyQemydChQ3Xugra+ovLRXr16QXMzMQ9w5jt37ujc5e7uDiaXzl1Q9hXVF5udTmelSUfMCSH8gUNjovf/GpMQnTPqx4o698pksoSEBJ27wPJ1c9M9DsTJycnDw4OYBzCloelI567s7GxHR90DnEFb0C6lc9cfsyOdnYQffx1I+AO3BtWv+up5g7Zezbta2ywwfXh0NeP87ngeWT8s3Gqw6j855NbZRGKTXNgT33kI/ybBcUtAXr7CJp3Krv2Of2PLTWT99xHVG7tVrMO/pRe4OLEw+nnO/tXRtjM1DAruzoPL81E9hLNTm2+fTws/lNDsg7KNwsxlAnOBR+EZF/YlQN7T7iO+zibg7uIKb1/Ld6946egi7DUm0N2bN52LeiLJJruWR6WnyDp9Wr5ibUsPBi9BuL68y94V0XEvc5zcRDWauFnHGh23ziQ//Ds9LUXqE+jQb6I/4Tn8WGDq4G8xsRHZMhlxcBQ4OAtd3EVCu4J3zi5Gplj0STG+Rx3CIqCU42xowlDwm/MDBYoQSpG7qReYyjuPIr6AoekChxPVolVaa5axRyqiKU6jHU6JiCybys6QZaYrFlYTCinoPO41zpdYBfwQEEv8S8mD8NSE1zk5WXJJtpymC4xBY5SN0sotRh3CbqnDKcX6Y6pAhlF+5g9oo+VyoUjIBipWumMEBQ9XrmDGFDH8TbXQnSZCEWMnEjq4CLx8Heo0d/OrzKeeimLhk4AsQNOmTcPDw4VCW5ycbxy4SqsaeJdomkb1GAQKSA10t4lEmCCGgemlBgVkBJheaqRSKTuNENEfFJAazIGMANNLDQrICDC91KCAjADTSw3aQEaAAlKDOZARYHqpQQEZAaaXGhSQEWB6qUEBGQGmlxoUkBFgeqlBARkBppcaFJARYHqpQQEZAaaXGmxINAIUkBrMgYwA00sNCsgIML3UoICMANNLDQrICDC91KARbQQoIDWYAxkBppcayH6cnHg8Tb1UQAGpoWk6I4NnHidLHRSQGii/oBQjiCGggNSggIwABaQGBWQEKCA1KCAjQAGpQQEZAQpIDQrICFBAalBARoACUoMCMgIUkBoUkBGggNQIhUIUkKGggNRgDmQEKCA1KCAjQAGpQQEZAQpIDQrICFBAalBARoACUoMCMgJcqZ5Mnjz53LlzrJMESA2BQOHewM7O7urVqwQpDmtzo2QE48ePDwgIECiBpiDWB3RgIJ8835YiKCBSsWLFVq1aaebEDg4O/fv3J4geoIAUDB482N9f7brL19e3Z8+eBNEDFJACUEzbtm3ZbTClQT04v0dPUEB5QCbE2j1+fn59+/YliH6UWi1MLiFXjiRmpUmkEvUNaPoYZL8Caq+DAoqmtSIrHAcyeUcV2Kv2KMg6EiwcIT8O6yUO6l4vIiJevowKCgyqUKFC3v58B3KUkGLkjMrDoeZNEg1nicrbyHNsqONybCBD8jzWMUTH3WogFAmd3ERNw8o4uhPOUjoC2r44OikhRywWQorLZBpPAxSjKSBK6VxQFVIglQWKWjel8GGp1EGB56ohIIp9ZIUefF4cpQ9DxV5GMTVMQAkKn0RxGTYOTXQ6u9Q6TyGHm+qIikAm/5503a3m7xNSQhElzZW7lREPnMbRWmEpCGjPL9EZKXSfiVhP1pcDq6OFAmbA1wGEe1haQDuWRkNjb48xvPdWbGGOrIuWy2gO5kOWNqIT43J7DEf1GEzXz/zTEiUZbwjXsKiA/jmeIrKjiJggRmBnL7x5PpFwDIu2dmRnyOQydBJtJFANTE+WEI5hUQHJoRiX0wQxChnNyOVywjGwvRUxCRQQb6AUDWMU4RgWFRClhCBGQQmgldK2BcQoIYhRQMcIBy1Iy+ZAbBM+YkVY3gZCBRkJBb1xtl6EKf5hEWYk0MsrsHEBIaYgh3YgmW3bQIj1YVEBCYXsnBnEGNAGgq4MmsaeDGNRjHTjnoC4niHMmv31lKlj3x3nxYtn7TqE3rt3W8/wAsye883Ur8YRY9mzd3tYp6bE/IANxHDPBrKogBQ5sIGvUOvWHTp27PLuOB4enoM/HVmuXHnYjoh43v+TboXDETNh2Wo8zRADa/Ed2r9fbBwvrzLDho5ht/99+khnuJXAvY4gy+ZAlME5kKoIg6wFyqPHTx7OmDkVNj7q32X1muXs8AZVUbVh45qFi+bEx8fB1127t2oWYRkZGbB37OdDPujaatCnvVat/iknJ0f/29i5a0uvPmGXL5/v069T+7DGgwb3PnnySOFocJM//7JwyLB+73/QYvSYQQcO7lbtgsPh66bN6zt0bNKtR5s5c6clJr4lhsK9VjQLV+ONf4FYV3BLl80bNHDEzBnzHz26/+XkUVWqVA/r0FkVB/IbiURy7vzJ7dsOE6WwVLv27tu+7a+N07+b5+7ukZGRvmLlYqFQOHrUBP0uDvVHUWZmxpmzx7duPiCVSffs2bZg0ewaNWoHBgZrRvt11dK4uJjJk6fDqxIVFQli8vHxbda0JXv/O3Zs6tKl1/59ZyS5uaPHDtr4529TJk8neqN4+4Scs1kt3JlKE9NeoTatw9q2CYONevUa+vn6P336WFNA7+CjDwe1ad0hODhvwteDB3evXQ/XX0BE6Y6uT+/+jgBxHDpk9N6928+cPTF0yCjNODNmzM/KyvQt7wfbDeqHHj9+EK7CCgjw9w8cNHC4YsvFtXFoc7h5YgiQ+zA23pmqwLRCvGrVGqptFxdXyEv0PBAygOs3/l6wcNaz50/ZRYA8Pb2IgaiuDhmMn19AVFREwRgMA8L659qVV69esgG+vv6FDwdcXd0gSyP8x9K98ZRpOZDRDZFr1604enT/6NET4dX38Sm//vdfjx47QAzE3t5eve3gUEAB0MY17buJUqnks5Hj69cPdXVx/WLiCM0Ipo+F4mBDooWN6NKpRzAMc+jwnt69P+7WtTeohyhsan2zLk0yMzNV27k5OQ4Ojpp7n/735MmTh2PHTHqvVTtQj9FXeQccHE1lUQGV1q+XSqXZ2dlly5Zjv4KhHf73RWI4t+9cZzdyc3OjXkVWqFBJc29qagp8eudfJTLyBfyRksOIOqwFsKyAzD8iMSAgCKrHUN9WWSGAWCwOCgo5dvxgdMxreMyLlsytU7t+enqaZo5SLFB6gn0DdStoO/hjw2rQUIf2WvZ7SHBFkUi0Y+fmtPQ0iAYVvcahzeLiY0kJoUg67nUEWVvfZrOmrUAcM2ZNhSqSZviM6T862DsMHdZv0OBejRo2GTlyPHzt3TcsNi5GzzNDBgBVuclTx0DHBRSI076eXaAOD4UjNBM8eny/Z6/2330/aeSIz3v06Pf48QNoFiLWi0Xnxp/dEf/4WvrgmZUJ34AOr1Wrl505dY2UHlt+fOEXIu45lltLLFh4VgYH2+J5gyLlbHxaD7tWE+Ek0E79118bde4KDqnYrm0nUtoIBJRAiNN6uDomunv3vu3a6VaJSCjy9i7Xt08pr9sqpxnaxoe0cnliIbTcsI03iEFYfkw0zsqwKixtA+GIVivDwn1hnFnSigAAEABJREFUODXVeKDwF9j8cA6cWGg8ykVkbXtAGQV96ZgBmYKtj0g0eUAZwjUs3pCITdHWhaWr8SgfK8PCRRgXh0QhpmBRAYnFYjt7nBxvJGJ7gYOLHeEYFn2cfhWdaDkWYkYilzJl/ewJx7CogCrVc6CE5GF4GkEMJOpfCS1nGnXwIBzD0gVKq64+d84ZPiPT5rm0N7p+mzKEe5SCu6fURPlfC1+W8bOvUMtN7EQVteoW68FL52R6SumaiwigaZYqcIxWU1v+V0pnF65yr9YuHY7AVGegGNVwJqboK6rczeXt1Epe1UkYdrXR/F3qk2ufXyASynPoFw8zE6Oze4319wnhopOR0nE4lxxHjm6MykiVySQMU1TzPFV0z32egzfTuvYpdtFG/VoWNNzXveOiBeRUVOSCwZrfNbYpASWyEzi5Ctv19Qmozjnrh4XCerUmzZo1u3LlilAoJIh+4BqJWsjlclSPQaCA1MhkMlSPoaCA1EilUnYRGUR/UEBqIAcSiTBBDAPTSw0KyAgwvdSggIwA00sNCsgIML3UoBFtBCggNZgDGQGmlxoUkBFgeqlBARkBppcatIGMAAWkBnMgI8D0UgM9qSggQ8H0UoM5kBFgeqlBARkBppcaMKJRQIaC6aUGcyAjwPRSgwIyAkwvNSggI8D0UoMNiUaAAlKDOZARYHqpQQEZAaaXGnt7e09PT4IYAgpIjUQiSUpKIoghoIDUQPnFulNF9AcFpAYFZAQoIDUoICNAAalBARkBCkgNCsgIUEBqUEBGgAJSgwIyAhSQGhSQEaCA1KCAjAAFpAYFZAQoIDUgILlcThBDQAGpwRzICFBAalBARoACUgMCkkqlBDEEFJAazIGMAAWkBgVkBLhSPRk+fPitW7fYFaJVqQFfr1+/TpDiQPdvZMKECQEBAZQSQT6BgYEE0QMUEKlfv37t2rVpDS87kP1069aNIHqAAlIwZMgQPz8/1VfIfnr37k0QPUABKahRo0bLli1VX1u3bo3TM/QEBZTHxx9/zNo9ISEh/fr1I4h+WE81PjuJvHqZRefXwykK6lMCJt/fILwpas92Chdziu/5MSml57mybRsOuJx9pVntZmkxrukx6YzaoyBhBBQp4BhP2zlhXpiI8g12dfUitoM1VOP/vZF9eX98bq4clCCX5v0chnUsmf9F6T8z/wClx0ktZ5Qavil1+DAscDgL5N2FfHUK7EBnROwgaNC6TKNObsQG4H0OlPBacm5nXLXGnqGduOLR+N751FsXE72DxUHVHIi1w+8c6Pm9rFNb4wZ+V5Fwjy3/e9HsA+8G7VyJVcNvI/rCnjdB1VwIJ6nexPPGGet3cM5vAeVmyRp0LEc4SWhHT1kuLckm1g2PBZSdoagGuXDYVGUY6m2MlSuI10a0nJFz2oCTy2na2nv3cTgHYhIoIMQkUECISaCAEJNAAZkRRScaRRGrhscCoqCDittPh1G29BOrhscCYqA73dbHc5c+fC7CKEWHO5ehCOF4Hmk6fBYQQyju50Ac17jJoBFtRpSD2dAGQkzB2q00HgtIQLheC7MFeCwgmmAtrPTBIgwxCdua1rNn7/a+H75/+cr5Dh2brPh1yeMnD9t1CIVPVYRBn/Zatfon2IiIeM7umjFzKmx81L/L6jXLcf2ywtiWgMRicVZW5sGDu7+dNrd3z4/eEZN1Xbh02bwOHTqfPP739G/n7dy15dz5U8QQKGVTA7Fq+CwgAWXo06EoKicnp3//IWEdOgcEBBUbv03rsLZtwkBM9eo19PP1f/r0MTEE5QwhrMZzFsbIh1O9Wi09Y1atWkO17eLimpGRTgwBW6K5DcMY10oHBZmeMQUCk3JoxuoLMKyFFUAmL9ExzBSx+uW7bFpA9mJ7+MzOzmK/ZmRkvH37hpQctjAeiMdGtOkt0YGBwa4urkePHWAYRiaTLVg0y9W1JGcJ2cJ4IB4LyPSWaKhezZgx/8mTh+3DGg8Y2L1tm46+vv64aKRB8HhufHaG/PfvI4bMqUy4ysbZz3qP8Q+o5kisFzSiEZPg94hEjkNZ/2gOzIHMCWP97Yj8FhBl/c+H8/C7JZrrJYQNZEFoA5kTGzCC0AYyIzgzldtw/uXGmakIUgy8nhuPtbDSh9dz47leC0MbCDEJtIEQpBh4LCAhEVIiThcQQrg9a39DeTweSOyi8IGSnsDpuVoBIdY8loPwfV6Yk6vdrXMcdSfw95FEewfIJYl1w28B9Rga+PpZJuEkkfdT3+vpQ6wd3vsLy0iRb/7fy8Dqrk06eztywO+KJJtcP/nmxf30gV8HxSU9r1atGrFqrMHhXFI0c2B9VE6mnNCMXJ7nBY4hlOasUParqnecYShKe1IZzVACSu1zjmj01TL5k5QLTDMtHEIJBQKKODgL2/bzGfdVH0dHR09Pz0aNGoUqMXGWGTexBgGpOLjv1MN7T8aNG6f4oulRME816u5xVUDeF+UMwKjoqJkzZgIVQyoq3B0yaiUxqhPmR1aFUJonFhJ3rzyrZ/Hixdu3b2eUUvXx8XFxcalVq1bLli3DwsKIFWE9Arp27drGjRtXrVpFjGXWrFmHDh3q3r37nDlziMncv39/0qRJKSkpqhBIasiTgoODt27dSqwFK8lUX758uXDhQlPU8/jx4xs3bkApA58RERHEZOrUqePt7a0ZAlkRCMia1EOsQ0A5OTkDBw7cs2cPMYENGzbExsbCRlxc3K5du0hJ0LlzZ0qjL8zV1fXUKcMWiOE+1iCgLl26HD16lJjA3bt379y5wxq58MgvX77MislE2rVrB9YPu+3k5AR5ErE6eC+g/v37r1271s3NpCnJ69atS0xMVH2NiYkpkUwoSAmYPs7OzhcvXoRbTUhIINYFvwU0ceLECRMmVK5s0uTU8PDwp0+fUtrjLs6fP09Kgm7duoF6Lly4ANstWrRwd3c/ffo0sSJ4XAubN29e7dq1e/XqRUxjwIABYEFD+QVJwZZijJLbt28TM5Camjpo0CCo7hGrgK8CgmILPkeNGkVKjoyMjDNnzvTs2ZOYGTCwPDw8oEZG+A8vi7ADBw7Ex8eXrHqAN2/ebNmyhZgfX19fUM/69eulUinhOfwTEJgskE/MmDGDlDRQze7duzexFMOHDwcLifAcnhVhz58/nz59OnQRECsCWquhRCP8hE85UFpa2meffWY+9UBN/vjx48TiQLkJLemEn/BJQKY3GL6bqKgoE5uzjWP8+PErVqwg/IQ3Aurbty/0Ijk4OBCzUbZs2Q8++ICUBkuWLIHPR48eEb7BjzHfY8aM+fbbb6Efm5iTQCWk9IAulMzMzMaNGxP+wIMcaNasWT169AgNDSVmJjo6+ty5c6T0gIYJ6JUjvILrAvr1119DQkLA+iHmBzo0zGpj6cPIkSPhs9RvQ384LaCdO3dClj5s2DBiEfz9/du3b084ANQ3oa2L8AHutgNBB+TBgweXLl1KbJKzZ89yRM3vhqM5EPRuQku/hdUDrZRXr14l3IBVz88//0y4DRcFBA16kyZN2rx5M7EsYMCWrhFdmBo1anC8356L1XgwmaHDi1icSpUqeXl5ES7RqVMnMO0Jh+GcDdS9e/e1a9dCfzVBNIDaGZTphHtwS0AzZ84cOHBgac3mfPLkCVT6GjVqRLjHvXv3IiIiLDBWyVC4ZQOJxWIwn0lpANIZPXo0N9UD1K1bt0OHDoR7cEtAderUuX//PikN4uPj9+/fT7hKTk4ONxupuSWg2rVrP3jwgFgcuVwOrYienp6EqyQnJy9YsIBwD24JCOpBMTEx2dnZxLJ07tw5KyuLcBhHR8dWrVoR7sG5Wti4ceOGDh3apEkTYilOnjzp4ODQunVrghgO5xoSLV+KQVsL99UDNtCVK1cI97B1Aa1evfrVq1eE86ANpC+WFNC+ffuSkpJKdxCZnqANZADQXLZq1SqoFhEzA9a6dczuK0W42JlqmUwIGnYtX90zGrSBDMACArp+/fqiRYu41nX6DtAGMgALtEe/fv2anQjBF9AGMozGjRtDJkEQzsPREYm1atUyUykG1S7oNCV8A20gwzCfGbRy5crx48cTvoE2kGGYzwyaOXMmH9cq5KwNZFs50LZt26DjnfAQDw+Pb775hnAPjgpowoQJ0dHR0KXasGHDBg0azJs3j5jM/PnzodNUKOSl/xzO2kCcG1Rfr149eMbsWoU0TcOGs7NzixYtiGlkZmb27t27evXqhJ+wNhAHZ2hwLgfq1q2bSKQla2juq1GjBjENOGfVqlUJb8F2IAMYMmQIGEDsuruQCYHN++effxIT2Lx5M9TeJ06cSJCShos2ELQRh4SEqL5CoUZMAAqvmJgYvqsH24EMwNvbe8qUKeXKlYNtNzc3Exd2AROKm/UXg8B2IMMAq3nAgAGgHnd3d1Ms30ePHlmHdxy+2kDndrx58SBDkkPLZQU9AWr66ytimyKaPgOVO/IQUIQu6E5Q4ysp4M27gL/Bgmcr4jwsmq4ItcIVPp91//b8k2vdvwpKJBAJKa9y4g8nBxCb510COvp7fOzL7Ao13ao3cmfyW08UT1GZsKxPPzaR8x6tlvO+fBd/bIj2s6ApRsCon3/eqbS/UspLsGelC0hKqSbFWfPjaN2GOqDwz9UI1R1D4wY046ouBD9LKIx9lvH4RnJGimz0/ArEIoANdPPmzZYtWxKOUaSADq6JeRMt+WhqCEGK4Nbp9Ke3Ez+bF0LMT2xs7KhRo3jTDpQYJY9+kYPqeTcNw1wdnER7VpSAZ7Fi4Vlf2JUjCU6u/FjAtXSpVNclMdYS42J51heWlUnbiazQR3WJ4+4tlktpYn541g6Umy3NlcgIUhwMzcgtkk6cbQfCcoofcNYGQgHxAxwPhJgE9oUhJsEzG0gxlIIiCHfgnQ3EoH44Bc9sIOje4K03cOsEbSDrJK8L2fxgO5B1wpAiu/RLFmwHslIoC+VAPLOBKAHBWpheMBbKgXhmA7HDtRDuwLcx0SVaC+vVJ2zTZi46CuERODeeB+zbv3P+wlmEk2BfGA/491+D3bZbrLGMZ3PjwYimDCzDuvVo88mAYfAMLl466+zsXKdOg+++/cHVxbVAtL37dly9eunx4wdie/t6dRuOGPG5v59ibsOcudOg/ySswwcLFs3Ozs6qWbPOmFETa9So/e6LvuMomUz2+x+rrv5zOSEhrnbt+r17ftSsmaIIOHXqKET+bfWWypUVM50fPX7w+fihc2Yv2rtv+927t4hi4foj587cIPphMUuRZ3PjKUVvmGGZk1Ao2rV7a7dufc6evr5owcqoqMgVKxcXiHP//h0IrFWr3ty5S6Z9Myc5Oel/P37P7hKJRA8f3Tt1+uia1ZuPHblsL7bXpzR5x1G/rFi0e8+23r0+3rb1UJvWHWbN+frCRYUf5I4duzRq2GTpMsVyHwzDwEZYh86t32u/fNlaUF6nTl31V48l4ZkNRNMMTRucPVeuVLVxaDOQHuQEPfUDFQoAABAASURBVHv0O3/+lFQq1YwA4Rt+3znwk2EN6odCzI8+HARZUWpaKrs3Oyvrq6kz/Xz9QRYd2nd+9eqlPg5QdB6Vm5t74uThTwYM7dG9r7ube5cPesKuTZvXsYdMmfx9ROTzo8cO7D+wKykpceKEaYTzcNYGKqo3Xse0vWKpXFntadDfLxDUExPzOjhYPXNKKBRCyK+rlj5+8iAzM5MNTElOgmcMG4FBIU5OTmygi7LsS09PU4UUhc6joNiSSCSNQ5urotWv1+jY8YMgVriWj0/54cPGrl23Qi6TTZ/+PxcXF2IsynLeEuUYZ+eF6RaQcZ2p9vYOqm0H5QrwmZkZmhGuXLnw/cwpkAONHjWxUqUqN27+8/U36uUK2TWBDEXnURkZ6fD5xcQRBcKTkxJZsfbp3X/jn7+JhKK6dRoQE1C+ZpawpDlrAxWdAxGD0ZRLjnIReAcHLUcCh4/uq1On/sgRn7Nf2cdsDsqU9SaKomq6v7+WH4xy5cqzG9t3bPL19Yc8cu26X76cyIMijG99YZQxFfy7d2+qtv979i8YJQWeX1paankftTvmS5fOEvMQ4B9kb28PG2BssSFgsIPJzBZ2kZEv/ty09peff5dJpRO+HNmpY1cwzgi34dt4IFrxZyhv3iZARUwul0MV7PCRve3adWKfogqwsq/fuHr7zg2oY0NMNjAuvuRndoJQhg4ZDVYz1PvAGIL619Svxy3/WdEVQNP0vB+nQ82/RvVakB12aP/+jwtmwv3ALpA7GPW3bnNxgXObGA/UrWvvhw/vhXVqOmRYv+CgCl+M/6pAhOHDxzVt0uL7GZM7dW4eHx8HNfnq1WpO+3bC6TPHSUnT/+PBUDvbtn1j955tf/5loZ9vwJQpiiaDrds2xMfFjh07iY02/vOpycmJm7coelq6d+0DVUhNs4w7cLYvTPfiCn/+EEnTpN+XIURvevbu0LfPgMGfjiS2ROSjzAs7Y8f/VJmYmZSUlN9++42DpVjRg+oRPbBYMvGsHYg7K29279G2qF3ffDO7Vcu2xDbgWTuQERzYd4aYgW3bimz5cHQofWeD2BdWRBEm5MqAssLdsZzCYhk1D8dE47QePWBwTLTOUEaO88L0gsIx0QThAzgvDDEJntlAAiGaQHphsVTimQ1Ey43pC7NBLFZXRRsIMQm0gRCT4JkNJLQTCORoBRWPUEgJhJbIxXlmAzk4iymKl75FLUx2BiO0IxaAZzZQpTpOmekSghTH8/tpLm6WMAN4Nje+QVs3sb3w5MY4gryTN9HZPUYGEfPDS39hG+a8dHUXvz/MlyCFuHsu+WF48sBpwS5eNl3WF+NwbsuPr9KSJWBTS3Pk5N0noorrPlPO9Hh3nHefRGGV0aadQdluY8oZAMib5TRjJxK06etTpaGFhpTwz1+YGjm5eTYlK7NYk4gqrmHWVAXFv0mIjY6tX7+e0WdQe1t81xmK2W9nJ/Kv5BxY3Z5YEM76C9PDABSSRh09CAc4efLOrYjzX/TqSGwP9BtfAkBNJD09PSjIEkYroid86srw9PS0WfVgX1gJcPHixU2bNhGbBPvCSoA4JcQmQRuoBHjz5o1EIvH39ycIZ+BTEebt7W2z6kEbqAQ4fPjwgQMHiE2CNlAJEB0dbdwiVFYA2kAlALTGgoB8fHwIwhn49EL7+vrarHrQBioB/vrrrzNnzDIDn/ugDVQCREVFiUQ2OogbbaAS4PXr15COZcqUIQhn4FMRFhAQYLPqQRuoBFi3bt21a9eITcI3f2Gc5NmzZ+np5lpamuOgDVQCREREeHl5ubu7E4Qz8CkHqlChgs2qB22gEuCnn356+PAhsUnQBioBnjx5Ai8isUnQBioB/vvvPz8/P2dnZ4JwBj7lQFWqVLFZ9aANVAIsW7YsMTGR2CRZWVnbtm2Ty+WEY/BJQB999NHMmTOJTXLjxg347UIh56ZR88kGQjgI/wb4Xb169fTp08RmOHDgwA8//EC4Cv8E1KxZM6jPHz58mNgA8fHxGRkZM2bMIFwFizDEJHg8Rn3t2rWpqanEehk7dmxUVBThNjzOgWia7tSpk7XaQzt27KhRo0bdunUJt8EiDDEJ3k+zioyMPHLkCLEi7t69u3TpUsITeC+gkJAQsIR++eUXYhWkpKRAi/OUKVMIT8AiDDEJ65kpfPTo0ejoaMJnVq1a9ezZM8IrrEdAXbp0GTNmTGxsLOEnGzduDA4OrlzZ7C7oSxZrK8Kgv5qDPY5WjLUtdpGZmblv3z7CK+Li4n7//XfCT6xNQG5ubmXKlJk8eTLhCZBlDho0aMSIEYSfWGctTCKRwO+yt7foWuC2iXWu1yQWix88ePD8+XP2a8+ePXv06EG4x6lTp3hX7SqA1S741ahRo2XLll27dg36y6B6D42Nly5dIlxi06ZNT5484V21qwBW3pDYrl07djY0/MyhQ4d+8cUXhBuA6SOTyaygkLXmJQfbtGmjOZf+3r17hBvk5uaeOHHCOkw0qxVQ48aNoUqv+kpRVExMTFJSEuEA3bt3b9asGbEKrFZAYWFhfn5+FKX27J6RkfHo0SNS2rx58+bQoUNeXl7EKhDOnj2bWCMgIHjLnZ2d3759K1ECNkdgYGBoaCgpPdiKodWohxhtRKe+Icf+jE5PkUpz5TStOIPKTRulLC9ohlGH5PuAE4oouUwrct5NUIrbUEXTOBWlukHNQwQCRWheuIaDOQhnb0YrvtLDHM3QsAEXoiCS9tUVZ1PszD9Q44RU/ol0phF7HkoAZ9cRzp5KYb0rLqs8p/KeKQGlPlbxH6Xz0tq3VzC4YALm/9Ci3O1pPh3tlC8yvthR5OwmbN/Pp3xFMSkaYwQUF5G7b3W0p7ejT4gDQ8lpOU3yRaA8Izx1SBRaoJSR5l0LhAI2skDhu5K9vOJfXsz8aGolKR8OoxGTvQGID5FZraivotiG+EoxKMWn+dwVsoF7Yu9QO9WElECu0Ij65KoTUuyzZ2VQOO2UlxAKBPAOae/Iu4BAkSaKUxd+Tnk/R9sHp4BQdCGtCgQCRvW6aFxbMyJ7n5ovbcFbZV9R5fVo7SN1HiCkhHBI3KuslHjJ+5/6VqhdpGdPgwV0/XjqzbNJA6dXIIhtsHX+i+oNXdt+5K1zr8FG9M1zie0HlCeIzfDh5xUfXy9yZUHDBHTlYIpAJPCtaCFXxQgXELsRsZPg1OY3Ovcatm7329hssZgiiI0hthekJOl2222YgHIzZZIcmiA2hjSXpoqwlW3UcwBiEIpar0B3yYMCQopH0RhBYw6EmAEUEFI8yiJM9y4b9SCJGATNFNnebFgOJBASW3VaatMIKM1hDVoYJiBaTmisxdseSiNa9y60gZDiUWQ/JZIDkaJPhFgxyrEouncZKKCiT4RYMWD4UkUoBYswpHjA8GVkuncZJiAGq2C2SUnZQBRWwWyTok0XG81S9u3fOX/hLILoB3amFuTff0t/fg+fYJgiJhaYX0DJyUnzF8x8+OheUGBIz54fvn4ddenyuT837IZdMpns9z9WXf3nckJCXO3a9Xv3/KhZszynfL36hA0bOiY1NeXPTWsdHR0bhzYf//nUMmXKwq6kpMRVq5c9eHg3JyencePmgweNDAwMhvAXL56N+Kz//P8tX7JsnoeH5/q1f0VEPD94aPet29fj4mJCgit26dKrZ49+EPPLyaPu3r0FGydPHvltzZaqVaofP3Ho4KE9ERHPKlSo3L5dp759BhTZ8prPrNlfC4VCHx/f7Ts2zZm9qPV77R8+vAd3++TJQ3cPz+bN3hsyeBTr3QwSf8/ev06cOPzq9cvgoAqhoc2GDxsLx+7ctWXbXxunTv5+2fIfU1KS/fwC4Ld06tSVPf+VKxfgbC+jItzdPSpXrjbxi298fBQjiefMnQb3FtbhgwWLZmdnZ9WsWWfMqIk1atSGXVFRkRs2rrlz9yZcsVatuv0/GlynTv13p7OeMEUnh2FFmEBECQxc/mvRkrlRryIXL1o174dl//xzBf4E+b0hv6xYtHvPtt69Pt629VCb1h1mzfn6wsUz7C47O7sdOzZBzP37zvy5Yc/9B3c2/vkbUU4pnzRlNKTRpC+/+2P9Dk8Pr3GfD4mOec0eAp+btqz/+KNPp0z+HrZ/XbX0+vW/J074ZsH8X0A9P/+y8Oo/Cp9ty5ethRSHR3XuzA1Qz+kzxxcumgMb27YcHDnic7illauKX2UXLvci4hn8/e+HZXXrNHgd/Wrq1+NycnNWrtjww5wlL178N2nyKHhyEHPv3u1btv7Rr+8n27cd7t6975Gj+0FzEC4UijIzM86cPb518wH4mR3avw+aePXqJey6cfOfmbO/gjvcuf3orBkL4uNjl/+S5zBVJBLB23jq9NE1qzcfO3LZXmzPlsUSiQReDNDlwgUrli5eLRKKpn8/ifUQ+o501peiW6INExAtY2hDXJ6lpqVevXr5ow8/rVmjNuQf8FwhM2B3KeaHnzz8yYChPbr3dXdz7/JBzw7tO2/avE51rL9/4KCBw11dXOFAyIGePn0Mgffv34H37Ltvf2japIWXV5mxY750c/fYs2cbUc5cgc/Goc0+7DewRvVasD1jxvzFi1c1bNC4Qf1QyHuqVa1x7Xp44Zs8enR/3boNvpw4zdPTCyIPGzJm//6dkHG++6cpZr3ExcyZtahFi9aQ4Z0+fcxOZAfSCQoKCQmpOHXKjP+e/Xv5ynmIefferWrVar7/fjeI1q1r719XbmzapCV7ElBYn979IYt1c3UbOmS0s5PzmbMnIPyPDashSwPNQfYDecm4sZMhGZ/kF7vZWVlfTZ3p5+sPYoJEA81lZWXBJ9wz5J3wJlSqVGXWzAVz5iyG8xebziZimICUTdoGtCRGvYyAz9q167FfXVxcGjZswm6DIOClAWWoItev1wiKIdAc+7Vq1RqqXa6ubvCywgZkRfDqw2POvx8KjoInpIpZtYr6KCg84O0fPLRvuw6h8AcPIKWQLGiahtJQ8zYaNGgMgffu3ybFAeWRg4MDu/3w4d3q1WvB82a/li/vC0USexL4+Tdv/rNo8VwoKOHX+fsFVK5cVX3D+T8TfgscEhWlSDHIwKor3wGWalVrEoXP4TyP1YFBIU5OTuy2i4srfKanpwUEBIFAIQ+D3O7Bg7uQecNrAwlebDrrBUVKpjNV2aRtQF8G+9SdnV1UIW5ueY7fMzIUM0W+mFhwabfkpER3ZRydxS4cJZVKQQ2agZBwqm1x/pIXIIJp302USiWfjRxfv34o5GSFr0WUOT+cEEwE+NO6jeTil2EQayyvATcGAi1wY/Bb4BMyEicn5yvhF6CghDyjbduOoz+bULZs3jQrzTU67B0cIMUAyDbs7R1U4axcsrLy1ooQ6BoRAef5+ad1UD5CaQW/BbQ4dPCojh27FJvO+gDPvKgmQPMa0WwSSyXqAf3JKXkPpowyBadMng5FleYh5cq9a9IZFGeQ4f9v3k+agUJddtnT/54Z0/0eAAAQAElEQVTAK7tk8apG+XkeJKV32XIFokEWAo+nU8eurVt30Az38w0ghuBVpixYrGD4awa6uykyJHjeUHLBX2Tki1u3rm3ctBZU8mP+T8jMzFR5Es7NyQGrjs3VcnKyVefJVEqnjFfZd98DlJ5QpsM9wFWOHT/444KZwSEVjUvnAija/0qkN97Q8UC+vv7wGRH5HMwColwfA34b1FxgO8A/iH35IKdlI8NLD9UHVeask0qVqmZnZ8OPh4KADYmJjfZw9ywcE2pw8KlSDDw8+KsQUknnOdMz0lW3ARlSbGx0uXI+xBAqVaxy8tSRenUbqrIHuBwUK7AB9S8opypUqASJAH9wrSNH1evI3r5zvVXLtkRpFEJto3nz9yCXAnMN6nSqOOx2xUpV3nEDYBqCcf1B5x6gPzDLmjZt2blLSyi/2rd734h01kGJNCQaOh7It7xfcHAFqI5CRQnUs/zn+aykiDJbBrMRrDmwi6EcgXoB1GKW/7zg3SeE7KRJkxZLlvwQHx8HEtl/YNeYsZ8eP36wcEyot8OT2LFzc1p6GiTuipWLwb6Oi89bhhxex8ePH0ANH1LzsxHjr1w5f/TYASj14Gbm/vDt5KljJBIJMYR+/QbC4VB9g4oP2LO/rf1l+MiPoY4Gu6CeBVWq8PCLbJXi0uWztWvlGYWgNrDS4PagdgmGM2gILFwIhxoTGOB79vwFN3/7zg1otgCzr0rlau+4gbS0VDCzVq9ZDvVBuIGt2zaABQ0XMi6d9cfs7UBfT50JDTOfDu4N7ygUyWAPwZNjd/X/eDC8/du2b4RsCcJr1aw7Zcr3xZ4QWnqgzWbuvG8fPboPLUBhYR/06dO/cDRoNZn+3TzQbs9e7UEu07/9ITHp7YyZU4cM6wetUN279oG386uvP4dKb2ijpmvXbIUUh6cOBQfcBrQ4GLp8GFSjfl+/Y/v2P0ePHQSCABP4q6kzoEJEFMXH9yt/XTJ9hmLlYag5Qln2Yb9B7FFg53304SDQa2LiWyiap309m23Tggr8m7cJO3ZtBkXCDwlt1AwsuXffAJjqkyd9B40d0LwEX+FHLVu6hs34jUtnTd4xHsiwxRV2LnuV8kY2YJoBKytAPgEvJdsIBnw7/Utoovhh7hJi8+zZux2yljOnrhHOs3tZpJ0DNejb4MK7DGxIFBJDGxKh5RSa1KD1GZS0ecvvUKHtoWwORniEIo8pESNaYQMZ0pAIzJq1cPGSuevWr3zzJh4aTqBdFWwRwge692hb1K5vvpnNWr42RGkVYfwF7JKidkHDplj8rnW7rIxdyyLFYmrQdB1FmIHjgYqeYGZ9sH23iIKSHFSP2B55qzrqwsCuDLrIXlnEuikq68BB9Ujx4MRCxFwYbkSjGWSDMCXUG88wBL2E2yJUkRNysAhDTAIFhJiEweOBipofhFgximHMRTx3w9qVHZ3sRPY4vdnmENoLnVx15zWGqaFKfffcLAN7UxH+k5shC6iiewSjYQKq2tjRTkxd3JtIEJvh9pkUkEmT9z107jW4PBoxN+T109QrB94SxAa4eSrp0T9Jn/2vyPEXRjqc+2NWpCyXFjsJaQmRFzpDnnOqvG1105FQyMjlhW0xRunNqmBgXheepqcthXsuSuMqROnMDRooBJqH57vGUkfWvJ+i7lDpJUzwzhvQClG459J4/RhCU+qvDFE5mSt8dQFD0RRT+Mbyb1ggUPo103XzAiFDy6n8G1aHK9r52DRUnqRgBIUXMkH+2ZQ/k2IPogrefP49wLEisSA3ixaKyMh57xq9Y7zb73+vZ/13Jy0jXUbkBa0iSihg5HkNT5oO/QR2FC0t7JJP6bdPeyF0VgDKUxFGnu82UKDVnJV3ZoHS8Rut5UEORK12aqc6ifpIZZ1Cnu9VTukSUDO+8q4E7PyBvL3KiAkJCd7lvAV58ZVPSeXbkFK7VmTdLFJCilFeQiAQ0BpTERQxFW7fWB+OAqJKKFV8IaW1MrxQfasCkYCW0ewtUQIBozqt0uOc8q4Ut02p7pB1Jpl/ZuUpFD9TEYEo74HKfy8YrXuAszu7iirUcanTwpW8Eyv3G1+yNG3aNDw8XCg0cFSvVYMNiQYwZcoUVE8BMAdCTAJbBfVFKpUuX76cINqggPQlKyvr0KFDBNEGbSB9sbe3nzhxIkG0QRsIMQkswvQlNTV17dq1BNEGBaQvKSkpJ06cIIg2aAPpi4eHx6hRowiiDdpAiElgEaYvMTExW7ZsIYg2KCB9gZ7U8+fPE0QbtIH0xc/Pb9CgQQTRBm0gxCSwCNOX58+f7927lyDaoID05dWrV+Hh4QTRBoswfXn9+nVUVFSLFi0IogEKCDEJLML05cGDB8eOHSOINiggfXn69Ont28W78LE1sB1IX+rWrRsSEkIQbdAGQkwCizB9uX79+oULFwiiDQpIX548eXLnzh2CaIM2kL40b948MzOTINqgDYSYBBZh+nL37t2jR48SRBsUkL5AX9i1azzw7WVh0AbSl3r16vn4GOZI1RZAGwgxCSzC9OW///7bs2cPQbRBAelLfHz8pUuXCKINFmH6AgJ69uxZy5YtCaIBCggxCSzC9OX169ebN28miDYoIH1JSUk5c+YMQbTBdiB9CQgIGDx4MEG0QRsIMQkswvQlMTER1wcqDApIX7Kzs7EztTBYhBXDuHHjEhISiHKV1szMTCcnJ0gxiUSCi02xoBFdDPXr11+/fr3KVwHUxeCzbNmyBFGCRVgx9O/fPygoSDMExFS9enWCKEEBFYObm1vPnj01PRx4enr27duXIEpQQMUzYMCAwMBAdhsMoIoVK7733nsEUYICKh6RSAQFmb29PWy7uLiAngiSDwpIL/r16xccHCyTyaA9un379gTJx5qr8YfXxSW/leRm0lqhlNJBospTm9Jpn8K/n9oxXp5nwvwYec7YoOqelZXl5OwotrPXdoeo9Eyo9AqY56VO07Mi60JQw+sei4OTwK2sXY8RvoTn7qOsU0Av7mWd3Bxn7yJydRPl5kq19lEKz/c0rXZXSJT+EtWeCbW8bBb8KhBQtMrRX16Q0tWhMlqeL0EtN52KL3lHaWDvaJeRKs1Jl7fsWbZOKzfCW6ywHejOxfSrR95++HVFsZhwn23zI9KT5S26exJ+YoU2UPjhNx9OqsAL9QCffFvh3qWkzCTCU6xNQIfWxzs5i8SOhEe4eNof2xJD+Im1CSglQeLqybNy2bmcKDUph/ATa7OBcjJlAr79JiZHJs3ia1UGO1NLH6XTd8JTUEClD0N43JaCAuICFBHwNQtCAZU+0PwIbeOEn6CAuAEWYYjR8Lo3CQXEAUBBNOEpVicgRWc74RnQ18rbPnmrExBD8a9AgJ56OeEp1pcDEd7lQJQiB8JqPEdgCO9yIEaRA2E1HjEWRUcGdmUgxkPxuC8MB9Ubw5y5044eO0BKCj43BKGAjOHffx+RkkOhH2wH4ilfTBzh6OC4aOFKVci3079MTU1ZtXIjbG/avP7EycNv3yaUK1e+fr1Gk778ViAQtOsQCrsWL/lh9ZqfDh04D9vHTxw6eGhPRMSzChUqt2/XqW+fAQaVSQI+d6baeg7Urk3Hm7euqdzw5OTk3LhxNax9Z9jesHHN/gM7x47+cveuEyOGjzt/4dSu3Vsh/PjRK/D51dQZrHpOnzm+cNGcqlWqb9tycOSIz3fv2bZy1VJiCDRUHWm+FmLWJiDo1hYY8pvatAmjafrS5bPs18tXzsPXtm07pmek/7X9z08HjWzVqq2ri2vbNmG9e328ZevvUqm0wBmOHt1ft26DLydO8/T0atig8bAhY/bv35mcbMAoeYrwuBZmbQJiCGVQv1KZMmWhbLp0+Rz79cqV840aNvHyKvPq1UvQSo0atVUxq1atkZGRER39SvNwUNuDh3cbhzZXhTRo0BgC7903wD0vw1v1EOtsSCSGAfnNyl+XQOElFAr/vnppwhdfQ2BS0lv4dLB3UEVzdHQiinXKsjSPlUgkoLPf/1gFf5rhBuVAiqZPHM7BX0BAv6xYFP73RbFYrCi/2nSEQGdnF/jMzslWRcvKUthJXl5aS0s5ODg4OTl16ti1desOmuF+vgFEbxRzZbEhkb+4u7lDsXXtWnhubk7LFm1AEBBYqVJVyJAePrxbo3otNtrjxw/AGPL2LlfADIKYYDA1qB/KfoW9sbHR5coZ4BiKYWj+NgRhO5ACMKXv3bt18+Y/kBuxIW6ubh3DumzZ+kd4+MW09LSTJ4/s27+jX7+BUI23t7cHGUFl7fadGzKZ7LMR48FygnZFyL3u378z94dvJ08dA0WbIdfnsRFkbYsrrJse4eQu6jE60KCjoBrfo1c7UMbB/edEorxcGUxmaOk5e+4EqMTPLyCswwcD+g9h9x44uBsq+TKZ9K9thyFbAot767YNYD/l5GTXqll31KgJ1avV1P/qp7dEJ0Tljl5YkfAQFFDpc2rL64SXuWMWVSI8BG2g0ofX7UAooNKHIViN5wwUNMsJ+PY0KAH/hlHmY3U5kICHo7Og7Zy3lqi1CQieBc23oREUnweUoQ1U+jB87g5DAXEAHFCGmIKiL4y3PQIooNJH0ReGORBiPBRF4fpAiPEoFlfAajxik6CAEJOwNgGJnQRie579KDt7sb0zX5fnsLYBZZ7e9pmpUsIrUt/kunjw9U22NgH1GFE+O11m2HjA0iY9Wdq5vz/hJ1Y3pFVImnYus2tJBOGJhv5aEFmnhYerAUOouYV1+gt7djfr5JY4FzeRs5dYliNThVNCorkWGCWkVAvzCEVEnh9R0+GXcptRNhcrncZR+b7CGHW0Aht5bucIO9YZurkUi6bl+6XLW5BV7CjISJZlpMhadC9brzWP/YVZrcdCuZwcXhuXkpibm6mWDCVkGLm6yU7TD1yegATQLUWpnc8pNUEr5rsyICCJRGZnJ8rTEK12cpi3IVBqhSYCIUUrdakIV0hH6YUuX3+sgBwchK7edl0H+4ldCK+xZpeXJU7Tpk3Dw8M1XYAj2A6kL4rmYppG9RQABaQvUqnUzs6OINqggPRFJpOppowhKjBF9AUFpBNMEX1BAekEU0RfUEA6wRTRFzSidYIC0hfMgXSCKaIvKCCdYIroCwpIJ5gi+gICQhuoMCggfcEcSCeYIvoCAsKOsMKggPQFcyCdYIroCwpIJ5gi+oINiTpBAekL5kA6wRTRFxSQTjBF9AUFpBNMEX1BG0gnKCB9wRxIJ5gi+oIC0gmmiL6ggHSCKaIv0I/h4OBAEG1QQAaQlZVFEG1QQPoC5Vdhj7sICkhfQEBgBhFEGxSQvqCAdIIC0hcUkE5QQPqCAtIJCkhfUEA6QQHpCwpIJyggfUEB6QQFpC8oIJ2ggPQFBaQTFJC+oIB0ggLSFxSQTlBA+oIC0gkKSF9QQDpBAekLCkgnKCB9QQHpBFeqL4Y5c+bs379fIBAwStgNJyeny5cvE8QKvfWUNMOHDw8JCVG4xRUIhEIho1rJzwAAB01JREFUbEBgxYoVCaIEBVQMgYGB7dq108ynXV1dP/74Y4IoQQEVzyeffFKhQgV2G5Tk6+vbtWtXgihBARVP2bJlO3fuDEUYbNvb2/fr148g+aCA9KJ///5sJgQlWo8ePQiSj3XWwu5fTo2Pys3OpOVSuUzKCIUK/3Osyzf4FFACmqFZX3FE4XZO4QNO4RmO5PskVHkszHc+CLvevHkTGxtX3tfHx6cchLGe49hzKs6gOECgrKblObETiBS71FdROktUpbVACBGJs4fIJ8ih7nvosZAL5JK9a6PfROdKcmlQADwfgUgIOgEUPgM1f2a+28C8bxQrIEbpiVB7v9IHYcFATY+Y6n2sP0t1PNZdofpCBS4qyDuJQm00EdlTZcrbv/+pr6sXz8oEaxCQXEI2L4jKSJGIHe1cyzj7VvUkfFsMM/6/lLSEjNxsqZOLqM8XwR5lKcITeC+gw7/HRT7McHRzqNTUl/CfFzfislNy/Co69v7cj/ABfgtow5yX0lym6nuBxLr499IrsJ5Gz+dBcyWPBbR+RoTYySGofjlijby69yYrOWv0Aq5riK/V+DXfvBCK7a1VPUBgXW+nMi6rv3pBuA0vc6DfZ0XaOVizelREP0zMSs787H8VCFfhXw50dH2cXEpsQT2Af60yUOPfuzKWcBW+CUhOXjzOtD6r+R1UaRkQG5kVHykhnIRnAvrzx5eOrvbExnD2dDy6kaOZEK8EJCEZyVLraO8xiJCGPlkZsoQoLo6H5JOA9q+PETlwdwxuRmby1BlN79w/TcwANLKf3cnFTIhPAop/lePq7UJsEk9/t9S3XFxgj08CkuUyflU8iU1SNthVKqVT38gJx+DNrIwHV9IpuFmz9ZKmpSceOrY88tU9iSSnWpVmYW2Gl/MOhvArV3eduvDH2OGrN23/Nj7hha9P5dYtBjRu2I096va9k8fP/JadnVaz+nttWg4k5kQoEjy+ltqsqxfhErzJgeKjstkxgeZALpev+WPc88hbfbtPmzJ+m4uz1y9rh79NfE0Uj80uOzt9/5ElH/X6bvHcq3Vrt9+5f15yShzsio1/tm33zNAGXaZ9uSe0ftcDR5YScyIQCt7G5BKOwRsBZWXK2BkR5iAi6k7C28gB/eZUr9rczbVM984TnJ08Lv29nd0rl0s7thsZHFgHbgCEAm330bFPITz8nz0e7uU7th3h5ORWuWKjpqG9iDmBHoOcTM5VxHhThDEMxRBz9bpEvrwrFNpVqRjKfgWhVKrQ8EXkbVWEIP9a7IaTo2L0YHZOOny+TXpV3kfd2RnoX5OYGTn3up14IyCRkGLMln7ZORmQzUAlXDPQxVltsOvM/LKy0sqWUbeJi8WOxJxQcsbOjnMj5XgjIE8f+4hHmcQ8uLqUgcc/fKCWEVOsyQUll1Sao/qam2uu22OR07SbF+cclvFGQDWbuN48nUjMg79vVYkk28PDp6xXABuSmBStmQPpxNPD99GTSzRNs1J79K95JzvLZUzF2s6EY/DGiHb3FhGKJEalEzNQpVLj6lWa79r/P6heZWSmXPln989rhl67dejdR9WrFQatz/uPLAWz+tmLm+H/7CZmIyMRsjqmYl0nwjH4tDqHi5soOTqtTJArMQPDBy37+/reLTu/f/nqvnfZ4Ib1Or/XvJj5y9WqNO32/hd/X9v71cxmUB0b+OGcX9ePJuax9N9Epjq7c9HhJp8GlD34O+PCnvhaHUKI7fH4/Mv6rT2bd+VcQzyfujJqN3cBYyP2STKxMd6+zIBmDA6qh/BugakGbbxunUvyra47KaUyyZyFH+jcJZNJoKVHZ228vHfF8aPWkZLj982TI6Lu6twlleba2ekYz+Tu6v3VhO1FnTDh+dvK9Tjai8y/MdHrpkfYOdqHNPLRuRe6HXSGF/XkFFCUo0NJPp7c3Cya1t3rKZHmiO0cDLqH1/ffZiZlcnZ6Bi8H1a+c/KxSswBHVxvw4k6TB2cixi+rTLgKL6f1dBvp/+JaNLEBHp1/2aavD+EwfJ1YmBgt277sZa2wEGK9PDgd0eMz/6Dq5u0hMREez0yNfpa9b1V0mSBP32oexLqAalfc0zft+/tA+zvhNvyeGy+VgE39XCgSVGjoL3bh25IcOpGT//5+LZPKB04OdivPg19kDcu77F0ZE/MiS+woKhvo4RXM9Ve2KJJeZSa+SpZkSn2CHftN9Cc8wXoWmNq7Ijr+VY5czgjthHZioYOrvdhBRAmgEzsvgrIJiF2ITDfKJZ8odvkohqjXmyLsElWKGDRhBNorReUdRVQrSFHKQ7Ui5F9XuYtSrnglFFJyGS3LledmSuQSuSyXZgRMOX+HD7/kjXRYrG2Ju+f3su+HJ799nUvThJbRik9a8wfmryNG2P/zHzZVeCkxjZiqPQL1knVFw+SteaY6nUI3yrMpz6yxKh4Riig7scDb37FaqFu1UM51lOoDrlSPmAT6ykBMAgWEmAQKCDEJFBBiEiggxCRQQIhJ/B8AAP//PlunqwAAAAZJREFUAwB44mxS/qLzoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a381e1fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m task = \u001b[33m\"\u001b[39m\u001b[33mWrite a strategic one-pager of building an AI startup\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# TODO: 解决执行时间过久的问题\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m graph.ainvoke(\n\u001b[32m      5\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: task}, config={\u001b[33m\"\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mmax_paths\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m}}\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3182\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3179\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3180\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3182\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3183\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3184\u001b[39m     config,\n\u001b[32m   3185\u001b[39m     context=context,\n\u001b[32m   3186\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3188\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3189\u001b[39m     print_mode=print_mode,\n\u001b[32m   3190\u001b[39m     output_keys=output_keys,\n\u001b[32m   3191\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3192\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3193\u001b[39m     durability=durability,\n\u001b[32m   3194\u001b[39m     **kwargs,\n\u001b[32m   3195\u001b[39m ):\n\u001b[32m   3196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3197\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36m_run_node\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Command(goto=\u001b[33m\"\u001b[39m\u001b[33mvote\u001b[39m\u001b[33m\"\u001b[39m, update={})\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m step = \u001b[38;5;28;01mawait\u001b[39;00m execution_agent.ainvoke(\n\u001b[32m     30\u001b[39m     {\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprevious_steps\u001b[39m\u001b[33m\"\u001b[39m: node.get_full_plan(),\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m: node.step,\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     34\u001b[39m     }\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m node.step_output = step[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content\n\u001b[32m     37\u001b[39m visited_ids.add(node.node_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3130\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3129\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3130\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3131\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3182\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3179\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3180\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3182\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3183\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3184\u001b[39m     config,\n\u001b[32m   3185\u001b[39m     context=context,\n\u001b[32m   3186\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3188\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3189\u001b[39m     print_mode=print_mode,\n\u001b[32m   3190\u001b[39m     output_keys=output_keys,\n\u001b[32m   3191\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3192\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3193\u001b[39m     durability=durability,\n\u001b[32m   3194\u001b[39m     **kwargs,\n\u001b[32m   3195\u001b[39m ):\n\u001b[32m   3196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3197\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1118\u001b[39m, in \u001b[36mcreate_agent.<locals>.amodel_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1105\u001b[39m request = ModelRequest(\n\u001b[32m   1106\u001b[39m     model=model,\n\u001b[32m   1107\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1113\u001b[39m     runtime=runtime,\n\u001b[32m   1114\u001b[39m )\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m awrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1117\u001b[39m     \u001b[38;5;66;03m# No async handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m _execute_model_async(request)\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;66;03m# Call composed async handler with base handler\u001b[39;00m\n\u001b[32m   1121\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m awrap_model_call_handler(request, _execute_model_async)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1091\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_async\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_prompt:\n\u001b[32m   1089\u001b[39m     messages = [SystemMessage(request.system_prompt), *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m model_.ainvoke(messages)\n\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n\u001b[32m   1094\u001b[39m handled_output = _handle_model_output(output, effective_response_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5502\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5495\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5496\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5497\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5500\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5501\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5503\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5504\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5505\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5506\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    394\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m     **kwargs: Any,\n\u001b[32m    400\u001b[39m ) -> AIMessage:\n\u001b[32m    401\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    403\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    404\u001b[39m         stop=stop,\n\u001b[32m    405\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    407\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    408\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    409\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    410\u001b[39m         **kwargs,\n\u001b[32m    411\u001b[39m     )\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    413\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    414\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1099\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1092\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1096\u001b[39m     **kwargs: Any,\n\u001b[32m   1097\u001b[39m ) -> LLMResult:\n\u001b[32m   1098\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1100\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1101\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1006\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   1008\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1013\u001b[39m     run_id=run_id,\n\u001b[32m   1014\u001b[39m )\n\u001b[32m   1016\u001b[39m input_messages = [\n\u001b[32m   1017\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m   1018\u001b[39m ]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1020\u001b[39m     *[\n\u001b[32m   1021\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m   1022\u001b[39m             m,\n\u001b[32m   1023\u001b[39m             stop=stop,\n\u001b[32m   1024\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1025\u001b[39m             **kwargs,\n\u001b[32m   1026\u001b[39m         )\n\u001b[32m   1027\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m   1028\u001b[39m     ],\n\u001b[32m   1029\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1030\u001b[39m )\n\u001b[32m   1031\u001b[39m exceptions = []\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1310\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1308\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1311\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1539\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1532\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1533\u001b[39m             response,\n\u001b[32m   1534\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1535\u001b[39m             metadata=generation_info,\n\u001b[32m   1536\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1537\u001b[39m         )\n\u001b[32m   1538\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1539\u001b[39m         raw_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.with_raw_response.create(\n\u001b[32m   1540\u001b[39m             **payload\n\u001b[32m   1541\u001b[39m         )\n\u001b[32m   1542\u001b[39m         response = raw_response.parse()\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2603\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2557\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2559\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2600\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2601\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2602\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2604\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2605\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2606\u001b[39m             {\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2621\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2622\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2623\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2625\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2626\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2627\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2628\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2629\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2630\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2631\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2632\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2633\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2634\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2635\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2636\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2637\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2638\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2639\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2640\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2641\u001b[39m             },\n\u001b[32m   2642\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2643\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2644\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2645\u001b[39m         ),\n\u001b[32m   2646\u001b[39m         options=make_request_options(\n\u001b[32m   2647\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2648\u001b[39m         ),\n\u001b[32m   2649\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2650\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2651\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2652\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/openai/_base_client.py:1529\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1527\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1530\u001b[39m         request,\n\u001b[32m   1531\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1532\u001b[39m         **kwargs,\n\u001b[32m   1533\u001b[39m     )\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1535\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:237\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:180\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1263\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1258\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1261\u001b[39m ):\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "task = \"Write a strategic one-pager of building an AI startup\"\n",
    "\n",
    "# TODO: 解决执行时间过久的问题\n",
    "result = await graph.ainvoke(\n",
    "    {\"task\": task}, config={\"recursion_limit\": 10, \"configurable\": {\"max_paths\": 3}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a504989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result[\"candidates\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be400b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"best_candidate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a36cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_plan': {'root': Node_id: 1, parent: None, 1 children., 'queue': deque([Node_id: 1, parent: None, 1 children., Node_id: 2, parent: 1, 1 children., Node_id: 3, parent: 2, 1 children., Node_id: 4, parent: 3, 1 children., Node_id: 5, parent: 4, 1 children., Node_id: 6, parent: 5, 1 children., Node_id: 7, parent: 6, 1 children., Node_id: 8, parent: 7, 1 children., Node_id: 9, parent: 8, 1 children., Node_id: 10, parent: 9, 0 children.]), 'max_id': 10}}\n",
      "{'run': {'current_node': Node_id: 1, parent: None, 1 children., 'queue': deque([Node_id: 2, parent: 1, 1 children., Node_id: 3, parent: 2, 1 children., Node_id: 4, parent: 3, 1 children., Node_id: 5, parent: 4, 1 children., Node_id: 6, parent: 5, 1 children., Node_id: 7, parent: 6, 1 children., Node_id: 8, parent: 7, 1 children., Node_id: 9, parent: 8, 1 children., Node_id: 10, parent: 9, 0 children.]), 'visited_ids': {1}, 'next_node': None}}\n",
      "{'plan_next': {'is_current_node_final': False, 'next_node': Node_id: 11, parent: 1, 0 children., 'max_id': 11}}\n",
      "{'run': {'current_node': Node_id: 11, parent: 1, 0 children., 'queue': deque([Node_id: 2, parent: 1, 1 children., Node_id: 3, parent: 2, 1 children., Node_id: 4, parent: 3, 1 children., Node_id: 5, parent: 4, 1 children., Node_id: 6, parent: 5, 1 children., Node_id: 7, parent: 6, 1 children., Node_id: 8, parent: 7, 1 children., Node_id: 9, parent: 8, 1 children., Node_id: 10, parent: 9, 0 children., Node_id: 11, parent: 1, 0 children.]), 'visited_ids': {1, 11}, 'next_node': None}}\n",
      "{'plan_next': {'is_current_node_final': False, 'next_node': Node_id: 12, parent: 11, 0 children., 'max_id': 12}}\n",
      "{'run': {'current_node': Node_id: 12, parent: 11, 0 children., 'queue': deque([Node_id: 2, parent: 1, 1 children., Node_id: 3, parent: 2, 1 children., Node_id: 4, parent: 3, 1 children., Node_id: 5, parent: 4, 1 children., Node_id: 6, parent: 5, 1 children., Node_id: 7, parent: 6, 1 children., Node_id: 8, parent: 7, 1 children., Node_id: 9, parent: 8, 1 children., Node_id: 10, parent: 9, 0 children., Node_id: 11, parent: 1, 1 children., Node_id: 12, parent: 11, 0 children.]), 'visited_ids': {1, 11, 12}, 'next_node': None}}\n",
      "{'plan_next': {'is_current_node_final': False, 'next_node': Node_id: 13, parent: 12, 0 children., 'max_id': 13}}\n",
      "{'run': {'current_node': Node_id: 13, parent: 12, 0 children., 'queue': deque([Node_id: 2, parent: 1, 1 children., Node_id: 3, parent: 2, 1 children., Node_id: 4, parent: 3, 1 children., Node_id: 5, parent: 4, 1 children., Node_id: 6, parent: 5, 1 children., Node_id: 7, parent: 6, 1 children., Node_id: 8, parent: 7, 1 children., Node_id: 9, parent: 8, 1 children., Node_id: 10, parent: 9, 0 children., Node_id: 11, parent: 1, 1 children., Node_id: 12, parent: 11, 1 children., Node_id: 13, parent: 12, 0 children.]), 'visited_ids': {1, 11, 12, 13}, 'next_node': None}}\n",
      "{'plan_next': {'is_current_node_final': False, 'next_node': Node_id: 14, parent: 13, 0 children., 'max_id': 14}}\n",
      "{'run': {'current_node': Node_id: 14, parent: 13, 0 children., 'queue': deque([Node_id: 2, parent: 1, 1 children., Node_id: 3, parent: 2, 1 children., Node_id: 4, parent: 3, 1 children., Node_id: 5, parent: 4, 1 children., Node_id: 6, parent: 5, 1 children., Node_id: 7, parent: 6, 1 children., Node_id: 8, parent: 7, 1 children., Node_id: 9, parent: 8, 1 children., Node_id: 10, parent: 9, 0 children., Node_id: 11, parent: 1, 1 children., Node_id: 12, parent: 11, 1 children., Node_id: 13, parent: 12, 1 children., Node_id: 14, parent: 13, 0 children.]), 'visited_ids': {1, 11, 12, 13, 14}, 'next_node': None}}\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 10 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 监控执行过程的脚本\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m graph.astream(\n\u001b[32m      3\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m: task}, config={\u001b[33m\"\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mmax_paths\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m}}\n\u001b[32m      4\u001b[39m ):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3036\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3028\u001b[39m     msg = create_error_message(\n\u001b[32m   3029\u001b[39m         message=(\n\u001b[32m   3030\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3034\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   3035\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3036\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   3037\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 10 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "# 监控执行过程的脚本\n",
    "async for e in graph.astream(\n",
    "    {\"task\": task}, config={\"recursion_limit\": 10, \"configurable\": {\"max_paths\": 3}}\n",
    "):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b24fa1",
   "metadata": {},
   "source": [
    "### Trimming ToT with MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729e236",
   "metadata": {},
   "source": [
    "## Agent memory\n",
    "\n",
    "Long-term memory helps an agent to accumulate knowledge and gain from historical\n",
    "experiences, and enables its continuous improvement on the long horizon.\n",
    "\n",
    "Key considerations to take into account when designing and using long-term memory in practice\n",
    "1. Extract useful information that you want to store during the runtime.\n",
    "1. Extract stored information during the next execution.\n",
    "1. Compact memory – periodically self-reflect on what you have learned, optimize it, and forget irrelevant facts.\n",
    "\n",
    "Implementations of long-term memory\n",
    "1. A built-in cache (a mechanism to cache LLMs responses)\n",
    "1. A built-in store (a persistent key-value store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a8d15",
   "metadata": {},
   "source": [
    "### Cache\n",
    "\n",
    "It caches responses based on the key that consists of a string representation of the prompt and the string\n",
    "representation of the LLM instance (produced by the llm._get_llm_string method).\n",
    "\n",
    "LangChain supports in-memory and SQLite caches out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1213053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of the United Kingdom is London.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-40a2fc8a-6b5a-45af-a209-d69896f78340', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--e0bd6ff5-6fc8-4d78-8617-a5ba9a6a2e03-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "\n",
    "cache = InMemoryCache()\n",
    "set_llm_cache(cache)\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "llm.invoke(\"What is the capital of UK?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da6fb483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_core.caches.InMemoryCache object at 0x7f6b567832f0>\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic import globals\n",
    "\n",
    "\n",
    "print(globals.get_llm_cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17dfa968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"model_name\": \"qwen3-max-2025-09-23\", \"openai_api_base\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\", \"openai_api_key\": {\"id\": [\"OPENAI_API_KEY\"], \"lc\": 1, \"type\": \"secret\"}}, \"lc\": 1, \"name\": \"ChatOpenAI\", \"type\": \"constructor\"}---[(\\'stop\\', None)]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._get_llm_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f4d20",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6eb46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "in_memory_store.put(\n",
    "    namespace=(\"users\", \"user1\"), key=\"fact1\", value={\"message1\": \"My name is John.\"}\n",
    ")\n",
    "in_memory_store.put(\n",
    "    namespace=(\"users\", \"user1\", \"conv1\"),\n",
    "    key=\"address\",\n",
    "    value={\"message\": \"I live in Berlin.\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3dc802b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(namespace=['users', 'user1', 'conv1'], key='address', value={'message': 'I live in Berlin.'}, created_at='2025-11-01T11:13:24.820598+00:00', updated_at='2025-11-01T11:13:24.820600+00:00')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_memory_store.get(namespace=(\"users\", \"user1\", \"conv1\"), key=\"address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92118d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store.get(namespace=(\"users\", \"user1\"), key=\"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c89598f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['users', 'user1', 'conv1'], key='address', value={'message': 'I live in Berlin.'}, created_at='2025-11-01T11:13:24.820598+00:00', updated_at='2025-11-01T11:13:24.820600+00:00', score=None)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_memory_store.search((\"users\", \"user1\", \"conv1\"), query=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77d56c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['users', 'user1'], key='fact1', value={'message1': 'My name is John.'}, created_at='2025-11-01T11:13:24.820380+00:00', updated_at='2025-11-01T11:13:24.820434+00:00', score=None),\n",
       " Item(namespace=['users', 'user1', 'conv1'], key='address', value={'message': 'I live in Berlin.'}, created_at='2025-11-01T11:13:24.820598+00:00', updated_at='2025-11-01T11:13:24.820600+00:00', score=None)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_memory_store.search((\"users\", \"user1\"), query=\"name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-with-lang-chain-2ed (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
