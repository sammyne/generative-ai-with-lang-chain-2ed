{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a726ef33",
   "metadata": {},
   "source": [
    "# 08. Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d9e56",
   "metadata": {},
   "source": [
    "## ÂÆâË£Ö‰æùËµñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bcc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=1.0 langchain-core~=1.0 langgraph~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c3ed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-classic~=1.0 langchain-community~=0.4 langchain-openai~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7d1126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c2342",
   "metadata": {},
   "source": [
    "Â∑•ÂÖ∑Á±ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1c2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import langsmith\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "\n",
    "        self.langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # ÂèÇËÄÉÔºöhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # ÂèÇËÄÉÔºöhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI ÊñáÊ°£ÂèÇËÄÉÔºöhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_langsmith_client(self, **kwargs) -> langsmith.Client:\n",
    "        if not self.langsmith_api_key:\n",
    "            raise ValueError(\"LANGSMITH_API_KEY is not set\")\n",
    "\n",
    "        return langsmith.Client(api_key=self.langsmith_api_key, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8fee8",
   "metadata": {},
   "source": [
    "## Why evaluation matters\n",
    "### Safety and alignment\n",
    "### Performance and efficiency\n",
    "### User and stakeholder value\n",
    "### Building consensus for LLM evaluation\n",
    "## What we evaluate: core agent capabilities\n",
    "### Task performance evaluation\n",
    "### Tool usage evaluation\n",
    "### RAG evaluation\n",
    "### Planning and reasoning evaluation\n",
    "## How we evaluate: methodologies and approaches\n",
    "### Automated evaluation approaches\n",
    "### Human-in-the-loop evaluation\n",
    "### System-level evaluation\n",
    "## Evaluating LLM agents in practice\n",
    "### Evaluating the correctness of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2971f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match result (correct answer): {'score': 1}\n",
      "Exact match result (incorrect answer): {'score': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import ExactMatchStringEvaluator\n",
    "\n",
    "prompt = \"What is the current Federal Reserve interest rate?\"\n",
    "reference_answer = \"0.25%\"  # Suppose this is the correct answer.\n",
    "\n",
    "# Example predictions:\n",
    "prediction_correct = \"0.25%\"\n",
    "prediction_incorrect = \"0.50%\"\n",
    "\n",
    "# Initialize an Exact Match evaluator that ignores case differences.\n",
    "exact_evaluator = ExactMatchStringEvaluator(ignore_case=True)\n",
    "\n",
    "# Evaluate the correct prediction.\n",
    "exact_result_correct = exact_evaluator.evaluate_strings(\n",
    "    prediction=prediction_correct, reference=reference_answer\n",
    ")\n",
    "print(\"Exact match result (correct answer):\", exact_result_correct)\n",
    "\n",
    "# Evaluate an incorrect prediction.\n",
    "exact_result_incorrect = exact_evaluator.evaluate_strings(\n",
    "    prediction=prediction_incorrect, reference=reference_answer\n",
    ")\n",
    "print(\"Exact match result (incorrect answer):\", exact_result_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fe800e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance Evaluation Result:\n",
      "{'reasoning': \"The assistant's response states that the current Federal Reserve interest rate is 0.25%. However, this figure is outdated and factually incorrect as of 2024. The Federal Reserve adjusts its target federal funds rate regularly in response to economic conditions, and as of mid-2024, the rate has been significantly higher‚Äîranging between 5.25% and 5.50%‚Äîas part of efforts to combat inflation. Because the answer provides an inaccurate value without context or a date reference, it fails on correctness. It also lacks helpfulness and depth, offering no explanation of what the rate represents or how to find the most up-to-date information. While the response is relevant to the question in intent, its factual inaccuracy severely undermines its quality.\\n\\nRating: [[2]]\", 'score': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation.scoring import ScoreStringEvalChain\n",
    "\n",
    "# Initialize the evaluator LLM\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Create the ScoreStringEvalChain from the LLM\n",
    "chain = ScoreStringEvalChain.from_llm(llm=llm)\n",
    "\n",
    "# Define the finance-related input, prediction, and reference answer\n",
    "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
    "finance_prediction = \"The current interest rate is 0.25%.\"\n",
    "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
    "\n",
    "# Evaluate the prediction using the scoring chain\n",
    "result_finance = chain.evaluate_strings(\n",
    "    input=finance_input,\n",
    "    prediction=finance_prediction,\n",
    ")\n",
    "\n",
    "print(\"Finance Evaluation Result:\")\n",
    "print(result_finance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9261db0",
   "metadata": {},
   "source": [
    "Âü∫‰∫éÂèÇËÄÉÁ≠îÊ°àÁöÑËØÑ‰º∞ÔºàÈÄÇÂêàÂ∑≤ÊúâÊòéÁ°Æ„ÄÅÁúüÂÆûÁ≠îÊ°àÁöÑÂú∫ÊôØÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd63dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance Evaluation Result (with reference):\n",
      "{'reasoning': \"The assistant's response directly states that the current Federal Reserve interest rate is 0.25%, which aligns exactly with the provided ground truth. The answer is concise, accurate, and relevant to the user's question. While it lacks elaboration or context (such as the date of the rate or potential implications), the question only asks for the rate itself, so additional detail is not strictly necessary. The response meets the criteria of correctness, relevance, and helpfulness for the given query. Depth is minimal but appropriate given the straightforward nature of the question.\\n\\nRating: [[10]]\", 'score': 10}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation.scoring import LabeledScoreStringEvalChain\n",
    "\n",
    "# Initialize the evaluator LLM\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Create the evaluation chain that can use reference answers\n",
    "labeled_chain = LabeledScoreStringEvalChain.from_llm(llm=llm)\n",
    "\n",
    "# Define the finance-related input, prediction, and reference answer\n",
    "finance_input = \"What is the current Federal Reserve interest rate?\"\n",
    "finance_prediction = \"The current interest rate is 0.25%.\"\n",
    "finance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n",
    "\n",
    "# Evaluate the prediction against the reference\n",
    "labeled_result = labeled_chain.evaluate_strings(\n",
    "    input=finance_input,\n",
    "    prediction=finance_prediction,\n",
    "    reference=finance_reference,\n",
    ")\n",
    "\n",
    "print(\"Finance Evaluation Result (with reference):\")\n",
    "print(labeled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2fdff",
   "metadata": {},
   "source": [
    "### Evaluating tone and conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e133b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm = Config().new_openai_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f44e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_health = \"What is a healthy blood pressure range for adults?\"\n",
    "\n",
    "# A sample LLM output from your healthcare assistant:\n",
    "prediction_health = (\n",
    "    \"A normal blood pressure reading is typically around 120/80 mmHg. \"\n",
    "    \"It's important to follow your doctor's advice for personal health management!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb2f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conciseness evaluation result: {'reasoning': 'To evaluate whether the submission meets the \"conciseness\" criterion, we need to assess if it is brief and directly addresses the question without unnecessary elaboration.\\n\\nStep 1: Identify the core information requested.  \\nThe input asks for a healthy blood pressure range for adults. The ideal answer should state the typical healthy range (e.g., less than 120/80 mmHg) clearly and succinctly.\\n\\nStep 2: Examine the submission content.  \\nThe submission states: \"A normal blood pressure reading is typically around 120/80 mmHg. It\\'s important to follow your doctor\\'s advice for personal health management!\"\\n\\nStep 3: Assess conciseness.  \\nThe first sentence directly answers the question and is concise. However, the second sentence adds general advice that, while well-intentioned, is not required to answer the specific question about the healthy range. This extra sentence makes the response slightly less concise than it could be.\\n\\nStep 4: Determine if the extra information violates conciseness.  \\nConciseness does not necessarily forbid any additional context, but the criterion emphasizes being \"to the point.\" The second sentence is tangential and not essential to answering the factual query. Therefore, the submission is not maximally concise.\\n\\nConclusion: The inclusion of non-essential advice reduces conciseness, so the submission does not fully meet the conciseness criterion.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import EvaluatorType, load_evaluator\n",
    "\n",
    "conciseness_evaluator = load_evaluator(\n",
    "    EvaluatorType.CRITERIA, criteria=\"conciseness\", llm=evaluation_llm\n",
    ")\n",
    "conciseness_result = conciseness_evaluator.evaluate_strings(\n",
    "    prediction=prediction_health, input=prompt_health\n",
    ")\n",
    "print(\"Conciseness evaluation result:\", conciseness_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5fc22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friendliness evaluation result: {'reasoning': 'To assess whether the submission meets the \"friendliness\" criterion, we need to evaluate if the tone is friendly and approachable.\\n\\nStep 1: Examine the language used. The submission states: \"A normal blood pressure reading is typically around 120/80 mmHg.\" This is factual and neutral, but not unfriendly.\\n\\nStep 2: Look at the second sentence: \"It\\'s important to follow your doctor\\'s advice for personal health management!\" This uses an exclamation mark, which often conveys warmth or encouragement. The phrase \"your doctor\\'s advice\" is personalized (\"your\"), making it feel more conversational and caring.\\n\\nStep 3: Consider overall tone. The response avoids clinical coldness by adding a supportive, advisory note that encourages the reader to consult a professional, framed in a positive and considerate way.\\n\\nStep 4: Determine if this qualifies as \"friendly and approachable.\" Yes‚Äîthe use of inclusive language (\"your\"), the exclamation point, and the supportive closing remark all contribute to a tone that is warm, helpful, and accessible to a general audience.\\n\\nTherefore, the submission does meet the friendliness criterion.\\n\\nY', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate friendliness with custom criterion\n",
    "custom_friendliness = {\n",
    "    \"friendliness\": \"Is the response written in a friendly and approachable tone?\"\n",
    "}\n",
    "friendliness_evaluator = load_evaluator(\n",
    "    EvaluatorType.CRITERIA, criteria=custom_friendliness, llm=evaluation_llm\n",
    ")\n",
    "friendliness_result = friendliness_evaluator.evaluate_strings(\n",
    "    prediction=prediction_health, input=prompt_health\n",
    ")\n",
    "print(\"Friendliness evaluation result:\", friendliness_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e8a77",
   "metadata": {},
   "source": [
    "### Evaluating the output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4fd163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON validity result (valid): {'score': 1}\n",
      "JSON validity result (invalid): {'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 63 (char 62)'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import JsonValidityEvaluator\n",
    "\n",
    "# Initialize the JSON validity evaluator.\n",
    "json_validator = JsonValidityEvaluator()\n",
    "\n",
    "valid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000}'\n",
    "invalid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000,}'\n",
    "\n",
    "# Evaluate the valid JSON.\n",
    "valid_result = json_validator.evaluate_strings(prediction=valid_json_output)\n",
    "print(\"JSON validity result (valid):\", valid_result)\n",
    "\n",
    "# Evaluate the invalid JSON.\n",
    "invalid_result = json_validator.evaluate_strings(prediction=invalid_json_output)\n",
    "print(\"JSON validity result (invalid):\", invalid_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56682bb9",
   "metadata": {},
   "source": [
    "### Evaluating agent trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ec95a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langsmith~=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9990ff79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'langsmith.schemas.Example'>(id=d55aff8b-f268-48ce-8a34-0965e7e03f23, dataset_id=db679684-73d5-461c-a354-d91d8b10415c, link='https://smith.langchain.com/o/87dee905-f3ea-4b4d-b51e-bfa1e3588b48/datasets/db679684-73d5-461c-a354-d91d8b10415c/e/d55aff8b-f268-48ce-8a34-0965e7e03f23')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:\n",
    "    \"\"\"Check how many of the desired steps the agent took.\"\"\"\n",
    "    if len(reference_outputs[\"trajectory\"]) > len(outputs[\"trajectory\"]):\n",
    "        return 0.0\n",
    "\n",
    "    i = j = 0\n",
    "    while i < len(reference_outputs[\"trajectory\"]) and j < len(outputs[\"trajectory\"]):\n",
    "        if reference_outputs[\"trajectory\"][i] == outputs[\"trajectory\"][j]:\n",
    "            i += 1\n",
    "        j += 1\n",
    "\n",
    "    return i / len(reference_outputs[\"trajectory\"])\n",
    "\n",
    "\n",
    "# Create example dataset with expected trajectories\n",
    "client = Config().new_langsmith_client()\n",
    "trajectory_dataset = client.create_dataset(\n",
    "    \"Healthcare Agent Trajectory Evaluation\",\n",
    "    description=\"Evaluates agent trajectory for medication queries\",\n",
    ")\n",
    "\n",
    "# Add example with expected trajectory\n",
    "client.create_example(\n",
    "    inputs={\"question\": \"What is the recommended dosage of ibuprofen for an adult?\"},\n",
    "    outputs={\n",
    "        \"trajectory\": [\n",
    "            \"intent_classifier\",\n",
    "            \"healthcare_agent\",\n",
    "            \"MedicalDatabaseSearch\",\n",
    "            \"format_response\",\n",
    "        ],\n",
    "        \"response\": \"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day.\",\n",
    "    },\n",
    "    dataset_id=trajectory_dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c86437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'healthcare-agent-trajectory-aee10872' at:\n",
      "https://smith.langchain.com/o/87dee905-f3ea-4b4d-b51e-bfa1e3588b48/datasets/db679684-73d5-461c-a354-d91d8b10415c/compare?selectedSessions=314ed5d3-50f7-47f8-9305-96d264199f35\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to run graph with trajectory tracking (example implementation)\n",
    "async def run_graph_with_trajectory(inputs: dict) -> dict:\n",
    "    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n",
    "    trajectory = []\n",
    "    final_response = \"\"\n",
    "    # Here you would implement your actual graph execution\n",
    "    # For the example, we'll just return a sample result\n",
    "    trajectory = [\n",
    "        \"intent_classifier\",\n",
    "        \"healthcare_agent\",\n",
    "        \"MedicalDatabaseSearch\",\n",
    "        \"format_response\",\n",
    "    ]\n",
    "    final_response = (\n",
    "        \"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day.\"\n",
    "    )\n",
    "    return {\"trajectory\": trajectory, \"response\": final_response}\n",
    "\n",
    "\n",
    "# Note: This is an async function, so in a notebook you'd need to use await\n",
    "experiment_results = await client.aevaluate(\n",
    "    run_graph_with_trajectory,\n",
    "    data=trajectory_dataset.id,\n",
    "    evaluators=[trajectory_subsequence],\n",
    "    experiment_prefix=\"healthcare-agent-trajectory\",\n",
    "    num_repetitions=1,\n",
    "    max_concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b0501d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/3] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 107ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install pandas~=2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24926e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average trajectory match score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# For demonstration without async:\n",
    "results_df = experiment_results.to_pandas()\n",
    "print(\n",
    "    f\"Average trajectory match score: {results_df['feedback.trajectory_subsequence'].mean()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde8de6",
   "metadata": {},
   "source": [
    "### Evaluating CoT reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1f46cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chain-of-Thought Reasoning Evaluation:\n",
      "{'reasoning': 'EXPLANATION:  \\nStep 1: Check whether the student correctly states the current Federal Reserve interest rate according to the context. The context specifies that the rate is ‚Äúaround 0.25%,‚Äù and the student states it as ‚Äú0.25%.‚Äù This aligns with the provided context.\\n\\nStep 2: Evaluate whether the student explains why the rate matters, as required by the question and supported by the context. The context indicates the low rate is maintained ‚Äúto support economic growth‚Äù and should mention implications for ‚Äúborrowing costs and consumer spending.‚Äù\\n\\nStep 3: The student explains that the low rate ‚Äúaims to stimulate economic growth by keeping borrowing costs low,‚Äù which directly corresponds to the context‚Äôs point about supporting economic growth and lowering borrowing costs.\\n\\nStep 4: The student also notes that this ‚Äúencourages consumer spending and business investment,‚Äù which aligns with the context‚Äôs mention of implications for consumer spending (and reasonably extends to business investment, which is consistent with economic principles and does not contradict the context).\\n\\nStep 5: There are no factual inaccuracies in the student‚Äôs answer relative to the given context. The additional mention of business investment does not conflict with the context and is a reasonable elaboration.\\n\\nGRADE: CORRECT', 'value': 'CORRECT', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "# Simulated chain-of-thought reasoning provided by the agent:\n",
    "agent_reasoning = (\n",
    "    \"The current interest rate is 0.25%. I determined this by recalling that recent monetary policies have aimed \"\n",
    "    \"to stimulate economic growth by keeping borrowing costs low. A rate of 0.25% is consistent with the ongoing \"\n",
    "    \"trend of low rates, which encourages consumer spending and business investment.\"\n",
    ")\n",
    "\n",
    "# Expected reasoning reference:\n",
    "expected_reasoning = (\n",
    "    \"An ideal reasoning should mention that the Federal Reserve has maintained a low interest rate‚Äîaround 0.25%‚Äîto \"\n",
    "    \"support economic growth, and it should briefly explain the implications for borrowing costs and consumer spending.\"\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Load the chain-of-thought evaluator.\n",
    "cot_evaluator = load_evaluator(EvaluatorType.COT_QA, llm=llm)\n",
    "\n",
    "result_reasoning = cot_evaluator.evaluate_strings(\n",
    "    input=\"What is the current Federal Reserve interest rate and why does it matter?\",\n",
    "    prediction=agent_reasoning,\n",
    "    reference=expected_reasoning,\n",
    ")\n",
    "\n",
    "print(\"\\nChain-of-Thought Reasoning Evaluation:\")\n",
    "print(result_reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b5bcd",
   "metadata": {},
   "source": [
    "## Offline evaluation\n",
    "### Evaluating RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e56fbef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 2 examples\n"
     ]
    }
   ],
   "source": [
    "client = Config().new_langsmith_client()\n",
    "\n",
    "# Sample financial examples\n",
    "financial_examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"What are the tax implications of early 401(k) withdrawal?\",\n",
    "            \"context_needed\": [\"retirement\", \"taxation\", \"penalties\"],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"Early withdrawals from a 401(k) typically incur a 10% penalty if you're under 59¬Ω years old, in addition to regular income taxes. However, certain hardship withdrawals may qualify for penalty exemptions.\",\n",
    "            \"key_points\": [\"10% penalty\", \"income tax\", \"hardship exemptions\"],\n",
    "            \"documents\": [\"IRS publication 575\", \"Retirement plan guidelines\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"How does dollar-cost averaging compare to lump-sum investing?\",\n",
    "            \"context_needed\": [\n",
    "                \"investment strategy\",\n",
    "                \"risk management\",\n",
    "                \"market timing\",\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"Dollar-cost averaging spreads investments over time to reduce timing risk, while lump-sum investing typically outperforms in rising markets due to longer market exposure. DCA may provide psychological benefits through reduced volatility exposure.\",\n",
    "            \"key_points\": [\"timing risk\", \"market exposure\", \"psychological benefits\"],\n",
    "            \"documents\": [\"Investment strategy comparisons\", \"Market timing research\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create dataset in LangSmith\n",
    "dataset_name = \"Financial Advisory RAG Evaluation\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Evaluation dataset for financial advisory RAG systems covering retirement, investments, and tax planning.\",\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "for example in financial_examples:\n",
    "    client.create_example(\n",
    "        inputs=example[\"inputs\"], outputs=example[\"outputs\"], dataset_id=dataset.id\n",
    "    )\n",
    "print(f\"Created evaluation dataset with {len(financial_examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02732091",
   "metadata": {},
   "source": [
    "### Evaluating a benchmark in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a4ad999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration for LangSmith:\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"My Project\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = \"lsv2_pt_xxxxxxx\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef0b314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: Hello! How can I help you today? üòä\n",
      "\n",
      "This run has been logged to LangSmith.\n",
      "You can view it in the LangSmith UI: https://smith.langchain.com\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ê≤°Âú® langsmith ÁúãÂà∞ËøΩË∏™‰ø°ÊÅØ\n",
    "\n",
    "# Create a simple LLM call that will be traced in LangSmith\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(f\"Model response: {response.content}\")\n",
    "print(\"\\nThis run has been logged to LangSmith.\")\n",
    "print(\"You can view it in the LangSmith UI: https://smith.langchain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd87f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation configuration\n",
    "from langchain_classic.smith import RunEvalConfig\n",
    "\n",
    "# Define evaluation criteria specific to RAG systems\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        {\n",
    "            \"criteria\": {\n",
    "                \"factual_accuracy\": \"Does the response contain only factually correct information consistent with the reference answer?\"\n",
    "            },\n",
    "            \"evaluator_type\": \"criteria\",\n",
    "        },\n",
    "        {\n",
    "            \"criteria\": {\n",
    "                \"groundedness\": \"Is the response fully supported by the retrieved documents without introducing unsupported information?\"\n",
    "            },\n",
    "            \"evaluator_type\": \"criteria\",\n",
    "        },\n",
    "        {\n",
    "            \"criteria\": {\n",
    "                \"retrieval_relevance\": \"Are the retrieved documents relevant to answering the question?\"\n",
    "            },\n",
    "            \"evaluator_type\": \"criteria\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55062efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ë°•ÂÖÖÂÆûÁé∞\n",
    "def construct_chain(input, **kwargs):\n",
    "    # This would be your actual RAG implementation\n",
    "    # For example: return RAGChain(...)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f8ab19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'formal-bill-74' at:\n",
      "https://smith.langchain.com/o/87dee905-f3ea-4b4d-b51e-bfa1e3588b48/datasets/a27a5894-a32a-4bd6-8b1c-bac2e87e1bcf/compare?selectedSessions=72eb9db1-5999-4867-8de4-37b76e4b054e\n",
      "\n",
      "View all tests for Dataset Financial Advisory RAG Evaluation at:\n",
      "https://smith.langchain.com/o/87dee905-f3ea-4b4d-b51e-bfa1e3588b48/datasets/a27a5894-a32a-4bd6-8b1c-bac2e87e1bcf\n",
      "[>                                                 ] 0/2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run 45b09524-2365-4ca0-aac2-ea0fd82aa2f7\n",
      "Traceback (most recent call last):\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 374, in evaluate_run\n",
      "    result = self({\"run\": run, \"example\": example}, include_run_info=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 188, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 413, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 167, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 333, in _call\n",
      "    evaluate_strings_inputs = self._prepare_input(inputs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 301, in _prepare_input\n",
      "    evaluate_strings_inputs = self.run_mapper(run)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 55, in __call__\n",
      "    return self.map(run)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 206, in map\n",
      "    input_ = self._get_key(run.inputs, self.input_key, \"input\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 177, in _get_key\n",
      "    raise ValueError(msg)\n",
      "ValueError: Could not map run input with multiple keys: {'question': 'How does dollar-cost averaging compare to lump-sum investing?', 'context_needed': ['investment strategy', 'risk management', 'market timing']}\n",
      "Please manually specify a input_key\n",
      "Error evaluating run 4db3d1f9-e1a7-4983-ae01-3979fea1effb\n",
      "Traceback (most recent call last):\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 374, in evaluate_run\n",
      "    result = self({\"run\": run, \"example\": example}, include_run_info=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 188, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 413, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 167, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 333, in _call\n",
      "    evaluate_strings_inputs = self._prepare_input(inputs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 301, in _prepare_input\n",
      "    evaluate_strings_inputs = self.run_mapper(run)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 55, in __call__\n",
      "    return self.map(run)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 206, in map\n",
      "    input_ = self._get_key(run.inputs, self.input_key, \"input\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 177, in _get_key\n",
      "    raise ValueError(msg)\n",
      "ValueError: Could not map run input with multiple keys: {'question': 'What are the tax implications of early 401(k) withdrawal?', 'context_needed': ['retirement', 'taxation', 'penalties']}\n",
      "Please manually specify a input_key\n",
      "Error evaluating run 45b09524-2365-4ca0-aac2-ea0fd82aa2f7\n",
      "Traceback (most recent call last):\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 374, in evaluate_run\n",
      "    result = self({\"run\": run, \"example\": example}, include_run_info=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 188, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 413, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 167, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 333, in _call\n",
      "    evaluate_strings_inputs = self._prepare_input(inputs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 301, in _prepare_input\n",
      "    evaluate_strings_inputs = self.run_mapper(run)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 55, in __call__\n",
      "    return self.map(run)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 206, in map\n",
      "    input_ = self._get_key(run.inputs, self.input_key, \"input\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 177, in _get_key\n",
      "    raise ValueError(msg)\n",
      "ValueError: Could not map run input with multiple keys: {'question': 'How does dollar-cost averaging compare to lump-sum investing?', 'context_needed': ['investment strategy', 'risk management', 'market timing']}\n",
      "Please manually specify a input_key\n",
      "Error evaluating run 4db3d1f9-e1a7-4983-ae01-3979fea1effb\n",
      "Traceback (most recent call last):\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 374, in evaluate_run\n",
      "    result = self({\"run\": run, \"example\": example}, include_run_info=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 188, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 413, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 167, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 333, in _call\n",
      "    evaluate_strings_inputs = self._prepare_input(inputs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 301, in _prepare_input\n",
      "    evaluate_strings_inputs = self.run_mapper(run)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 55, in __call__\n",
      "    return self.map(run)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 206, in map\n",
      "    input_ = self._get_key(run.inputs, self.input_key, \"input\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 177, in _get_key\n",
      "    raise ValueError(msg)\n",
      "ValueError: Could not map run input with multiple keys: {'question': 'What are the tax implications of early 401(k) withdrawal?', 'context_needed': ['retirement', 'taxation', 'penalties']}\n",
      "Please manually specify a input_key\n",
      "Error evaluating run 45b09524-2365-4ca0-aac2-ea0fd82aa2f7\n",
      "Traceback (most recent call last):\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 374, in evaluate_run\n",
      "    result = self({\"run\": run, \"example\": example}, include_run_info=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 188, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 413, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 167, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 333, in _call\n",
      "    evaluate_strings_inputs = self._prepare_input(inputs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 301, in _prepare_input\n",
      "    evaluate_strings_inputs = self.run_mapper(run)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 55, in __call__\n",
      "    return self.map(run)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 206, in map\n",
      "    input_ = self._get_key(run.inputs, self.input_key, \"input\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 177, in _get_key\n",
      "    raise ValueError(msg)\n",
      "ValueError: Could not map run input with multiple keys: {'question': 'How does dollar-cost averaging compare to lump-sum investing?', 'context_needed': ['investment strategy', 'risk management', 'market timing']}\n",
      "Please manually specify a input_key\n",
      "Error evaluating run 4db3d1f9-e1a7-4983-ae01-3979fea1effb\n",
      "Traceback (most recent call last):\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 374, in evaluate_run\n",
      "    result = self({\"run\": run, \"example\": example}, include_run_info=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 188, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 413, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py\", line 167, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 333, in _call\n",
      "    evaluate_strings_inputs = self._prepare_input(inputs)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 301, in _prepare_input\n",
      "    evaluate_strings_inputs = self.run_mapper(run)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 55, in __call__\n",
      "    return self.map(run)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 206, in map\n",
      "    input_ = self._get_key(run.inputs, self.input_key, \"input\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/github.com/sammyne/generative-ai-with-lang-chain-2ed/.venv/lib/python3.12/site-packages/langchain_classic/smith/evaluation/string_run_evaluator.py\", line 177, in _get_key\n",
      "    raise ValueError(msg)\n",
      "ValueError: Could not map run input with multiple keys: {'question': 'What are the tax implications of early 401(k) withdrawal?', 'context_needed': ['retirement', 'taxation', 'penalties']}\n",
      "Please manually specify a input_key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 2/2\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.smith import run_on_dataset\n",
    "\n",
    "results = run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=construct_chain,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc68f2",
   "metadata": {},
   "source": [
    "### Evaluating a benchmark with HF datasets and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62e7800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m41 packages\u001b[0m \u001b[2min 326ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/12] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m12 packages\u001b[0m \u001b[2min 192ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mevaluate\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install datasets~=3.4 evaluate~=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b83c601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9da1228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass@1': np.float64(0.5), 'pass@2': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "# from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "human_eval = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "code_eval_metric = load(\"code_eval\")\n",
    "\n",
    "test_cases = [\"assert add(2,3)==5\"]\n",
    "candidates = [[\"def add(a,b): return a*b\", \"def add(a, b): return a+b\"]]\n",
    "\n",
    "pass_at_k, results = code_eval_metric.compute(\n",
    "    references=test_cases, predictions=candidates, k=[1, 2]\n",
    ")\n",
    "print(pass_at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb302a9e",
   "metadata": {},
   "source": [
    "### Evaluating email extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b7c093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of synthetic insurance claim examples\n",
    "example_inputs = [\n",
    "    (\n",
    "        \"I was involved in a car accident on 2023-08-15. My name is Jane Smith, Claim ID INS78910, \"\n",
    "        \"Policy Number POL12345, and the damage is estimated at $3500.\",\n",
    "        {\n",
    "            \"claimant_name\": \"Jane Smith\",\n",
    "            \"claim_id\": \"INS78910\",\n",
    "            \"policy_number\": \"POL12345\",\n",
    "            \"claim_amount\": \"$3500\",\n",
    "            \"accident_date\": \"2023-08-15\",\n",
    "            \"accident_description\": \"Car accident causing damage\",\n",
    "            \"status\": \"pending\",\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"My motorcycle was hit in a minor collision on 2023-07-20. I am John Doe, with Claim ID INS112233 \"\n",
    "        \"and Policy Number POL99887. The estimated damage is $1500.\",\n",
    "        {\n",
    "            \"claimant_name\": \"John Doe\",\n",
    "            \"claim_id\": \"INS112233\",\n",
    "            \"policy_number\": \"POL99887\",\n",
    "            \"claim_amount\": \"$1500\",\n",
    "            \"accident_date\": \"2023-07-20\",\n",
    "            \"accident_description\": \"Minor motorcycle collision\",\n",
    "            \"status\": \"pending\",\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5255ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Config().new_langsmith_client()\n",
    "\n",
    "dataset_name = \"Insurance Claims\"\n",
    "\n",
    "# Create the dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Synthetic dataset for insurance claim extraction tasks\",\n",
    ")\n",
    "\n",
    "# Store examples in the dataset\n",
    "for input_text, expected_output in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"input\": input_text},\n",
    "        outputs={\"output\": expected_output},\n",
    "        metadata={\"source\": \"Synthetic\"},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7282ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extraction schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class InsuranceClaim(BaseModel):\n",
    "    claimant_name: str = Field(..., description=\"The name of the claimant\")\n",
    "    claim_id: str = Field(..., description=\"The unique insurance claim identifier\")\n",
    "    policy_number: str = Field(\n",
    "        ..., description=\"The policy number associated with the claim\"\n",
    "    )\n",
    "    claim_amount: str = Field(..., description=\"The claimed amount (e.g., '$5000')\")\n",
    "    accident_date: str = Field(..., description=\"The date of the accident (YYYY-MM-DD)\")\n",
    "    accident_description: str = Field(\n",
    "        ..., description=\"A brief description of the accident\"\n",
    "    )\n",
    "    status: str = Field(\"pending\", description=\"The current status of the claim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ebbabd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction Result:\n",
      "{'claimant_name': 'Jane Smith', 'claim_id': 'INS78910', 'policy_number': 'POL12345', 'claim_amount': '$3500', 'accident_date': '2023-08-15', 'accident_description': 'Car accident', 'status': 'pending'}\n"
     ]
    }
   ],
   "source": [
    "# Create extraction chain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=InsuranceClaim)\n",
    "\n",
    "instructions = PromptTemplate(\n",
    "    template=(\n",
    "        \"Extract the following structured information from the insurance claim text: \"\n",
    "        \"claimant_name, claim_id, policy_number, claim_amount, accident_date, \"\n",
    "        \"accident_description, and status.\\n\"\n",
    "        \"{format_instructions}\\n\"\n",
    "        \"{input}\\n\"\n",
    "    ),\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "extraction_chain = instructions | llm | output_parser\n",
    "\n",
    "# Test the extraction chain\n",
    "sample_claim_text = (\n",
    "    \"I was involved in a car accident on 2023-08-15. My name is Jane Smith, \"\n",
    "    \"Claim ID INS78910, Policy Number POL12345, and the damage is estimated at $3500. \"\n",
    "    \"Please process my claim.\"\n",
    ")\n",
    "\n",
    "result = extraction_chain.invoke({\"input\": sample_claim_text})\n",
    "print(\"Extraction Result:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-with-lang-chain-2ed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
