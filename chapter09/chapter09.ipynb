{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1d3664",
   "metadata": {},
   "source": [
    "# 09. Production-Ready LLM Deployment and Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342148f",
   "metadata": {},
   "source": [
    "## 0. 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3437cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-community~=0.4 langchain-openai~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d150a50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a245f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b943f84",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43fe1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "\n",
    "        self.langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624651f4",
   "metadata": {},
   "source": [
    "## Security considerations for LLM applications\n",
    "## Deploying LLM apps\n",
    "### Web framework deployment with FastAPI\n",
    "### Scalable deployment with Ray Serve\n",
    "#### Building the index\n",
    "#### Serving the index\n",
    "#### Running the application\n",
    "### Deployment considerations for LangChain applications\n",
    "### LangGraph platform\n",
    "#### Local development with the LangGraph CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe47b8",
   "metadata": {},
   "source": [
    "### Serverless deployment options\n",
    "- AWS Lambda: For lightweight LangChain applications, though with limitations on execution time and memory\n",
    "- Google Cloud Run: Supports containerized LangChain applications with automatic scaling\n",
    "- Azure Functions: Similar to AWS Lambda but in the Microsoft ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e89a6",
   "metadata": {},
   "source": [
    "### UI frameworks\n",
    "- [Chainlit](https://chainlit.io/)\n",
    "- [Gradio](https://www.gradio.app/)\n",
    "- [Streamlit](https://streamlit.io/)\n",
    "- [Mesop](https://mesop-dev.github.io/mesop/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc661c6",
   "metadata": {},
   "source": [
    "### Model Context Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ce0ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-mcp-adapters~=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e48cddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of $(3 + 5) \\times 12$ is $96$.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain.agents import create_agent\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "model = Config().new_openai_like()\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Update with the full absolute path to math_server.py\n",
    "    args=[\"static/math_server.py\"],\n",
    ")\n",
    "\n",
    "\n",
    "async def run_agent():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools = await load_mcp_tools(session)\n",
    "            agent = create_agent(model, tools)\n",
    "            response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "            response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "await run_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f7799",
   "metadata": {},
   "source": [
    "### Infrastructure considerations\n",
    "#### How to choose your deployment model\n",
    "1. 先看数据监管要求：严监管->本地部署；否则考虑上云；\n",
    "1. 需要绝对控制 -> 本地部署；\n",
    "1. 上云+自己运维\n",
    "1. 混合部署\n",
    "    - 敏感数据->本地，非敏感数据->云服务\n",
    "    - 特化任务->本地，通用任务->云服务\n",
    "    - 平时流量->本地，高峰流量->云服务\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c71352",
   "metadata": {},
   "source": [
    "#### Model serving infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a278fe",
   "metadata": {},
   "source": [
    "- The key to cost-effective LLM deployment is memory optimization. Quantization reduces your\n",
    "    models from 16-bit to 8-bit or 4-bit precision, cutting memory usage by 50-75% with minimal quality loss\n",
    "- Request batching is equally important – configure your serving layer to automatically group multiple user requests when possible. This improves throughput by 3-5x.\n",
    "- Pay attention to the attention key-value cache. Setting appropriate context length limits and implementing cache\n",
    "expiration strategies prevents memory overflow during long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e64523ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-litellm==0.3.2 litellm~=1.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd18837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LiteLLM enhances reliability for LLM applications by offering a unified interface to access multiple large language models, enabling seamless model switching, automatic retry logic, and fallback mechanisms. It ensures consistent performance during API failures or high latency by routing requests to alternative models or endpoints, thus improving application resilience and uptime.\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM with LangChain\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_litellm import ChatLiteLLMRouter\n",
    "from litellm import Router\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Configure multiple model deployments with fallbacks\n",
    "# openai/ 前缀的必要性参见 https://docs.litellm.ai/docs/providers/openai_compatible\n",
    "model_list = [\n",
    "    {\n",
    "        \"model_name\": f\"anthropic/{os.environ['ANTHROPIC_MODEL']}\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": f\"anthropic/{os.environ['ANTHROPIC_MODEL_FALLBACK']}\",  # Automatic fallback option\n",
    "            \"api_key\": os.environ[\"ANTHROPIC_API_KEY\"],\n",
    "            \"api_base\": os.environ[\"ANTHROPIC_BASE_URL\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": f\"openai/{os.environ['OPENAI_MODEL']}\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": f\"openai/{os.environ['OPENAI_MODEL_FALLBACK']}\",  # Automatic fallback option\n",
    "            \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "            \"api_base\": os.environ[\"OPENAI_API_BASE_URL\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Setup router with reliability features\n",
    "router = Router(\n",
    "    model_list=model_list,\n",
    "    routing_strategy=\"usage-based-routing-v2\",\n",
    "    cache_responses=True,  # Enable caching\n",
    "    num_retries=3,  # Auto-retry failed requests\n",
    ")\n",
    "\n",
    "model_name = f\"openai/{os.environ['OPENAI_MODEL']}\"\n",
    "# Create LangChain LLM with router\n",
    "router_llm = ChatLiteLLMRouter(router=router, model_name=model_name)\n",
    "\n",
    "# Build and use a LangChain\n",
    "prompt = PromptTemplate.from_template(\"Summarize: {text}\")\n",
    "chain = prompt | router_llm\n",
    "result = chain.invoke({\"text\": \"LiteLLM provides reliability for LLM applications\"})\n",
    "result.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a4dbc",
   "metadata": {},
   "source": [
    "## How to observe LLM apps\n",
    "### Operational metrics for LLM applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fafe77",
   "metadata": {},
   "source": [
    "### Tracking responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3848236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install pydantic~=2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9eb9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The latency for https://langchain.com is approximately **55 milliseconds** based on the ping result. This indicates a fairly responsive connection with no packet loss.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tracing of agent calls and intermediate results.\"\"\"\n",
    "\n",
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from pydantic import HttpUrl\n",
    "\n",
    "\n",
    "def ping(url: HttpUrl, return_error: bool) -> str:\n",
    "    \"\"\"Ping the fully specified url. Must include https:// in the url.\"\"\"\n",
    "    hostname = urlparse(str(url)).netloc\n",
    "    completed_process = subprocess.run(\n",
    "        [\"ping\", \"-c\", \"1\", hostname], capture_output=True, text=True\n",
    "    )\n",
    "    output = completed_process.stdout\n",
    "    if return_error and completed_process.returncode != 0:\n",
    "        return completed_process.stderr\n",
    "    return output\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm, tools=[ping], system_prompt=\"You are a helpful assistant\"\n",
    ")\n",
    "\n",
    "# 参考 https://python.langchain.com/docs/how_to/migrate_agent/#return_intermediate_steps\n",
    "# 输出的 result 已包含所有中间执行步骤\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What's the latency like for https://langchain.com?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a246b4f",
   "metadata": {},
   "source": [
    "From Ben Auffarth’s work at Chelsea AI Ventures\n",
    "\n",
    "- For all requests, track only the request ID, timestamp, token counts, latency, error codes, and endpoint called.\n",
    "- Sample 5% of non-critical interactions for deeper analysis. For customer service, increase to 15% during the first month after deployment or after major updates.\n",
    "- For critical use cases (financial advice or healthcare), track complete data for 20% of interactions. Never go below 10% for regulated domains.\n",
    "- Delete or aggregate data older than 30 days unless compliance requires longer retention. For most applications, keep only aggregate metrics after 90 days.\n",
    "- Use extraction patterns to remove PII from logged prompts – never store raw user inputs containing email addresses, phone numbers, or account details.\n",
    "\n",
    "This approach cuts storage requirements by 85-95% while maintaining sufficient data for troubleshooting and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d3b44",
   "metadata": {},
   "source": [
    "### Hallucination detection\n",
    "1. Retrieval-based validation: comparing the outputs of LLMs against retrieved external content to verify factual claims.\n",
    "2. LLM-as-judge: a more powerful LLM is used to assess the factual correctness of a response.\n",
    "3. External knowledge verification: entails cross-referencing model responses against trusted external sources to ensure accuracy.\n",
    "\n",
    "### Bias detection and monitoring\n",
    "- `demographic_parity_difference` function from the `Fairlearn` library\n",
    "\n",
    "#### LangSmith\n",
    "### Observability strategy\n",
    "### Continuous improvement for LLM applications\n",
    "## Cost management for LangChain applications\n",
    "Factors that drive costs in LLM applications:\n",
    "- **Token-based pricing**: Most LLM providers charge per token processed, with separate rates for input tokens (what you send) and output tokens (what the model generates).\n",
    "- **Output token premium**: Output tokens typically cost 2-5 times more than input tokens. For example, with GPT-4o, input tokens cost $0.005 per 1K tokens, while output tokens cost $0.015 per 1K tokens.\n",
    "- **Model tier differential**: More capable models command significantly higher prices. For instance, Claude 3 Opus costs substantially more than Claude 3 Sonnet, which is in turn more expensive than Claude 3 Haiku.\n",
    "- **Context window utilization**: As conversation history grows, the number of input tokens can increase dramatically, affecting costs\n",
    "\n",
    "### Model selection strategies in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f420f89",
   "metadata": {},
   "source": [
    "#### Tiered model selection\n",
    "**How**: Use a lightweight model to classify a query and select an appropriate model accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98653cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using affordable model for: what is the sum of 2+3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 2 + 3 is **5**.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "base_url = os.environ[\"OPENAI_API_BASE_URL\"]\n",
    "\n",
    "# Define models with different capabilities and costs\n",
    "affordable_model = ChatOpenAI(\n",
    "    model=os.environ[\"OPENAI_MODEL_AFFORDABLE\"],\n",
    "    base_url=base_url,\n",
    ")  # ~10× cheaper than gpt-4o\n",
    "\n",
    "powerful_model = ChatOpenAI(\n",
    "    model=os.environ[\"OPENAI_MODEL_POWERFUL\"],\n",
    "    base_url=base_url,\n",
    ")  # More capable but more expensive\n",
    "\n",
    "# Create classifier prompt\n",
    "classifier_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Determine if the following query is simple or complex based on these\n",
    "criteria:\n",
    "- Simple: factual questions, straightforward tasks, general knowledge\n",
    "- Complex: multi-step reasoning, nuanced analysis, specialized expertise\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Respond with only one word: \"simple\" or \"complex\"\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create the classifier chain\n",
    "classifier = classifier_prompt | affordable_model | StrOutputParser()\n",
    "\n",
    "\n",
    "def route_query(query):\n",
    "    \"\"\"Route the query to the appropriate model based on complexity.\"\"\"\n",
    "    complexity = classifier.invoke({\"query\": query})\n",
    "\n",
    "    if \"simple\" in complexity.lower():\n",
    "        print(f\"Using affordable model for: {query}\")\n",
    "        return affordable_model\n",
    "    else:\n",
    "        print(f\"Using powerful model for: {query}\")\n",
    "        return powerful_model\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def process_query(query):\n",
    "    model = route_query(query)\n",
    "    return model.invoke(query)\n",
    "\n",
    "\n",
    "simple_query = \"what is the sum of 2+3\"\n",
    "# print(process_query(simple_query))\n",
    "process_query(simple_query).pretty_print()\n",
    "\n",
    "# complex_query = \"plan a app serving billion users\"\n",
    "# print(process_query(complex_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78887c02",
   "metadata": {},
   "source": [
    "#### Cascading model approach\n",
    "**How**: First attempts a response using a cheaper model and escalates to a stronger one only if the initial output is inadequate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6619ec",
   "metadata": {},
   "source": [
    "### Output token optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43c81181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    if n == 2:\n",
      "        return True\n",
      "    if n % 2 == 0:\n",
      "        return False\n",
      "    for i in range(3, int(n**0.5) + 1, 2):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the LLM with max_tokens parameter\n",
    "llm = Config().new_openai_like(max_tokens=150) # Limit to approximately 100-120 words\n",
    "\n",
    "# Create a prompt template with length guidance\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that provides concise, accurate information. Your responses should be no more than 100 words unless explicitly asked for more detail.\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\"query\": \"write simple python function checking is a integer is prime\"}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1175d",
   "metadata": {},
   "source": [
    "### Other strategies\n",
    "- 缓存\n",
    "    1. In-memory caching: Simple caching to help reduce costs appropriate in a development environment.\n",
    "    1. Redis cache: Robust cache appropriate for production environments enabling persistence across application restarts and across multiple instances of your application.\n",
    "    1. Semantic caching: This advanced caching approach allows you to reuse responses for semantically similar queries, dramatically increasing cache hit rates.\n",
    "- Use structured outputs to eliminate unnecessary narrative text.\n",
    "- Implementing token-based context windowing is particularly important as it provides predictable cost control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab962f11",
   "metadata": {},
   "source": [
    "### Monitoring and cost analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d243e7",
   "metadata": {},
   "source": [
    "LangChain provides callbacks for tracking token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b868373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 522\n",
      "Prompt Tokens: 15\n",
      "Completion Tokens: 507\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm.invoke(\"Explain quantum computing in simple terms\")\n",
    "\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter09",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
