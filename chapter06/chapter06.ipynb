{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77fa30a",
   "metadata": {},
   "source": [
    "# 06. Advanced Applications and Multi-Agent Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83c2d9",
   "metadata": {},
   "source": [
    "# 0. 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain~=0.3 langchain-core~=0.3 langchain-community~=0.3 langchain-openai~=0.3 langgraph~=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4073b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23405207",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install arxiv~=2.2 ddgs~=9.6 wikipedia~=1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b10359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2760b2d",
   "metadata": {},
   "source": [
    "## Agentic architectures\n",
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c45c1",
   "metadata": {},
   "source": [
    "## Multi-agent architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5affe26",
   "metadata": {},
   "source": [
    "### Agent roles and specialization\n",
    "### Consensus mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c99dd",
   "metadata": {},
   "source": [
    "### Communication protocols\n",
    "#### Semantic router\n",
    "#### Organizing interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install datasets~=4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc50eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(\"cais/mmlu\", \"high_school_geography\")\n",
    "\n",
    "ds_dict = ds[\"test\"].take(100).to_dict()\n",
    "\n",
    "ds_dict[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f231e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict[\"choices\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "research_tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a hard-working, curious and creative student. \"\n",
    "    \"You're working on exam question. Think step by step.\"\n",
    "    \"Always provide an argumentation for your answer. \"\n",
    "    \"Do not assume anything, use available tools to search \"\n",
    "    \"for evidence and supporting statements.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dadfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "raw_prompt_template = (\n",
    "    \"Answer the following multiple-choice question. \"\n",
    "    \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", raw_prompt_template),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ResearchState(AgentState):\n",
    "    question: str\n",
    "    options: str\n",
    "\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=llm, tools=research_tools, state_schema=ResearchState, prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b276fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# 'Output in JSON format' 的必要性参见\n",
    "# - Qwen 的 https://help.aliyun.com/zh/model-studio/json-mode?spm=0.0.0.i2#6f7bb9cd64o7o\n",
    "# - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format#supported-schemas\n",
    "reflection_prompt = (\n",
    "    \"You are a university professor and you're supervising a student who is \"\n",
    "    \"working on multiple-choice exam question. \"\n",
    "    \"nQUESTION: {question}.\\nANSWER OPTIONS:\\n{options}\\n.\"\n",
    "    \"STUDENT'S ANSWER:\\n{answer}\\n\"\n",
    "    \"Reflect on the answer and provide a feedback whether the answer \"\n",
    "    \"is right or wrong. If you think the final answer is correct, reply with \"\n",
    "    \"the final answer. Only provide critique if you think the answer might \"\n",
    "    \"be incorrect or there are reasoning flaws. Do not assume anything, \"\n",
    "    \"evaluate only the reasoning the student provided and whether there is \"\n",
    "    \"enough evidence for their answer. \"\n",
    "    \"Output in JSON format, where the correct answer is put in the 'answer' field, \"\n",
    "    \"and critique is put in the 'critique' field.\"\n",
    ")\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"A final response to the user.\"\"\"\n",
    "\n",
    "    answer: str | None = Field(\n",
    "        description=\"The final answer. It should be empty if critique has been provided.\",\n",
    "        default=None,\n",
    "    )\n",
    "    critique: str | None = Field(\n",
    "        description=\"A critique of the initial answer. If you think it might be incorrect, provide an actionable feedback\",\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "\n",
    "reflection_chain = PromptTemplate.from_template(\n",
    "    reflection_prompt\n",
    ") | llm.with_structured_output(Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prompt_template_with_critique = (\n",
    "    \"You tried to answer the exam question and you get feedback from your \"\n",
    "    \"professor. Work on improving your answer and incorporating the feedback. \"\n",
    "    \"\\nQUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n",
    "    \"INITIAL ANSWER:\\n{answer}\\n\\nFEEDBACK:\\n{feedback}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", raw_prompt_template_with_critique),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ReflectionState(ResearchState):\n",
    "    answer: str\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "research_agent_with_critique = create_react_agent(\n",
    "    model=llm, tools=research_tools, state_schema=ReflectionState, prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3024bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "class ReflectionAgentState(TypedDict):\n",
    "    question: str\n",
    "    options: str\n",
    "    answer: str\n",
    "    steps: Annotated[int, add]\n",
    "    response: Response\n",
    "\n",
    "\n",
    "def _should_end(\n",
    "    state: ReflectionAgentState, config: RunnableConfig\n",
    ") -> Literal[\"research\", END]:\n",
    "    max_reasoning_steps = config[\"configurable\"].get(\"max_reasoning_steps\", 10)\n",
    "    if state.get(\"response\") and state[\"response\"].answer:\n",
    "        return END\n",
    "    if state.get(\"steps\", 1) > max_reasoning_steps:\n",
    "        return END\n",
    "    return \"research\"\n",
    "\n",
    "\n",
    "reflection_chain = PromptTemplate.from_template(\n",
    "    reflection_prompt\n",
    ") | llm.with_structured_output(Response)\n",
    "\n",
    "\n",
    "def _reflection_step(state):\n",
    "    result = reflection_chain.invoke(state)\n",
    "    return {\"response\": result, \"steps\": 1}\n",
    "\n",
    "\n",
    "def _research_start(state):\n",
    "    answer = research_agent.invoke(state)\n",
    "    return {\"answer\": answer[\"messages\"][-1].content}\n",
    "\n",
    "\n",
    "def _research(state):\n",
    "    agent_state = {\n",
    "        \"answer\": state[\"answer\"],\n",
    "        \"question\": state[\"question\"],\n",
    "        \"options\": state[\"options\"],\n",
    "        \"feedback\": state[\"response\"].critique,\n",
    "    }\n",
    "    answer = research_agent_with_critique.invoke(agent_state)\n",
    "    return {\"answer\": answer[\"messages\"][-1].content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ReflectionAgentState)\n",
    "builder.add_node(\"research_start\", _research_start)\n",
    "builder.add_node(\"research\", _research)\n",
    "builder.add_node(\"reflect\", _reflection_step)\n",
    "\n",
    "builder.add_edge(START, \"research_start\")\n",
    "builder.add_edge(\"research_start\", \"reflect\")\n",
    "builder.add_edge(\"research\", \"reflect\")\n",
    "builder.add_conditional_edges(\"reflect\", _should_end)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de393b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "question = ds_dict[\"question\"][i]\n",
    "options = \"\\n\".join([f\"{i}. {a}\" for i, a in enumerate(ds_dict[\"choices\"][i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e424bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 解决无法终止的问题\n",
    "async for _, event in graph.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"updates\"]\n",
    "):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee93be1",
   "metadata": {},
   "source": [
    "### LangGraph streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for _, event in research_agent.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"values\"]\n",
    "):\n",
    "    print(len(event[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75891d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for _, event in research_agent.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"updates\"]\n",
    "):\n",
    "    node = list(event.keys())[0]\n",
    "    print(node, len(event[node].get(\"messages\", [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19719c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_events = set([])\n",
    "async for event in research_agent.astream_events(\n",
    "    {\"question\": question, \"options\": options}, version=\"v1\"\n",
    "):\n",
    "    if event[\"event\"] not in seen_events:\n",
    "        seen_events.add(event[\"event\"])\n",
    "\n",
    "print(seen_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc00604",
   "metadata": {},
   "source": [
    "### Handoffs\n",
    "#### Communication via a shared messages list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "research_tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a hard-working, curious and creative student. \"\n",
    "    \"You're working on exam question. Think step by step.\"\n",
    "    \"Always provide an argumentation for your answer. \"\n",
    "    \"Do not assume anything, use available tools to search \"\n",
    "    \"for evidence and supporting statements.\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=llm, tools=research_tools, prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2799f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = (\n",
    "    \"You are a university professor and you're supervising a student who is \"\n",
    "    \"working on multiple-choice exam question. \"\n",
    "    \"Given the dialogue above, reflect on the answer provided and give a feedback \"\n",
    "    \" if needed. If you think the final answer is correct, reply with \"\n",
    "    \"an empty message. Only provide critique if you think the last answer might \"\n",
    "    \"be incorrect or there are reasoning flaws. Do not assume anything, \"\n",
    "    \"evaluate only the reasoning the student provided and whether there is \"\n",
    "    \"enough evidence for their answer.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.types import Command\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "question_template = PromptTemplate.from_template(\n",
    "    \"QUESTION:\\n{question}\\n\\nANSWER OPTIONS:\\n{options}\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def _ask_question(state):\n",
    "    return {\"messages\": [(\"human\", question_template.invoke(state).text)]}\n",
    "\n",
    "\n",
    "def _give_feedback(state, config: RunnableConfig):\n",
    "    messages = event[\"messages\"] + [(\"human\", reflection_prompt)]\n",
    "    max_messages = config[\"configurable\"].get(\"max_messages\", 20)\n",
    "\n",
    "    if len(messages) > max_messages:\n",
    "        return Command(update={}, goto=END)\n",
    "\n",
    "    result = llm.invoke(messages)\n",
    "\n",
    "    if result.content:\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [\n",
    "                    (\"assistant\", result.content),\n",
    "                    (\"human\", \"Please, address the feedback above and give an answer.\"),\n",
    "                ]\n",
    "            },\n",
    "            goto=\"research\",\n",
    "        )\n",
    "    return Command(update={}, goto=END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "\n",
    "\n",
    "class ReflectionAgentState(MessagesState):\n",
    "    question: str\n",
    "    options: str\n",
    "\n",
    "\n",
    "builder = StateGraph(ReflectionAgentState)\n",
    "builder.add_node(\"ask_question\", _ask_question)\n",
    "builder.add_node(\"research\", research_agent)\n",
    "builder.add_node(\"reflect\", _give_feedback)\n",
    "\n",
    "builder.add_edge(START, \"ask_question\")\n",
    "builder.add_edge(\"ask_question\", \"research\")\n",
    "builder.add_edge(\"research\", \"reflect\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5445e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for _, event in graph.astream(\n",
    "    {\"question\": question, \"options\": options}, stream_mode=[\"values\"]\n",
    "):\n",
    "    print(len(event[\"messages\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db072431",
   "metadata": {},
   "source": [
    "## LangGraph platform\n",
    "## Building adaptive systems\n",
    "### Dynamic behavior adjustment\n",
    "### Human-in-the-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416575f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    home_address: str | None\n",
    "\n",
    "\n",
    "def _human_input(state: State):\n",
    "    address = interrupt(\"What is your address?\")\n",
    "    return {\"home_address\": address}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"human_input\", _human_input)\n",
    "builder.add_edge(START, \"human_input\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for chunk in graph.stream({\"messages\": [(\"human\", \"What is weather today?\")]}, config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed40368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "for chunk in graph.stream(Command(resume=\"Munich\"), config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa69e1a",
   "metadata": {},
   "source": [
    "## Exploring reasoning paths\n",
    "### Tree of Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc43d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "system_prompt_template = (\n",
    "    \"For the given task, come up with a step by step plan.\\n\"\n",
    "    \"This plan should involve individual tasks, that if executed correctly will \"\n",
    "    \"yield the correct answer. Do not add any superfluous steps.\\n\"\n",
    "    \"The result of the final step should be the final answer. Make sure that each \"\n",
    "    \"step has all the information needed - do not skip steps.\\n\"\n",
    "    \"Output in JSON format described as follows.\\n\"\n",
    ")\n",
    "# 注意 system 消息中的插值变量 `formatting_instructions` 需要和 partial 函数的入参完全匹配。\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_template + \"{formatting_instructions}\"),\n",
    "        (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\"),\n",
    "    ]\n",
    ").partial(formatting_instructions=parser.get_format_instructions())\n",
    "\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "planner = planner_prompt | llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "tools = load_tools(tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"], llm=llm)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You're a smart assistant that carefully helps to solve complex tasks.\\n\"\n",
    "    \" Given a general plan to solve a task and a specific step, work on this step. \"\n",
    "    \" Don't assume anything, keep in minds things might change and always try to \"\n",
    "    \"use tools to double-check yourself.\\nUse Search to gather \"\n",
    "    \"information about common facts, fresh events and news, use Arxiv to get \"\n",
    "    \"ideas on recent research and use Wikipedia for common knowledge.\"\n",
    ")\n",
    "\n",
    "step_template = (\n",
    "    \"Given the task and the plan, try to execute on a specific step of the plan.\\n\"\n",
    "    \"TASK:\\n{task}\\n\\nPLAN:\\n{previous_steps}\\n\\nSTEP TO EXECUTE:\\n{step}\\n\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", step_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "execution_agent = prompt_template | create_react_agent(model=llm, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class ReplanStep(BaseModel):\n",
    "    \"\"\"Replanned next step in the plan.\"\"\"\n",
    "\n",
    "    steps: list[str] = Field(description=\"different options of the proposed next step\")\n",
    "\n",
    "\n",
    "llm_replanner = llm.with_structured_output(ReplanStep)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Plan)\n",
    "\n",
    "replanner_prompt_template = (\n",
    "    \"Suggest next action in the plan. Do not add any superfluous steps.\\n\"\n",
    "    \"If you think no actions are needed, just return an empty list of steps. \"\n",
    "    \"TASK: {task}\\n PREVIOUS STEPS WITH OUTPUTS: {current_plan}\\n\"\n",
    "    \"Output in JSON format described as follows.\\n\"\n",
    ")\n",
    "# 参考 https://zhuanlan.zhihu.com/p/1901678624639255242\n",
    "replanner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a helpful assistant. You goal is to help with planning actions to solve the task. Do not solve the task itself.\"\n",
    "            \"{formatting_instructions}\",\n",
    "        ),\n",
    "        (\"user\", replanner_prompt_template),\n",
    "    ]\n",
    ").partial(formatting_instructions=parser.get_format_instructions())\n",
    "\n",
    "replanner = replanner_prompt | llm_replanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_id: int,\n",
    "        step: str,\n",
    "        step_output: str | None = None,\n",
    "        parent: Optional[\"TreeNode\"] = None,\n",
    "    ):\n",
    "        self.node_id = node_id\n",
    "        self.step = step\n",
    "        self.step_output = step_output\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.final_response = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        parent_id = self.parent.node_id if self.parent else \"None\"\n",
    "        return f\"Node_id: {self.node_id}, parent: {parent_id}, {len(self.children)} children.\"\n",
    "\n",
    "    def get_full_plan(self) -> str:\n",
    "        \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n",
    "        steps = []\n",
    "        node = self\n",
    "        while node.parent:\n",
    "            steps.append((node.step, node.step_output))\n",
    "            node = node.parent\n",
    "\n",
    "        full_plan = []\n",
    "        for i, (step, result) in enumerate(steps[::-1]):\n",
    "            if result:\n",
    "                full_plan.append(f\"# {i+1}. Planned step: {step}\\nResult: {result}\\n\")\n",
    "        return \"\\n\".join(full_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import deque\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class PlanState(TypedDict):\n",
    "    task: str\n",
    "    root: TreeNode\n",
    "    queue: deque[TreeNode]\n",
    "    current_node: TreeNode\n",
    "    next_node: TreeNode\n",
    "    is_current_node_final: bool\n",
    "    paths_explored: Annotated[int, operator.add]\n",
    "    visited_ids: set[int]\n",
    "    max_id: int\n",
    "    candidates: Annotated[list[str], operator.add]\n",
    "    best_candidate: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549609cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.types import Command\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "responder = final_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "async def _run_node(state: PlanState, config: RunnableConfig):\n",
    "    node = state.get(\"next_node\")\n",
    "    visited_ids = state.get(\"visited_ids\", set())\n",
    "    queue = state[\"queue\"]\n",
    "    if node is None:\n",
    "        while queue and not node:\n",
    "            node = state[\"queue\"].popleft()\n",
    "            if node.node_id in visited_ids:\n",
    "                node = None\n",
    "        if not node:\n",
    "            return Command(goto=\"vote\", update={})\n",
    "\n",
    "    step = await execution_agent.ainvoke(\n",
    "        {\n",
    "            \"previous_steps\": node.get_full_plan(),\n",
    "            \"step\": node.step,\n",
    "            \"task\": state[\"task\"],\n",
    "        }\n",
    "    )\n",
    "    node.step_output = step[\"messages\"][-1].content\n",
    "    visited_ids.add(node.node_id)\n",
    "    return {\n",
    "        \"current_node\": node,\n",
    "        \"queue\": queue,\n",
    "        \"visited_ids\": visited_ids,\n",
    "        \"next_node\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "async def _plan_next(state: PlanState, config: RunnableConfig) -> PlanState:\n",
    "    max_candidates = config[\"configurable\"].get(\"max_candidates\", 1)\n",
    "    node = state[\"current_node\"]\n",
    "    next_step = await replanner.ainvoke(\n",
    "        {\"task\": state[\"task\"], \"current_plan\": node.get_full_plan()}\n",
    "    )\n",
    "    if not next_step.steps:\n",
    "        return {\"is_current_node_final\": True}\n",
    "    max_id = state[\"max_id\"]\n",
    "    for step in next_step.steps[:max_candidates]:\n",
    "        child = TreeNode(node_id=max_id + 1, step=step, parent=node)\n",
    "        max_id += 1\n",
    "        node.children.append(child)\n",
    "        state[\"queue\"].append(child)\n",
    "    return {\"is_current_node_final\": False, \"next_node\": child, \"max_id\": max_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf667d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_voting = PromptTemplate.from_template(\n",
    "    \"Pick the best solution for a given task. \"\n",
    "    \"\\nTASK:{task}\\n\\nSOLUTIONS:\\n{candidates}\\n\"\n",
    "    \"Output 1-based index of the best solution.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def _vote_for_the_best_option(state):\n",
    "    candidates = state.get(\"candidates\", [])\n",
    "    if not candidates:\n",
    "        return {\"best_response\": None}\n",
    "    all_candidates = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        all_candidates.append(f\"OPTION {i+1}: {candidate}\")\n",
    "\n",
    "    llm_enum = Config().new_openai_like()\n",
    "\n",
    "    result = (prompt_voting | llm_enum | StrOutputParser()).invoke(\n",
    "        {\"candidates\": \"\\n\".join(all_candidates), \"task\": state[\"task\"]}\n",
    "    )\n",
    "    return {\"best_candidate\": candidates[int(result) - 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_vote_for_the_best_option({\"candidates\": [\"1\", \"5\", \"4\"], \"task\": \"How much is 2+2?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb709f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(\n",
    "    \"You're a helpful assistant that has executed on a plan.\"\n",
    "    \"Given the results of the execution, prepare the final response.\\n\"\n",
    "    \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n",
    "    \"FINAL RESPONSE:\\n\"\n",
    ")\n",
    "\n",
    "responder = final_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "async def _build_initial_plan(state: PlanState) -> PlanState:\n",
    "    plan = await planner.ainvoke(state[\"task\"])\n",
    "    queue = deque()\n",
    "    root = TreeNode(step=plan.steps[0], node_id=1)\n",
    "    queue.append(root)\n",
    "    current_root = root\n",
    "    for i, step in enumerate(plan.steps[1:]):\n",
    "        child = TreeNode(node_id=i + 2, step=step, parent=current_root)\n",
    "        current_root.children.append(child)\n",
    "        queue.append(child)\n",
    "        current_root = child\n",
    "    return {\"root\": root, \"queue\": queue, \"max_id\": i + 2}\n",
    "\n",
    "\n",
    "async def _get_final_response(state: PlanState) -> PlanState:\n",
    "    node = state[\"current_node\"]\n",
    "    final_response = await responder.ainvoke(\n",
    "        {\"task\": state[\"task\"], \"plan\": node.get_full_plan()}\n",
    "    )\n",
    "    node.final_response = final_response\n",
    "    return {\"paths_explored\": 1, \"candidates\": [final_response]}\n",
    "\n",
    "\n",
    "def _should_create_final_response(\n",
    "    state: PlanState,\n",
    ") -> Literal[\"run\", \"generate_response\"]:\n",
    "    return \"generate_response\" if state[\"is_current_node_final\"] else \"run\"\n",
    "\n",
    "\n",
    "def _should_continue(\n",
    "    state: PlanState, config: RunnableConfig\n",
    ") -> Literal[\"run\", \"vote\"]:\n",
    "    max_paths = config[\"configurable\"].get(\"max_paths\", 30)\n",
    "    if state.get(\"paths_explored\", 1) >= max_paths:\n",
    "        return \"vote\"\n",
    "    if state[\"queue\"] or state.get(\"next_node\"):\n",
    "        return \"run\"\n",
    "    return \"vote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6afe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(PlanState)\n",
    "builder.add_node(\"initial_plan\", _build_initial_plan)\n",
    "builder.add_node(\"run\", _run_node)\n",
    "builder.add_node(\"plan_next\", _plan_next)\n",
    "builder.add_node(\"generate_response\", _get_final_response)\n",
    "builder.add_node(\"vote\", _vote_for_the_best_option)\n",
    "\n",
    "builder.add_edge(START, \"initial_plan\")\n",
    "builder.add_edge(\"initial_plan\", \"run\")\n",
    "builder.add_edge(\"run\", \"plan_next\")\n",
    "builder.add_conditional_edges(\"plan_next\", _should_create_final_response)\n",
    "builder.add_conditional_edges(\"generate_response\", _should_continue)\n",
    "builder.add_edge(\"vote\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Write a strategic one-pager of building an AI startup\"\n",
    "\n",
    "# TODO: 解决执行时间过久的问题\n",
    "result = await graph.ainvoke(\n",
    "    {\"task\": task}, config={\"recursion_limit\": 10000, \"configurable\": {\"max_paths\": 10}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a504989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result[\"candidates\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be400b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"best_candidate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监控执行过程的脚本\n",
    "async for e in graph.astream(\n",
    "    {\"task\": task}, config={\"recursion_limit\": 10000, \"configurable\": {\"max_paths\": 10}}\n",
    "):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b24fa1",
   "metadata": {},
   "source": [
    "### Trimming ToT with MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729e236",
   "metadata": {},
   "source": [
    "## Agent memory\n",
    "### Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1213053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "\n",
    "cache = InMemoryCache()\n",
    "set_llm_cache(cache)\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "llm.invoke(\"What is the capital of UK?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fb483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "\n",
    "print(langchain.llm_cache._cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfa968",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._get_llm_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f4d20",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "in_memory_store.put(\n",
    "    namespace=(\"users\", \"user1\"), key=\"fact1\", value={\"message1\": \"My name is John.\"}\n",
    ")\n",
    "in_memory_store.put(\n",
    "    namespace=(\"users\", \"user1\", \"conv1\"),\n",
    "    key=\"address\",\n",
    "    value={\"message\": \"I live in Berlin.\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc802b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store.get(namespace=(\"users\", \"user1\", \"conv1\"), key=\"address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92118d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store.get(namespace=(\"users\", \"user1\"), key=\"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89598f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store.search((\"users\", \"user1\", \"conv1\"), query=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d56c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store.search((\"users\", \"user1\"), query=\"name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
