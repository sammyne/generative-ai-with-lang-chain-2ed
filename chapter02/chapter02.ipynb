{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aad480",
   "metadata": {},
   "source": [
    "# 02. First Steps with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_vl(self, **kwargs) -> ChatOpenAI:\n",
    "        if not self.vl_model:\n",
    "            raise ValueError(\"OPENAI_VL_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.vl_model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546703e",
   "metadata": {},
   "source": [
    "## Running local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68292e22",
   "metadata": {},
   "source": [
    "### Getting started with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99748b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh run deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a492658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Ollama with your chosen model\n",
    "local_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0,\n",
    "    # 将 base_url 的地址部分替换为上一步打印出来的地址。\n",
    "    base_url=\"http://172.17.0.4:11434\",\n",
    ")\n",
    "# Create an LCEL chain using the local model\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "local_chain = prompt | local_llm | StrOutputParser()\n",
    "# Use the chain with your local model\n",
    "result = local_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75c495",
   "metadata": {},
   "source": [
    "### Working with Hugging Face models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d126502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-huggingface transformers torch==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Create a pipeline with a small model:\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "# Use it like any other LangChain LLM\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Explain the concept of machine learning in simple terms\"),\n",
    "]\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "print(ai_msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667494a8",
   "metadata": {},
   "source": [
    "### Tips for local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741caa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def safe_model_call(llm, prompt, max_retries=2):\n",
    "    \"\"\"Safely call a local model with retry logic and graceful failure\"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except RuntimeError as e:\n",
    "            # Common error with local models when running out of VRAM\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\n",
    "                    f\"GPU memory error, waiting and retrying ({retries+1}/{max_retries+1})\"\n",
    "                )\n",
    "                time.sleep(2)  # Give system time to free resources\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"Runtime error: {e}\")\n",
    "                return \"An error occurred while processing your request.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error calling model: {e}\")\n",
    "            return \"An error occurred while processing your request.\"\n",
    "    # If we exhausted retries\n",
    "    return \"Model is currently experiencing high load. Please try again later.\"\n",
    "\n",
    "\n",
    "# Use the safety wrapper in your LCEL chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "safe_llm = RunnableLambda(lambda x: safe_model_call(llm, x))\n",
    "safe_chain = prompt | safe_llm\n",
    "response = safe_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22f578",
   "metadata": {},
   "source": [
    "## Multimodal AI applications\n",
    "### Text-to-image\n",
    "#### Using DALL-E through OpenAI\n",
    "TODO：找到或实现支持 langchain 接口的千义同问文生图模型（候选 qwen-image-plus）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515f244",
   "metadata": {},
   "source": [
    "### Image understanding\n",
    "#### Using Qwen-VL (similar to Gemini 1.5 Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "with open(\"static/stable-diffusion.png\", \"rb\") as image_file:\n",
    "    image_bytes = image_file.read()\n",
    "    base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    {\"type\": \"text\", \"text\": \"Describe the image: \"},\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_bytes}\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like_vl()\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from utils import Config\n",
    "\n",
    "\n",
    "def analyze_image(image_path: str, question: str) -> str:\n",
    "    # chat = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=256)\n",
    "    chat = Config().new_openai_like_vl(max_tokens=256)\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_bytes}\",\n",
    "                    \"detail\": \"auto\",\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = chat.invoke([message])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = \"static/skyscrapers.png\"\n",
    "questions = [\n",
    "    \"What objects do you see in this image?\",\n",
    "    \"What is the overall mood or atmosphere?\",\n",
    "    \"Are there any people in the image?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {analyze_image(image_path, question)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
