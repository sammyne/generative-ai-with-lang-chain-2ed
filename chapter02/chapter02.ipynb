{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aad480",
   "metadata": {},
   "source": [
    "# 02. First Steps with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec867464",
   "metadata": {},
   "source": [
    "## ÂÆâË£Ö‰æùËµñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95146c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m59 packages\u001b[0m \u001b[2min 101ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/6] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==0.6.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==0.6.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=0.3 langchain-core~=0.3 langchain-community~=0.3 langchain-openai~=0.3 langgraph~=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307b1a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc7aa2",
   "metadata": {},
   "source": [
    "Â∑•ÂÖ∑Á±ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829f7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # ÂèÇËÄÉÔºöhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # ÂèÇËÄÉÔºöhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI ÊñáÊ°£ÂèÇËÄÉÔºöhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_vl(self, **kwargs) -> ChatOpenAI:\n",
    "        if not self.vl_model:\n",
    "            raise ValueError(\"OPENAI_VL_MODEL is not set\")\n",
    "\n",
    "        # ÂèÇËÄÉÔºöhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # ÂèÇËÄÉÔºöhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI ÊñáÊ°£ÂèÇËÄÉÔºöhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.vl_model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7214c31",
   "metadata": {},
   "source": [
    "## Exploring LangChain‚Äôs building blocks\n",
    "### Model interfaces\n",
    "#### LLM interaction patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a classic light bulb joke for you:\n",
      "\n",
      "**How many programmers does it take to change a light bulb?**  \n",
      "*None ‚Äî It‚Äôs a hardware problem.*\n",
      "\n",
      "üòÑ\n",
      "\n",
      "And here‚Äôs another one for variety:\n",
      "\n",
      "**How many psychologists does it take to change a light bulb?**  \n",
      "*Just one ‚Äî but the light bulb has to really want to change.*\n",
      "\n",
      "Let me know if you want a joke about a specific type of bulb ‚Äî LED, fluorescent, or even a 19th-century gas lamp!\n"
     ]
    }
   ],
   "source": [
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Both can be used with the same interface\n",
    "response = llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02175e00",
   "metadata": {},
   "source": [
    "#### Development testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2d4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import FakeListLLM\n",
    "\n",
    "# Create a fake LLM that always returns the same responses\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "\n",
    "result = fake_llm.invoke(\"Any input will return Hello\")\n",
    "print(result)  # Output: Hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b32849",
   "metadata": {},
   "source": [
    "#### Working with chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab261f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function to calculate the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a non-negative integer n.\n",
      "    \n",
      "    Args:\n",
      "        n (int): A non-negative integer\n",
      "        \n",
      "    Returns:\n",
      "        int: The factorial of n (n!)\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: If n is negative\n",
      "        TypeError: If n is not an integer\n",
      "    \"\"\"\n",
      "    # Input validation\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Factorial is only defined for integers\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "    \n",
      "    # Base cases\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Calculate factorial iteratively\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "Here's also a recursive version for comparison:\n",
      "\n",
      "```python\n",
      "def factorial_recursive(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a non-negative integer n using recursion.\n",
      "    \n",
      "    Args:\n",
      "        n (int): A non-negative integer\n",
      "        \n",
      "    Returns:\n",
      "        int: The factorial of n (n!)\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: If n is negative\n",
      "        TypeError: If n is not an integer\n",
      "    \"\"\"\n",
      "    # Input validation\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Factorial is only defined for integers\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "    \n",
      "    # Base cases\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Recursive case\n",
      "    return n * factorial_recursive(n - 1)\n",
      "```\n",
      "\n",
      "And here's an example of how to use it:\n",
      "\n",
      "```python\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    # Test cases\n",
      "    test_numbers = [0, 1, 5, 10]\n",
      "    \n",
      "    for num in test_numbers:\n",
      "        print(f\"{num}! = {factorial(num)}\")\n",
      "    \n",
      "    # Output:\n",
      "    # 0! = 1\n",
      "    # 1! = 1\n",
      "    # 5! = 120\n",
      "    # 10! = 3628800\n",
      "```\n",
      "\n",
      "**Notes:**\n",
      "- The iterative version is generally preferred for larger numbers as it avoids potential stack overflow issues with recursion\n",
      "- Both versions include proper input validation\n",
      "- Factorial of 0 is defined as 1 (mathematical convention)\n",
      "- For very large numbers, consider using Python's built-in `math.factorial()` function which is optimized\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful programming assistant\"),\n",
    "    HumanMessage(content=\"Write a Python function to calculate factorial\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a3869",
   "metadata": {},
   "source": [
    "#### Reasoning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429e1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the **k-th largest element** in an unsorted array with **optimal time complexity**, we use the **Quickselect algorithm**, which is a selection algorithm based on the divide-and-conquer strategy of Quicksort.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ Algorithm: **Quickselect**\n",
      "\n",
      "### **Intuition**\n",
      "Quickselect is similar to Quicksort but only recurses into one partition (the one containing the desired element), making it more efficient for selection problems.\n",
      "\n",
      "We want the **k-th largest** element. This is equivalent to finding the **(n - k + 1)-th smallest** element, where `n` is the array length. For simplicity, we can **redefine k** as the index in a 0-based sorted array:  \n",
      "> **k-th largest = (n - k)-th smallest** (0-indexed)\n",
      "\n",
      "Alternatively, we can modify the partitioning to find the k-th largest directly by reversing the comparison.\n",
      "\n",
      "---\n",
      "\n",
      "### **Algorithm Steps**\n",
      "\n",
      "1. **Input**: Array `arr` of length `n`, integer `k` (1-indexed k-th largest)\n",
      "2. **Convert**: Let `target_index = n - k` (we want the element that would be at this index if array were sorted in ascending order)\n",
      "3. **Use Quickselect**:\n",
      "   - Choose a **pivot** (randomized for best average-case performance)\n",
      "   - Partition the array around the pivot: elements < pivot go left, elements ‚â• pivot go right\n",
      "   - Let `pivot_index` be the final position of the pivot after partitioning\n",
      "   - If `pivot_index == target_index`: return `arr[pivot_index]`\n",
      "   - If `pivot_index < target_index`: recurse on the right subarray\n",
      "   - If `pivot_index > target_index`: recurse on the left subarray\n",
      "4. **Return** the element at `target_index`\n",
      "\n",
      "> We use **randomized pivot selection** to avoid worst-case O(n¬≤) behavior on adversarial inputs.\n",
      "\n",
      "---\n",
      "\n",
      "### **Pseudocode**\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def quickselect(arr, k):\n",
      "    # k is 1-indexed: k-th largest\n",
      "    n = len(arr)\n",
      "    target_index = n - k  # convert to 0-indexed smallest rank\n",
      "\n",
      "    def select(left, right):\n",
      "        if left == right:\n",
      "            return arr[left]\n",
      "        \n",
      "        # Random pivot selection for good average performance\n",
      "        pivot_index = random.randint(left, right)\n",
      "        pivot_index = partition(left, right, pivot_index)\n",
      "        \n",
      "        if pivot_index == target_index:\n",
      "            return arr[pivot_index]\n",
      "        elif pivot_index < target_index:\n",
      "            return select(pivot_index + 1, right)\n",
      "        else:\n",
      "            return select(left, pivot_index - 1)\n",
      "\n",
      "    def partition(left, right, pivot_index):\n",
      "        pivot_value = arr[pivot_index]\n",
      "        # Move pivot to end\n",
      "        arr[pivot_index], arr[right] = arr[right], arr[pivot_index]\n",
      "        \n",
      "        store_index = left\n",
      "        for i in range(left, right):\n",
      "            if arr[i] < pivot_value:  # for smallest selection\n",
      "                arr[store_index], arr[i] = arr[i], arr[store_index]\n",
      "                store_index += 1\n",
      "        \n",
      "        # Move pivot to its final place\n",
      "        arr[right], arr[store_index] = arr[store_index], arr[right]\n",
      "        return store_index\n",
      "\n",
      "    return select(0, n - 1)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ Time Complexity Analysis\n",
      "\n",
      "| Case | Time Complexity |\n",
      "|------|-----------------|\n",
      "| **Best/Average** | **O(n)** |\n",
      "| **Worst** | **O(n¬≤)** |\n",
      "\n",
      "### Why Average O(n)?\n",
      "\n",
      "- In each recursive call, we eliminate roughly half the array (on average).\n",
      "- The recurrence relation is:  \n",
      "  `T(n) = T(n/2) + O(n)`  \n",
      "  Solving this:  \n",
      "  `T(n) = O(n) + O(n/2) + O(n/4) + ... = O(n)` (geometric series)\n",
      "\n",
      "### Why Worst Case O(n¬≤)?\n",
      "\n",
      "- If we always pick the smallest or largest element as pivot (e.g., sorted array and pivot is first), then each partition reduces size by only 1.\n",
      "- Then: `T(n) = T(n-1) + O(n) = O(n¬≤)`\n",
      "\n",
      "### **Mitigation**: Random pivot selection ensures that the probability of consistently bad pivots is negligible. With high probability, the algorithm runs in **O(n)** time.\n",
      "\n",
      "> **Note**: There exists a deterministic O(n) algorithm called **Median of Medians**, but it has large constant factors and is rarely used in practice. Quickselect with randomization is **practically optimal**.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ Space Complexity\n",
      "\n",
      "| Type | Space |\n",
      "|------|-------|\n",
      "| **Auxiliary Space** | **O(log n)** (recursion stack) |\n",
      "| **In-place?** | ‚úÖ Yes, modifies input array |\n",
      "\n",
      "- Partitioning is done in-place.\n",
      "- Recursion depth: average `O(log n)`, worst-case `O(n)`\n",
      "- If we use **iterative quickselect**, space can be reduced to **O(1)**.\n",
      "\n",
      "> For practical purposes, **O(log n)** space is acceptable and typical.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ Why Is This Optimal?\n",
      "\n",
      "### Lower Bound Argument:\n",
      "\n",
      "- To find the k-th largest element, we must examine **at least n - k + 1** elements (in the worst case, the k-th largest could be the smallest among the top k elements).\n",
      "- However, more formally: **any comparison-based selection algorithm must make at least Œ©(n) comparisons** in the worst case to identify the k-th element.\n",
      "- Quickselect achieves **O(n)** expected time, which matches the **information-theoretic lower bound** for selection.\n",
      "- No comparison-based algorithm can do better than O(n) on average ‚Äî so **Quickselect is asymptotically optimal**.\n",
      "\n",
      "### Comparison with Alternatives:\n",
      "\n",
      "| Method | Time | Space | Notes |\n",
      "|--------|------|-------|-------|\n",
      "| Sort + Index | O(n log n) | O(1) or O(n) | Suboptimal |\n",
      "| Min-Heap of size k | O(n log k) | O(k) | Good for small k, but not optimal when k ~ n |\n",
      "| Quickselect | **O(n) avg** | **O(log n)** | ‚úÖ **Optimal** for general case |\n",
      "| Median of Medians | O(n) worst | O(1) | Deterministic, but impractical due to large constants |\n",
      "\n",
      "> **Quickselect is optimal in practice** because it achieves the theoretical lower bound with low constant factors and is widely used in libraries (e.g., C++ `std::nth_element`).\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ Conclusion\n",
      "\n",
      "- **Algorithm**: **Quickselect with randomized pivot**\n",
      "- **Time Complexity**: **O(n) average**, O(n¬≤) worst-case (but rare with randomization)\n",
      "- **Space Complexity**: **O(log n)** average (recursion stack)\n",
      "- **Optimality**: Matches the Œ©(n) lower bound for selection ‚Äî **no comparison-based algorithm can be faster on average**\n",
      "- **Practical Use**: Used in production systems (e.g., `std::nth_element` in C++, `numpy.partition` in Python)\n",
      "\n",
      "> ‚úÖ **Quickselect is the optimal algorithm for finding the k-th largest element in an unsorted array.**\n",
      "\n",
      "---\n",
      "\n",
      "### Bonus: One-liner in Python (for reference, not optimal in practice)\n",
      "\n",
      "```python\n",
      "import heapq\n",
      "def kth_largest(arr, k):\n",
      "    return heapq.nlargest(k, arr)[-1]  # O(n log k)\n",
      "```\n",
      "\n",
      "But this is **not optimal** when `k` is large (e.g., k = n/2 ‚Üí O(n log n)), whereas Quickselect remains O(n).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# initialize OpenAI-like model with reasoning_effort parameter\n",
    "# llm = Config().new_openai_like(reasoning_effort=\"high\")\n",
    "llm = Config().new_openai_like(\n",
    "    reasoning_effort=\"high\"  # Options: \"low\", \"medium\", \"high\"\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an experienced programmer and mathematical analyst.\"),\n",
    "        (\"user\", \"{problem}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize with reasoning_effort parameter\n",
    "# chat = ChatOpenAI(\n",
    "# model=\"o3-mini\",\"\n",
    "# reasoning_effort=\"high\" # Options: \"low\", \"medium\", \"high\"\n",
    "# )\n",
    "chain = template | llm\n",
    "\n",
    "problem = \"\"\"\n",
    "Design an algorithm to find the kth largest element in an unsorted array\n",
    "with the optimal time complexity. Analyze the time and space complexity\n",
    "of your solution and explain why it's optimal.\n",
    "\"\"\"\n",
    "response = chain.invoke({\"problem\": problem})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754da7d4",
   "metadata": {},
   "source": [
    "### Prompts and templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fec17e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment allez-vous ?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English to French translator.\"),\n",
    "        (\"user\", \"Translate this to French: {text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_messages = template.format_messages(text=\"Hello, how are you?\")\n",
    "result = chat.invoke(formatted_messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133dd75",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a58f2",
   "metadata": {},
   "source": [
    "#### Simple workflows with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4da501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a classic one for you:\n",
      "\n",
      "**Why do programmers always mix up Halloween and Christmas?**\n",
      "\n",
      "Because  \n",
      "**Oct 31 == Dec 25** üéÉüéÑ\n",
      "\n",
      "*Explanation:*  \n",
      "In octal (base-8), \"31\" equals 25 in decimal (base-10):  \n",
      "‚Çà31 = 3√ó8 + 1 = 25‚ÇÅ‚ÇÄ\n",
      "\n",
      "So‚Ä¶ it‚Äôs not a mix-up ‚Äî it‚Äôs *correct*‚Ä¶ if you‚Äôre a programmer. üòÑ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create components\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together using LCEL\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Use the chain\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a7e65",
   "metadata": {},
   "source": [
    "#### Complex chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945f9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This story‚Äôs mood is **quietly hopeful, tenderly melancholic, and ultimately warm**‚Äîa delicate balance between sorrow and renewal, isolation and connection. It unfolds like a gentle rain: slow, immersive, and emotionally resonant.\n",
      "\n",
      "### Breakdown of the Mood:\n",
      "\n",
      "#### 1. **Melancholy and Solitude (Opening)**\n",
      "The story begins with a subdued, almost mournful tone:\n",
      "- ‚Äú*The rain came like a whispered secret*‚Äù ‚Äî evokes intimacy but also secrecy, something unspoken, hidden.\n",
      "- ‚Äú*Turned the world gray*‚Äù ‚Äî visual and emotional desaturation.\n",
      "- The man in the long coat ‚Äú*didn‚Äôt look up. No one did.*‚Äù ‚Äî emphasizes emotional detachment, urban alienation.\n",
      "- Clara sits alone, touching a ‚Äú*forgotten poetry collection*‚Äù ‚Äî suggesting neglect, loneliness, perhaps grief.\n",
      "\n",
      "This establishes a mood of quiet sadness, a world washed in gray, both literally and emotionally.\n",
      "\n",
      "#### 2. **Whispered Wonder and Innocence (The Girl‚Äôs Arrival)**\n",
      "The mood shifts subtly but decisively with the girl‚Äôs appearance:\n",
      "- ‚Äú*A small hand, gloved in yellow rubber*‚Äù ‚Äî the yellow is a visual rupture in the gray, symbolizing hope, playfulness, childhood.\n",
      "- Her eyes are ‚Äú*wide and bright as polished stones*‚Äù ‚Äî purity, wonder, unfiltered perception.\n",
      "- Her question ‚Äî ‚Äú*Are you selling books?*‚Äù ‚Äî framed with ‚Äú*awe*‚Äù ‚Äî transforms the mundane into the magical.\n",
      "- She believes the books ‚Äú*were crying*‚Äù ‚Äî a child‚Äôs metaphor imbues inanimate objects with emotion, suggesting empathy and imagination.\n",
      "\n",
      "Here, the melancholy doesn‚Äôt vanish‚Äîit‚Äôs softened, illuminated by innocence. The mood becomes **poetic and tender**, as if the storm is no longer just weather, but a vessel for unseen stories.\n",
      "\n",
      "#### 3. **Warmth and Reconnection (The Reading Scene)**\n",
      "The interior shift is profound:\n",
      "- ‚Äú*They sat on the floor by the heater, wrapped in wool blankets*‚Äù ‚Äî physical warmth mirrors emotional warmth.\n",
      "- Clara reads *The Little Raindrop Who Wanted to Dance* ‚Äî the title itself is a metaphor for joy found in adversity.\n",
      "- ‚Äú*The girl giggled at every splash.*‚Äù ‚Äî pure, unburdened joy.\n",
      "\n",
      "The mood now is **intimate, nurturing, and sacred**. The bookstore, once a silent tomb of forgotten things, becomes a sanctuary. The rain outside is still present, but its power to depress is broken.\n",
      "\n",
      "#### 4. **Quiet Triumph and Renewal (Closing)**\n",
      "The ending transforms the entire emotional landscape:\n",
      "- ‚Äú*Her box now empty, her arms full of new stories*‚Äù ‚Äî reversal of loss into abundance.\n",
      "- ‚Äú*Her yellow boots splashing joy into the gray*‚Äù ‚Äî the child‚Äôs presence redefines the environment.\n",
      "- ‚Äú*The rain didn‚Äôt stop. But something else had.*‚Äù ‚Äî This is the emotional pivot. The external world remains unchanged; the internal world is transformed.\n",
      "- ‚Äú*The bookstore‚Ä¶ now smelled like paper and possibility.*‚Äù ‚Äî The scent of possibility replaces the scent of dust and silence.\n",
      "- ‚Äú*Clara didn‚Äôt mind the storm.*‚Äù ‚Äî Not because the storm is gone, but because she is no longer afraid of it. She has found meaning within it.\n",
      "\n",
      "### Overall Mood:  \n",
      "> **A gentle, luminous hope rising from quiet sorrow ‚Äî like sunlight breaking through rainclouds, not with a blaze, but with a slow, steady glow.**\n",
      "\n",
      "It is **lyrical, intimate, and deeply humane**. The mood doesn‚Äôt shout; it breathes. It finds magic in small acts of kindness, in the belief that broken things can be mended by attention, and that loneliness can be eased not by grand gestures, but by the quiet act of reading a soggy picture book to a child.\n",
      "\n",
      "### Literary Resonance:\n",
      "The mood echoes the quiet magic of **Ray Bradbury‚Äôs small-town wonder**, **Miyazaki‚Äôs gentle films**, or **Hans Christian Andersen‚Äôs bittersweet fairy tales** ‚Äî where sorrow is not erased, but transformed by love and imagination.\n",
      "\n",
      "### Final Note:\n",
      "The story‚Äôs mood is **not optimistic in a cheerful way** ‚Äî it‚Äôs **hopeful in a deeply human way**. It says: *Even when the world is gray, even when you‚Äôve forgotten how to feel, a child with yellow boots and a box of wet books can remind you that stories still matter ‚Äî and so do you.*\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "# Combine chains\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "\n",
    "# Run the combined chain\n",
    "result = story_with_analysis.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e28b77e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['topic', 'story', 'analysis'])\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "\n",
    "# Using RunnablePassthrough.assign to preserve data\n",
    "enhanced_chain = RunnablePassthrough.assign(\n",
    "    story=story_chain  # Add 'story' key with generated content\n",
    ").assign(\n",
    "    analysis=analysis_chain  # Add 'analysis' key with analysis of the story\n",
    ")\n",
    "\n",
    "result = enhanced_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys())  # Output: dict_keys(['topic', 'story', 'analysis'])\n",
    "# dict_keys(['topic', 'story', 'analysis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['story', 'topic', 'analysis'])\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "\n",
    "# Alternative approach using dictionary construction\n",
    "manual_chain = (\n",
    "    RunnablePassthrough()  # Pass through input\n",
    "    | {\n",
    "        \"story\": story_chain,  # Add story result\n",
    "        \"topic\": itemgetter(\"topic\"),  # Preserve original topic\n",
    "    }\n",
    "    | RunnablePassthrough().assign(  # Add analysis based on story\n",
    "        analysis=analysis_chain\n",
    "    )\n",
    ")\n",
    "result = manual_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys())  # Output: dict_keys(['story', 'topic', 'analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546703e",
   "metadata": {},
   "source": [
    "## Running local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68292e22",
   "metadata": {},
   "source": [
    "### Getting started with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99748b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh run deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a492658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Ollama with your chosen model\n",
    "local_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0,\n",
    "    # Â∞Ü base_url ÁöÑÂú∞ÂùÄÈÉ®ÂàÜÊõøÊç¢‰∏∫‰∏ä‰∏ÄÊ≠•ÊâìÂç∞Âá∫Êù•ÁöÑÂú∞ÂùÄ„ÄÇ\n",
    "    base_url=\"http://172.17.0.4:11434\",\n",
    ")\n",
    "# Create an LCEL chain using the local model\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "local_chain = prompt | local_llm | StrOutputParser()\n",
    "# Use the chain with your local model\n",
    "result = local_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75c495",
   "metadata": {},
   "source": [
    "### Working with Hugging Face models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d126502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-huggingface transformers torch==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Create a pipeline with a small model:\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "# Use it like any other LangChain LLM\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Explain the concept of machine learning in simple terms\"),\n",
    "]\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667494a8",
   "metadata": {},
   "source": [
    "### Tips for local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741caa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def safe_model_call(llm, prompt, max_retries=2):\n",
    "    \"\"\"Safely call a local model with retry logic and graceful failure\"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except RuntimeError as e:\n",
    "            # Common error with local models when running out of VRAM\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\n",
    "                    f\"GPU memory error, waiting and retrying ({retries+1}/{max_retries+1})\"\n",
    "                )\n",
    "                time.sleep(2)  # Give system time to free resources\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"Runtime error: {e}\")\n",
    "                return \"An error occurred while processing your request.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error calling model: {e}\")\n",
    "            return \"An error occurred while processing your request.\"\n",
    "    # If we exhausted retries\n",
    "    return \"Model is currently experiencing high load. Please try again later.\"\n",
    "\n",
    "\n",
    "# Use the safety wrapper in your LCEL chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "safe_llm = RunnableLambda(lambda x: safe_model_call(llm, x))\n",
    "safe_chain = prompt | safe_llm\n",
    "response = safe_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22f578",
   "metadata": {},
   "source": [
    "## Multimodal AI applications\n",
    "### Text-to-image\n",
    "#### Using DALL-E through OpenAI\n",
    "TODOÔºöÊâæÂà∞ÊàñÂÆûÁé∞ÊîØÊåÅ langchain Êé•Âè£ÁöÑÂçÉ‰πâÂêåÈóÆÊñáÁîüÂõæÊ®°ÂûãÔºàÂÄôÈÄâ qwen-image-plusÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515f244",
   "metadata": {},
   "source": [
    "### Image understanding\n",
    "#### Using Qwen-VL (similar to Gemini 1.5 Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "with open(\"static/stable-diffusion.png\", \"rb\") as image_file:\n",
    "    image_bytes = image_file.read()\n",
    "    base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    {\"type\": \"text\", \"text\": \"Describe the image: \"},\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_bytes}\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like_vl()\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from utils import Config\n",
    "\n",
    "\n",
    "def analyze_image(image_path: str, question: str) -> str:\n",
    "    # chat = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=256)\n",
    "    chat = Config().new_openai_like_vl(max_tokens=256)\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_bytes}\",\n",
    "                    \"detail\": \"auto\",\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = chat.invoke([message])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = \"static/skyscrapers.png\"\n",
    "questions = [\n",
    "    \"What objects do you see in this image?\",\n",
    "    \"What is the overall mood or atmosphere?\",\n",
    "    \"Are there any people in the image?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {analyze_image(image_path, question)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
