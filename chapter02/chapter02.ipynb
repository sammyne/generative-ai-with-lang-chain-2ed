{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aad480",
   "metadata": {},
   "source": [
    "# 02. First Steps with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546703e",
   "metadata": {},
   "source": [
    "# Running local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68292e22",
   "metadata": {},
   "source": [
    "## Getting started with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99748b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh run deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a492658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Ollama with your chosen model\n",
    "local_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0,\n",
    "    # å°† base_url çš„åœ°å€éƒ¨åˆ†æ›¿æ¢ä¸ºä¸Šä¸€æ­¥æ‰“å°å‡ºæ¥çš„åœ°å€ã€‚\n",
    "    base_url=\"http://172.17.0.4:11434\",\n",
    ")\n",
    "# Create an LCEL chain using the local model\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "local_chain = prompt | local_llm | StrOutputParser()\n",
    "# Use the chain with your local model\n",
    "result = local_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75c495",
   "metadata": {},
   "source": [
    "## Working with Hugging Face models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d126502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-huggingface transformers torch==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Create a pipeline with a small model:\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "# Use it like any other LangChain LLM\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Explain the concept of machine learning in simple terms\"),\n",
    "]\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "print(ai_msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667494a8",
   "metadata": {},
   "source": [
    "## Tips for local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0539e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # å‚è€ƒï¼šhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # å‚è€ƒï¼šhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI æ–‡æ¡£å‚è€ƒï¼šhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741caa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure! Letâ€™s break down **quantum computing** in simple terms â€” no physics degree required ðŸ˜Š\\n\\n---\\n\\n### ðŸŒŸ Think of a Regular Computer First:\\nA normal computer (like your laptop or phone) uses **bits** to store and process information.  \\nâž¡ï¸ A **bit** is like a light switch: it can be either **ON (1)** or **OFF (0)**.\\n\\nEverything you do â€” typing, watching videos, playing games â€” is just billions of these 1s and 0s working together.\\n\\n---\\n\\n### ðŸ’« Now, Enter Quantum Computing:\\nA **quantum computer** uses **qubits** (quantum bits) instead of regular bits.\\n\\nHereâ€™s what makes qubits special:\\n\\n#### 1. **Superposition â€” Being in Two States at Once**\\nA qubit isnâ€™t just 0 or 1 â€” it can be **both 0 and 1 at the same time** (like a spinning coin before it lands).  \\nâž¡ï¸ This means a quantum computer can explore *many possibilities at once*.\\n\\n> ðŸŽ¯ Imagine youâ€™re lost in a maze. A regular computer tries one path at a time.  \\n> A quantum computer tries *all paths at once* â€” and finds the exit way faster!\\n\\n#### 2. **Entanglement â€” Spooky Connection**\\nWhen two qubits are **entangled**, changing one instantly affects the other â€” even if theyâ€™re miles apart!  \\nâž¡ï¸ This lets quantum computers coordinate information in powerful, weird ways that normal computers canâ€™t.\\n\\n#### 3. **Interference â€” Amplifying the Right Answer**\\nQuantum computers use wave-like behavior to **cancel out wrong answers** and **boost the right ones**, like noise-canceling headphones for math.\\n\\n---\\n\\n### ðŸ” Why Is This a Big Deal?\\nQuantum computers wonâ€™t replace your laptop. But for *certain tough problems*, they could be **way, way faster**:\\n\\n- ðŸ” Breaking complex codes (for cybersecurity)\\n- ðŸ§¬ Designing new medicines by simulating molecules\\n- ðŸš— Optimizing traffic or supply chains in real time\\n- ðŸŒ Modeling climate change with super-accurate physics\\n\\n---\\n\\n### âš ï¸ Important Note:\\nQuantum computers are still **very new and fragile**. They need super-cold temperatures and careful handling.  \\nTheyâ€™re not good for everyday tasks like browsing the web or watching Netflix.\\n\\nThink of them like **super-powered scientific calculators** â€” amazing for specific, complex jobs, but not for your everyday needs.\\n\\n---\\n\\n### âœ… In One Sentence:\\n> **A quantum computer uses the weird rules of quantum physics to try many solutions at once â€” making it potentially super-fast for solving problems that would take regular computers millions of years.**\\n\\nItâ€™s like going from flipping a coin onceâ€¦ to flipping a million coins all at once â€” and knowing which one landed heads ðŸ˜„\\n\\nLet me know if you want a fun analogy with cats, dice, or pizza!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 610, 'prompt_tokens': 15, 'total_tokens': 625, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-plus-2025-09-11', 'system_fingerprint': None, 'id': 'chatcmpl-64bb0f19-7070-40a4-9e05-90fd71981a75', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--1d55101d-4fdc-43b3-9029-453b5d3aea87-0' usage_metadata={'input_tokens': 15, 'output_tokens': 610, 'total_tokens': 625, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def safe_model_call(llm, prompt, max_retries=2):\n",
    "    \"\"\"Safely call a local model with retry logic and graceful failure\"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except RuntimeError as e:\n",
    "            # Common error with local models when running out of VRAM\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\n",
    "                    f\"GPU memory error, waiting and retrying ({retries+1}/{max_retries+1})\"\n",
    "                )\n",
    "                time.sleep(2)  # Give system time to free resources\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"Runtime error: {e}\")\n",
    "                return \"An error occurred while processing your request.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error calling model: {e}\")\n",
    "            return \"An error occurred while processing your request.\"\n",
    "    # If we exhausted retries\n",
    "    return \"Model is currently experiencing high load. Please try again later.\"\n",
    "\n",
    "\n",
    "# Use the safety wrapper in your LCEL chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "safe_llm = RunnableLambda(lambda x: safe_model_call(llm, x))\n",
    "safe_chain = prompt | safe_llm\n",
    "response = safe_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
