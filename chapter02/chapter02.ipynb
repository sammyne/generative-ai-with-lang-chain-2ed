{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aad480",
   "metadata": {},
   "source": [
    "# 02. First Steps with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec867464",
   "metadata": {},
   "source": [
    "## ÂÆâË£Ö‰æùËµñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95146c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain~=1.0 langchain-core~=1.0 langchain-openai~=1.0 langgraph~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307b1a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 380ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/20.73 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--\u001b[2m--------\u001b[0m\u001b[0m 14.88 KiB/20.73 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[2m\u001b[0m\u001b[0m 20.73 KiB/20.73 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 173ms\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e75776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-core\n",
      "Version: 1.0.1\n",
      "Location: /github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter02/.venv/lib/python3.12/site-packages\n",
      "Requires: jsonpatch, langsmith, packaging, pydantic, pyyaml, tenacity, typing-extensions\n",
      "Required-by: langchain, langchain-openai, langgraph, langgraph-checkpoint, langgraph-prebuilt\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip show langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc7aa2",
   "metadata": {},
   "source": [
    "Â∑•ÂÖ∑Á±ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829f7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # ÂèÇËÄÉÔºöhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # ÂèÇËÄÉÔºöhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI ÊñáÊ°£ÂèÇËÄÉÔºöhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_coder(self, **kwargs) -> ChatOpenAI:\n",
    "        model = os.environ[\"OPENAI_CODER_MODEL\"]\n",
    "        # ÂèÇËÄÉÔºöhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # ÂèÇËÄÉÔºöhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI ÊñáÊ°£ÂèÇËÄÉÔºöhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_vl(self, **kwargs) -> ChatOpenAI:\n",
    "        if not self.vl_model:\n",
    "            raise ValueError(\"OPENAI_VL_MODEL is not set\")\n",
    "\n",
    "        # ÂèÇËÄÉÔºöhttps://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # ÂèÇËÄÉÔºöhttps://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI ÊñáÊ°£ÂèÇËÄÉÔºöhttps://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.vl_model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7214c31",
   "metadata": {},
   "source": [
    "## Exploring LangChain‚Äôs building blocks\n",
    "### Model interfaces\n",
    "#### LLM interaction patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68b1047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a classic with a twist:\n",
      "\n",
      "**How many programmers does it take to change a light bulb?**  \n",
      "*None‚Äîit‚Äôs a hardware problem!* üí°\n",
      "\n",
      "Or if you prefer a punny one:  \n",
      "**Why did the light bulb go to therapy?**  \n",
      "*Because it had deep-seated issues‚Ä¶ and it was feeling a little dim!* üòÑ\n",
      "\n",
      "Want another? I‚Äôve got watts more!\n"
     ]
    }
   ],
   "source": [
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Both can be used with the same interface\n",
    "response = llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02175e00",
   "metadata": {},
   "source": [
    "#### Development testing (TODO: Á≠âÂæÖ langchain-community v1.0.0 Ê≠£ÂºèÂèëÂ∏É)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.llms import FakeListLLM\n",
    "\n",
    "# # Create a fake LLM that always returns the same responses\n",
    "# fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "\n",
    "# result = fake_llm.invoke(\"Any input will return Hello\")\n",
    "# print(result)  # Output: Hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b32849",
   "metadata": {},
   "source": [
    "#### Working with chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab261f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few different ways to write a factorial function in Python:\n",
      "\n",
      "## Method 1: Recursive approach\n",
      "```python\n",
      "def factorial_recursive(n):\n",
      "    \"\"\"\n",
      "    Calculate factorial using recursion.\n",
      "    \n",
      "    Args:\n",
      "        n (int): Non-negative integer\n",
      "    \n",
      "    Returns:\n",
      "        int: Factorial of n\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    return n * factorial_recursive(n - 1)\n",
      "```\n",
      "\n",
      "## Method 2: Iterative approach (recommended)\n",
      "```python\n",
      "def factorial_iterative(n):\n",
      "    \"\"\"\n",
      "    Calculate factorial using iteration.\n",
      "    \n",
      "    Args:\n",
      "        n (int): Non-negative integer\n",
      "    \n",
      "    Returns:\n",
      "        int: Factorial of n\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "    \n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "```\n",
      "\n",
      "## Method 3: Using math.factorial (built-in)\n",
      "```python\n",
      "import math\n",
      "\n",
      "def factorial_builtin(n):\n",
      "    \"\"\"\n",
      "    Calculate factorial using built-in math.factorial.\n",
      "    \n",
      "    Args:\n",
      "        n (int): Non-negative integer\n",
      "    \n",
      "    Returns:\n",
      "        int: Factorial of n\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "    return math.factorial(n)\n",
      "```\n",
      "\n",
      "## Example usage:\n",
      "```python\n",
      "# Test the functions\n",
      "print(factorial_iterative(5))  # Output: 120\n",
      "print(factorial_recursive(5))  # Output: 120\n",
      "print(factorial_builtin(5))    # Output: 120\n",
      "\n",
      "# Edge cases\n",
      "print(factorial_iterative(0))  # Output: 1\n",
      "print(factorial_iterative(1))  # Output: 1\n",
      "```\n",
      "\n",
      "**Recommendation:** Use the iterative approach for better performance and to avoid potential stack overflow issues with large numbers. The recursive approach is more intuitive but less efficient for large values of `n`.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like_coder()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful programming assistant\"),\n",
    "    HumanMessage(content=\"Write a Python function to calculate factorial\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a3869",
   "metadata": {},
   "source": [
    "#### Reasoning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429e1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll design an algorithm to find the kth largest element using the QuickSelect algorithm, which provides optimal average-case performance.\n",
      "\n",
      "## Algorithm: QuickSelect (Partition-based Selection)\n",
      "\n",
      "```python\n",
      "def find_kth_largest(nums, k):\n",
      "    \"\"\"\n",
      "    Find the kth largest element in an unsorted array.\n",
      "    \n",
      "    Args:\n",
      "        nums: List of integers\n",
      "        k: Integer representing kth largest position (1-indexed)\n",
      "    \n",
      "    Returns:\n",
      "        The kth largest element\n",
      "    \"\"\"\n",
      "    def quickselect(left, right, k_smallest):\n",
      "        \"\"\"\n",
      "        Find the k_smallest-th smallest element (0-indexed).\n",
      "        \"\"\"\n",
      "        if left == right:\n",
      "            return nums[left]\n",
      "        \n",
      "        # Choose a random pivot to avoid worst-case on sorted arrays\n",
      "        import random\n",
      "        pivot_index = random.randint(left, right)\n",
      "        pivot_index = partition(left, right, pivot_index)\n",
      "        \n",
      "        if k_smallest == pivot_index:\n",
      "            return nums[k_smallest]\n",
      "        elif k_smallest < pivot_index:\n",
      "            return quickselect(left, pivot_index - 1, k_smallest)\n",
      "        else:\n",
      "            return quickselect(pivot_index + 1, right, k_smallest)\n",
      "    \n",
      "    def partition(left, right, pivot_index):\n",
      "        \"\"\"\n",
      "        Partition the array around the pivot element.\n",
      "        Elements larger than pivot go to the left,\n",
      "        elements smaller than pivot go to the right.\n",
      "        \"\"\"\n",
      "        pivot = nums[pivot_index]\n",
      "        # Move pivot to end\n",
      "        nums[pivot_index], nums[right] = nums[right], nums[pivot_index]\n",
      "        \n",
      "        store_index = left\n",
      "        for i in range(left, right):\n",
      "            if nums[i] > pivot:  # For kth largest, we want larger elements first\n",
      "                nums[store_index], nums[i] = nums[i], nums[store_index]\n",
      "                store_index += 1\n",
      "        \n",
      "        # Move pivot to its final place\n",
      "        nums[right], nums[store_index] = nums[store_index], nums[right]\n",
      "        return store_index\n",
      "    \n",
      "    # Convert kth largest to kth smallest indexing\n",
      "    # kth largest = (n-k)th smallest (0-indexed)\n",
      "    n = len(nums)\n",
      "    return quickselect(0, n - 1, n - k)\n",
      "\n",
      "# Alternative implementation with median-of-medians for guaranteed O(n) worst case\n",
      "def find_kth_largest_medians(nums, k):\n",
      "    \"\"\"\n",
      "    Using median-of-medians approach for guaranteed O(n) worst-case time.\n",
      "    \"\"\"\n",
      "    def median_of_medians(arr, left, right):\n",
      "        \"\"\"Find median of medians for pivot selection.\"\"\"\n",
      "        if right - left < 5:\n",
      "            # Sort small subarray and return median\n",
      "            subarray = sorted(arr[left:right+1])\n",
      "            return subarray[len(subarray)//2]\n",
      "        \n",
      "        medians = []\n",
      "        for i in range(left, right + 1, 5):\n",
      "            end = min(i + 4, right)\n",
      "            median_5 = sorted(arr[i:end+1])[len(arr[i:end+1])//2]\n",
      "            medians.append(median_5)\n",
      "        \n",
      "        return quickselect_median(medians, 0, len(medians) - 1, len(medians) // 2)\n",
      "    \n",
      "    def quickselect_median(left, right, k):\n",
      "        if left == right:\n",
      "            return nums[left]\n",
      "        \n",
      "        # Use median-of-medians to get good pivot\n",
      "        pivot = median_of_medians(nums, left, right)\n",
      "        pivot_index = nums.index(pivot, left, right + 1)\n",
      "        \n",
      "        pivot_index = partition(left, right, pivot_index)\n",
      "        \n",
      "        if k == pivot_index:\n",
      "            return nums[k]\n",
      "        elif k < pivot_index:\n",
      "            return quickselect_median(left, pivot_index - 1, k)\n",
      "        else:\n",
      "            return quickselect_median(pivot_index + 1, right, k)\n",
      "    \n",
      "    n = len(nums)\n",
      "    return quickselect_median(0, n - 1, n - k)\n",
      "```\n",
      "\n",
      "## Complexity Analysis\n",
      "\n",
      "### Time Complexity:\n",
      "- **Average Case: O(n)** - Each recursive call processes roughly half the remaining elements\n",
      "- **Best Case: O(n)** - Lucky pivot selections\n",
      "- **Worst Case: O(n¬≤)** for basic QuickSelect - when pivot is always the smallest/largest element\n",
      "- **Worst Case: O(n)** for median-of-medians variant - guaranteed good pivot\n",
      "\n",
      "### Space Complexity:\n",
      "- **O(log n)** average case - recursion depth\n",
      "- **O(n)** worst case - recursion depth in basic QuickSelect\n",
      "- **O(log n)** for median-of-medians - due to recursive median finding\n",
      "\n",
      "## Why This Algorithm is Optimal\n",
      "\n",
      "### 1. **Information-Theoretic Lower Bound**\n",
      "- To find the kth largest element, we need to examine at least some elements to make a decision\n",
      "- Any comparison-based algorithm requires Œ©(n) time in the worst case\n",
      "- We must potentially examine all elements to verify our answer\n",
      "\n",
      "### 2. **Average-Case Optimality**\n",
      "- QuickSelect achieves O(n) average time, matching the theoretical lower bound\n",
      "- It's more efficient than sorting (O(n log n)) when we only need one element\n",
      "\n",
      "### 3. **Practical Efficiency**\n",
      "- Better constant factors than sorting approaches\n",
      "- In-place algorithm (modifies input array)\n",
      "- Cache-friendly due to partitioning strategy\n",
      "\n",
      "## Comparison with Alternative Approaches\n",
      "\n",
      "| Approach | Time Complexity | Space Complexity | Notes |\n",
      "|----------|----------------|------------------|-------|\n",
      "| **QuickSelect** | O(n) avg, O(n¬≤) worst | O(log n) avg | Practical choice |\n",
      "| **Median-of-Medians** | O(n) guaranteed | O(log n) | Theoretically optimal |\n",
      "| **Sorting** | O(n log n) | O(1) or O(n) | Overkill for single element |\n",
      "| **Min-Heap of size k** | O(n log k) | O(k) | Good when k is small |\n",
      "| **Built-in functions** | O(n log n) | O(1) | Usually sort-based |\n",
      "\n",
      "## Example Usage\n",
      "\n",
      "```python\n",
      "# Test the algorithm\n",
      "nums1 = [3, 2, 1, 5, 6, 4]\n",
      "k1 = 2\n",
      "result1 = find_kth_largest(nums1.copy(), k1)\n",
      "print(f\"k={k1} largest in {nums1}: {result1}\")  # Output: 5\n",
      "\n",
      "nums2 = [3, 2, 3, 1, 2, 4, 5, 5, 6]\n",
      "k2 = 4\n",
      "result2 = find_kth_largest(nums2.copy(), k2)\n",
      "print(f\"k={k2} largest in {nums2}: {result2}\")  # Output: 4\n",
      "```\n",
      "\n",
      "The QuickSelect algorithm provides the optimal average-case performance for this problem, making it the preferred solution in most practical scenarios.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# initialize OpenAI-like model with reasoning_effort parameter\n",
    "llm = Config().new_openai_like_coder(\n",
    "    reasoning_effort=\"high\"  # Options: \"low\", \"medium\", \"high\"\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an experienced programmer and mathematical analyst.\"),\n",
    "        (\"user\", \"{problem}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "problem = \"\"\"\n",
    "Design an algorithm to find the kth largest element in an unsorted array\n",
    "with the optimal time complexity. Analyze the time and space complexity\n",
    "of your solution and explain why it's optimal.\n",
    "\"\"\"\n",
    "response = chain.invoke({\"problem\": problem})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754da7d4",
   "metadata": {},
   "source": [
    "### Prompts and templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fec17e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment allez-vous ?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English to French translator.\"),\n",
    "        (\"user\", \"Translate this to French: {text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_messages = template.format_messages(text=\"Hello, how are you?\")\n",
    "result = chat.invoke(formatted_messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133dd75",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a58f2",
   "metadata": {},
   "source": [
    "#### Simple workflows with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4da501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs! üêõüíª\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create components\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together using LCEL\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Use the chain\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a7e65",
   "metadata": {},
   "source": [
    "#### Complex chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945f9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mood of this story evolves significantly, moving from **melancholy and introspection** to **quiet hope and renewal**, ultimately settling into a tone of **peaceful acceptance and gentle optimism**.\n",
      "\n",
      "### Initial Mood: Melancholy and Isolation  \n",
      "The opening establishes a somber, almost oppressive atmosphere. The rain doesn‚Äôt ‚Äúfall‚Äù but ‚Äúsettles‚Äù‚Äîa verb that conveys weight, stagnation, and quiet suffocation. Descriptions like ‚Äúcool, grey wool blanket wrapped tight around the world,‚Äù ‚Äúuntouched coffee,‚Äù and Arthur‚Äôs sense of being ‚Äúdissolved‚Äù evoke emotional numbness, loneliness, and resignation. The world outside is muted‚Äî‚Äúwatercolor blur,‚Äù ‚Äúmuffled symphony,‚Äù ‚Äúdark mirrors‚Äù‚Äîreinforcing a sense of detachment and internal stagnation. Arthur is physically present but emotionally adrift, viewing life as a series of disconnected disappointments.\n",
      "\n",
      "### Turning Point: Sudden Joy and Contrast  \n",
      "The arrival of the child in the yellow raincoat introduces a dramatic shift. Her laughter is described as cutting through the rain ‚Äúlike a bell‚Äù‚Äîa sharp, clear sound that disrupts the prevailing gloom. Her uninhibited joy‚Äîdancing in puddles, embracing the rain‚Äîstands in stark contrast to Arthur‚Äôs passivity. This moment injects **vibrancy, spontaneity, and innocence** into the narrative, acting as a catalyst for emotional change.\n",
      "\n",
      "### Final Mood: Quiet Renewal and Presence  \n",
      "Arthur‚Äôs internal transformation is subtle but profound. The warmth that ‚Äúbloomed in his chest‚Äù signals reawakening. Though the external conditions haven‚Äôt changed‚Äîthe rain continues, the sky remains heavy‚Äîhis perception shifts. The grey now feels ‚Äúsofter,‚Äù ‚Äúfull of possibility.‚Äù His decision to walk out into the rain without an umbrella symbolizes willingness to engage with life again, imperfections and all. The closing lines emphasize **presence, connection, and appreciation for the ordinary**: he feels ‚Äúhere,‚Äù part of the ‚Äúwet, wonderful, ordinary miracle.‚Äù\n",
      "\n",
      "### Overall Mood Arc  \n",
      "- **Beginning**: Heavy, introspective, melancholic  \n",
      "- **Middle**: Disruptive joy, contrast, awakening  \n",
      "- **End**: Hopeful, serene, gently uplifting  \n",
      "\n",
      "The story masterfully uses sensory details (sound of rain, warmth of coffee, visual of yellow raincoat) and metaphor (raindrops as fragmented life, rain as cleansing) to trace an emotional journey from isolation to reconnection. The final mood isn‚Äôt exuberant‚Äîit‚Äôs **contemplative yet warm**, suggesting that healing often comes not through grand gestures, but through small moments of presence and the courage to step into the rain.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "# Combine chains\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "\n",
    "# Run the combined chain\n",
    "result = story_with_analysis.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28b77e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['topic', 'story', 'analysis'])\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "\n",
    "# Using RunnablePassthrough.assign to preserve data\n",
    "enhanced_chain = RunnablePassthrough.assign(\n",
    "    story=story_chain  # Add 'story' key with generated content\n",
    ").assign(\n",
    "    analysis=analysis_chain  # Add 'analysis' key with analysis of the story\n",
    ")\n",
    "\n",
    "result = enhanced_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys())  # Output: dict_keys(['topic', 'story', 'analysis'])\n",
    "# dict_keys(['topic', 'story', 'analysis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836a60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['story', 'topic', 'analysis'])\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "\n",
    "# Alternative approach using dictionary construction\n",
    "manual_chain = (\n",
    "    RunnablePassthrough()  # Pass through input\n",
    "    | {\n",
    "        \"story\": story_chain,  # Add story result\n",
    "        \"topic\": itemgetter(\"topic\"),  # Preserve original topic\n",
    "    }\n",
    "    | RunnablePassthrough().assign(  # Add analysis based on story\n",
    "        analysis=analysis_chain\n",
    "    )\n",
    ")\n",
    "result = manual_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys())  # Output: dict_keys(['story', 'topic', 'analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546703e",
   "metadata": {},
   "source": [
    "## Running local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68292e22",
   "metadata": {},
   "source": [
    "### Getting started with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99748b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh run deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e7c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-ollama~=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a492658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain quantum computing in simple terms. Hmm, where do I start? I remember from school that computers use bits, which are either 0 or 1. But wait, quantum computing uses something called qubits instead of regular bits. Maybe it's like a superposition thingy?\n",
      "\n",
      "Let me think... Quantum bits can be both 0 and 1 at the same time because they're in a state of superposition. That means when you measure them, you don't get just one result; there's a chance they could be 0 or 1 simultaneously. So that's different from classical computers where each bit is either 0 or 1.\n",
      "\n",
      "But how does this help with computing? Oh right, quantum computers can process multiple possibilities at once because of superposition and something called entanglement. Entanglement means that the state of one qubit is linked to another, so changing one affects the other instantly. That could mean solving problems faster by considering all possible solutions simultaneously.\n",
      "\n",
      "Wait, but how does this translate into actual computation? I think it's about using these quantum states to perform operations on a vast number of possibilities at once. So instead of taking a long time to check each possibility one by one, you can do them all in parallel. That would make tasks like factoring large numbers or simulating complex systems much quicker.\n",
      "\n",
      "But then there are other factors too. Quantum computers require qubits that are entangled and have the right properties. Also, quantum interference is important because it can amplify certain outcomes while canceling others. So you need to carefully control these aspects to get the desired results.\n",
      "\n",
      "I'm a bit fuzzy on how exactly the qubits work together. Maybe I should think of an example. Like, if you have two qubits, they could represent four different states: 00, 01, 10, and 11. So processing all these at once instead of just one or two.\n",
      "\n",
      "But wait, how does the actual computation happen? Do you apply operations to each state simultaneously, and then measure them all together? That seems a bit abstract. Maybe it's similar to how classical computers use binary numbers but in parallel for different parts of the problem.\n",
      "\n",
      "I also remember something about quantum algorithms like Shor's algorithm for factoring and Grover's algorithm for searching. These are supposed to work faster than their classical counterparts because they leverage the principles of quantum computing. But I'm not entirely sure how that works, especially with qubits and entanglement.\n",
      "\n",
      "So putting it all together: Quantum computers use qubits which can be superpositions of 0 and 1, and entangled qubits allow for parallel processing. This leads to solving problems faster than classical computers by considering multiple possibilities at once. But I need to make sure my explanation is simple enough without getting too technical.\n",
      "\n",
      "Maybe I should structure it like this: Start with the basics of bits vs. qubits, explain superposition and entanglement, then talk about how quantum computing processes information in parallel, and finally mention some examples or algorithms that demonstrate its potential.\n",
      "\n",
      "I think I have a rough idea now. Let me try to outline it step by step.\n",
      "</think>\n",
      "\n",
      "Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Unlike classical computers, which use bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously thanks to the principles of superposition and entanglement.\n",
      "\n",
      "Here's a simple explanation:\n",
      "\n",
      "1. **Bits vs. Qubits**: Classical computers use bits, which are either 0 or 1. Quantum computers use qubits, which can be both 0 and 1 at the same time due to superposition. This means each qubit has two states instead of one.\n",
      "\n",
      "2. **Superposition**: When a qubit is in a superposition state, it can represent multiple computational paths simultaneously. For example, a single qubit can represent four different states (00, 01, 10, 11) when combined with another qubit.\n",
      "\n",
      "3. **Entanglement**: This phenomenon links the states of two or more qubits, allowing their states to be correlated in ways that classical bits cannot. This enables quantum computers to process a vast number of possibilities at once.\n",
      "\n",
      "4. **Parallel Processing**: Quantum computers can evaluate multiple possibilities simultaneously because they are in superposition. This allows them to solve complex problems faster than classical computers by considering all possible solutions at the same time.\n",
      "\n",
      "5. **Quantum Algorithms**: Certain algorithms, like Shor's algorithm for factoring large numbers and Grover's algorithm for searching unsorted data, leverage quantum principles to achieve significant speedups over classical methods.\n",
      "\n",
      "In summary, quantum computing uses qubits with superposition and entanglement to process information in parallel, offering potential solutions to problems that are intractable for classical computers.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Ollama with your chosen model\n",
    "local_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0,\n",
    "    # Â∞Ü base_url ÁöÑÂú∞ÂùÄÈÉ®ÂàÜÊõøÊç¢‰∏∫‰∏ä‰∏ÄÊ≠•ÊâìÂç∞Âá∫Êù•ÁöÑÂú∞ÂùÄ„ÄÇ\n",
    "    base_url=\"http://172.17.0.3:11434\",\n",
    ")\n",
    "# Create an LCEL chain using the local model\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "local_chain = prompt | local_llm | StrOutputParser()\n",
    "# Use the chain with your local model\n",
    "result = local_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75c495",
   "metadata": {},
   "source": [
    "### Working with Hugging Face models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d126502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m58 packages\u001b[0m \u001b[2min 377ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---------------\u001b[0m\u001b[0m     0 B/26.85 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)[2m------------\u001b[0m\u001b[0m 16.00 KiB/26.85 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----------\u001b[2m\u001b[0m\u001b[0m 26.85 KiB/26.85 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 173ms\u001b[0m\u001b[0m                                                  \u001b[1A\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/31] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m31 packages\u001b[0m \u001b[2min 26.24s\u001b[0m\u001b[0m                             \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-huggingface\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install langchain-huggingface~=1.0 transformers~=4.57 torch==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd7e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/github.com/sammyne/generative-ai-with-lang-chain-2ed/chapter02/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You're a helpful assistant</s>\n",
      "<|user|>\n",
      "Explain the concept of machine learning in simple terms</s>\n",
      "<|assistant|>\n",
      "Machine learning is a field that involves the use of algorithms and data to learn from and improve upon the behavior of machines or systems. It is a powerful tool for automating tasks and improving the performance of systems by learning from data and making predictions based on that data.\n",
      "\n",
      "In simple terms, machine learning is the process of using algorithms to analyze and learn from data. This process involves collecting large amounts of data, training the algorithm to identify patterns and relationships in that data, and then using that knowledge to make predictions or make decisions.\n",
      "\n",
      "For example, a machine learning algorithm might be used to predict the likelihood of a customer purchasing a particular product based on their previous purchase history and other factors. Or it might be used to recommend products to a user based on their browsing and purchase history.\n",
      "\n",
      "Machine learning can be applied to a wide range of applications, from financial services to healthcare to transportation. It has the potential to revolutionize many industries by improving efficiency, reducing costs, and enabling new levels of personalization and customization.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Create a pipeline with a small model:\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Use it like any other LangChain LLM\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Explain the concept of machine learning in simple terms\"),\n",
    "]\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667494a8",
   "metadata": {},
   "source": [
    "### Tips for local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741caa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def safe_model_call(llm, prompt, max_retries=2):\n",
    "    \"\"\"Safely call a local model with retry logic and graceful failure\"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except RuntimeError as e:\n",
    "            # Common error with local models when running out of VRAM\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\n",
    "                    f\"GPU memory error, waiting and retrying ({retries+1}/{max_retries+1})\"\n",
    "                )\n",
    "                time.sleep(2)  # Give system time to free resources\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"Runtime error: {e}\")\n",
    "                return \"An error occurred while processing your request.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error calling model: {e}\")\n",
    "            return \"An error occurred while processing your request.\"\n",
    "    # If we exhausted retries\n",
    "    return \"Model is currently experiencing high load. Please try again later.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4bba045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure! Let‚Äôs break down quantum computing in simple terms:\\n\\n### What is a regular (classical) computer?\\nA regular computer‚Äîlike your laptop or phone‚Äîuses **bits** to process information. A bit can be either a **0** or a **1**. Everything you do on a computer‚Äîwatching videos, sending messages, playing games‚Äîis built from combinations of these 0s and 1s.\\n\\n### What‚Äôs different about a quantum computer?\\nA quantum computer uses **quantum bits**, or **qubits**. Qubits are special because they can be **0, 1, or both at the same time**! This is thanks to a weird but real feature of quantum physics called **superposition**.\\n\\n#### Think of it like this:\\n- A regular bit is like a light switch: it‚Äôs either **ON (1)** or **OFF (0)**.\\n- A qubit is like a spinning coin: while it‚Äôs spinning, it‚Äôs kind of **both heads and tails at once**. Only when you stop it (measure it) does it ‚Äúchoose‚Äù one side.\\n\\n### Another quantum trick: Entanglement\\nQubits can also be **entangled**. That means two qubits can be linked so that whatever happens to one instantly affects the other‚Äîeven if they‚Äôre far apart! Einstein called this ‚Äúspooky action at a distance.‚Äù This lets quantum computers process information in ways that classical computers can‚Äôt.\\n\\n### Why does this matter?\\nBecause qubits can explore many possibilities at once, a quantum computer can solve certain problems **much faster** than a regular computer. For example:\\n- **Breaking complex codes** (which is why cybersecurity is paying attention)\\n- **Simulating molecules** for new medicines or materials\\n- **Optimizing complex systems**, like traffic flow or financial portfolios\\n\\n### But‚Ä¶ it‚Äôs not magic\\nQuantum computers **won‚Äôt replace** your laptop. They‚Äôre only better at specific, very complex tasks. Also, they‚Äôre extremely hard to build and operate‚Äîthey need super-cold temperatures and are very sensitive to errors.\\n\\n### In a nutshell:\\n> **Quantum computing uses the strange rules of quantum physics to process information in powerful new ways‚Äîby letting bits be 0 and 1 at the same time and linking them together in spooky ways.**\\n\\nIt‚Äôs still early days, but scientists believe it could revolutionize fields like medicine, cryptography, and artificial intelligence in the future!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 499, 'prompt_tokens': 15, 'total_tokens': 514, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3-max-2025-09-23', 'system_fingerprint': None, 'id': 'chatcmpl-e4af4734-abef-412c-9b74-11f83f14e827', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--5692f20f-f0e1-4ac3-9c64-1c6adba193d8-0' usage_metadata={'input_tokens': 15, 'output_tokens': 499, 'total_tokens': 514, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# Use the safety wrapper in your LCEL chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "safe_llm = RunnableLambda(lambda x: safe_model_call(llm, x))\n",
    "safe_chain = prompt | safe_llm\n",
    "response = safe_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22f578",
   "metadata": {},
   "source": [
    "## Multimodal AI applications\n",
    "### Text-to-image\n",
    "#### Using DALL-E through OpenAI\n",
    "TODOÔºöÊâæÂà∞ÊàñÂÆûÁé∞ÊîØÊåÅ langchain Êé•Âè£ÁöÑÂçÉ‰πâÈÄöÈóÆÊñáÁîüÂõæÊ®°ÂûãÔºàÂÄôÈÄâ qwen-image-plusÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515f244",
   "metadata": {},
   "source": [
    "### Image understanding\n",
    "#### Using Qwen-VL (similar to Gemini 1.5 Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eae3dfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a highly detailed, futuristic digital illustration of a humanoid robot or android, presented in a side-profile view against a high-tech interface background.\n",
      "\n",
      "**The Robot:**\n",
      "- **Design:** The robot has a sleek, feminine form with smooth, glossy white armor plating over its head, shoulders, and chest. The design is elegant yet mechanical.\n",
      "- **Head:** Its face is dark, almost black, with glowing golden-yellow eyes that give it an intense, intelligent gaze. The helmet is partially open on the sides, revealing complex internal circuitry and glowing orange lights within the neck and jaw area.\n",
      "- **Exposed Mechanics:** The neck and upper torso are partially exposed, showcasing intricate networks of wires, tubes, and glowing components, suggesting advanced internal systems. Small, warm orange lights dot the joints and seams of the armor.\n",
      "- **Shoulders & Chest:** The shoulder plates are large and curved, while the chest plate is broad and segmented, all contributing to a powerful yet graceful silhouette.\n",
      "\n",
      "**Background:**\n",
      "- The robot is set against a deep blue, digital HUD (Heads-Up Display) filled with glowing cyan and light blue interfaces.\n",
      "- The background contains numerous floating panels displaying graphs, charts, circular gauges, data readouts, and abstract symbols ‚Äî evoking a sci-fi control room or neural network visualization.\n",
      "- Thin, glowing lines connect various elements, reinforcing the theme of connectivity and data flow.\n",
      "\n",
      "**Color Palette & Atmosphere:**\n",
      "- Dominated by cool blues and cyans, contrasted with warm golds and oranges from the robot‚Äôs internal lights and eyes.\n",
      "- The lighting creates a dramatic, cinematic effect, highlighting the robot‚Äôs contours and the depth of the digital space.\n",
      "- Overall mood: futuristic, sophisticated, intelligent, and slightly mysterious ‚Äî blending organic elegance with cold technology.\n",
      "\n",
      "This image powerfully visualizes themes of artificial intelligence, human-machine integration, and advanced technology ‚Äî ideal for sci-fi concepts, AI branding, or tech-related media.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "with open(\"static/stable-diffusion.png\", \"rb\") as image_file:\n",
    "    image_bytes = image_file.read()\n",
    "    base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    {\"type\": \"text\", \"text\": \"Describe the image: \"},\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_bytes}\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like_vl()\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ae00fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What objects do you see in this image?\n",
      "A: Based on the image provided, here are the objects and key visual elements I can identify:\n",
      "\n",
      "-   **Futuristic Cityscape:** The dominant feature is a dense skyline of very tall, modern skyscrapers. Their design is sleek and geometric, with many appearing as glass or reflective structures.\n",
      "-   **Sun / Light Source:** A bright, stylized sun is positioned in the upper center of the sky, casting a strong glare and lens flare effect.\n",
      "-   **Sky:** The background is a clear, vibrant blue sky, suggesting either dawn or dusk due to the intense light.\n",
      "-   **Road / Highway:** In the foreground, there is a multi-lane road or highway stretching towards the horizon and the city.\n",
      "-   **Light Trails (Motion Blur):** The most prominent feature on the road is long, streaking lines of light. These represent the motion blur of vehicle lights captured over time:\n",
      "    -   **Red Streaks:** Likely from the taillights of cars moving away from the viewer.\n",
      "    -   **White/Blue Streaks:** Likely from the headlights of cars moving towards the viewer.\n",
      "-   **Reflections:** The surface of the road appears wet or highly polished, reflecting the lights from the buildings and the streaks of traffic.\n",
      "-   **\"ai\" Watermark:** In the bottom right corner, there is a small, circular watermark with the letters \"ai\", indicating the image was likely generated by artificial intelligence.\n",
      "\n",
      "The overall scene depicts a dynamic, high-speed journey into a technologically advanced, futuristic metropolis.\n",
      "\n",
      "Q: What is the overall mood or atmosphere?\n",
      "A: The overall mood or atmosphere of this image is **futuristic, dynamic, and optimistic**.\n",
      "\n",
      "Here‚Äôs a breakdown of why:\n",
      "\n",
      "- **Futuristic / Sci-Fi**: The sleek, glowing skyscrapers with translucent blue panels and the hyper-stylized light trails suggest a technologically advanced, possibly utopian city. The architecture feels imagined rather than real ‚Äî clean, towering, and otherworldly.\n",
      "\n",
      "- **Dynamic / Energetic**: The streaks of red and white light on the road create a powerful sense of motion and speed, as if you‚Äôre racing toward the horizon. This conveys energy, progress, and forward momentum.\n",
      "\n",
      "- **Optimistic / Hopeful**: The bright sun shining at the center of the skyline acts as a focal point of hope and enlightenment. The clear blue sky and radiant lighting give the scene an uplifting, aspirational tone ‚Äî suggesting a bright future ahead.\n",
      "\n",
      "- **Surreal / Dreamlike**: The combination of impossible architecture, glowing surfaces, and exaggerated light effects gives the image a dreamy, almost cyberpunk-meets-utopia aesthetic. It feels like a vision of tomorrow ‚Äî not gritty or dystopian, but polished and idealized.\n",
      "\n",
      "In summary:  \n",
      "> **A vibrant, high-tech metropolis bathed in sunlight, radiating energy, progress, and a hopeful vision of the future.**\n",
      "\n",
      "It‚Äôs the kind of image that could serve as a backdrop for a sci-fi movie, a tech company‚Äôs vision statement, or a concept art piece for a futuristic city.\n",
      "\n",
      "Q: Are there any people in the image?\n",
      "A: No, there are no people visible in the image.\n",
      "\n",
      "The image depicts a futuristic, stylized cityscape with tall skyscrapers and light trails on a road or highway, suggesting fast-moving vehicles. The scene is dominated by architecture, light effects, and a bright sun in the sky ‚Äî but no human figures, pedestrians, or drivers can be seen.\n",
      "\n",
      "It‚Äôs an abstract or AI-generated representation of a high-tech urban environment, not a photograph of a real place with people present.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def analyze_image(image_path: str, question: str) -> str:\n",
    "    chat = Config().new_openai_like_vl(max_tokens=256)\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_bytes}\",\n",
    "                    \"detail\": \"auto\",\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = chat.invoke([message])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = \"static/skyscrapers.png\"\n",
    "questions = [\n",
    "    \"What objects do you see in this image?\",\n",
    "    \"What is the overall mood or atmosphere?\",\n",
    "    \"Are there any people in the image?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {analyze_image(image_path, question)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter02 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
