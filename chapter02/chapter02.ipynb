{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6aad480",
   "metadata": {},
   "source": [
    "# 02. First Steps with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec867464",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95146c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain~=0.3 langchain-core~=0.3 langchain-community~=0.3 langchain-openai~=0.3 langgraph~=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install python-dotenv~=1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc7aa2",
   "metadata": {},
   "source": [
    "工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # By default, load_dotenv doesn't override existing environment variables and looks for a .env file in same directory as python script or searches for it incrementally higher up.\n",
    "        dotenv_path = dotenv.find_dotenv(usecwd=True)\n",
    "        if not dotenv_path:\n",
    "            raise ValueError(\"No .env file found\")\n",
    "        dotenv.load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_API_BASE_URL\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"OPENAI_API_BASE_URL is not set\")\n",
    "\n",
    "        model = os.getenv(\"OPENAI_MODEL\")\n",
    "        if not model:\n",
    "            raise ValueError(\"OPENAI_MODEL is not set\")\n",
    "\n",
    "        vl_model = os.getenv(\"OPENAI_VL_MODEL\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.vl_model = vl_model\n",
    "\n",
    "    def new_openai_like(self, **kwargs) -> ChatOpenAI:\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_coder(self, **kwargs) -> ChatOpenAI:\n",
    "        model = os.environ[\"OPENAI_CODER_MODEL\"]\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=model, **kwargs\n",
    "        )\n",
    "\n",
    "    def new_openai_like_vl(self, **kwargs) -> ChatOpenAI:\n",
    "        if not self.vl_model:\n",
    "            raise ValueError(\"OPENAI_VL_MODEL is not set\")\n",
    "\n",
    "        # 参考：https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2587654\n",
    "        # 参考：https://help.aliyun.com/zh/model-studio/models\n",
    "        # ChatOpenAI 文档参考：https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            api_key=self.api_key, base_url=self.base_url, model=self.vl_model, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7214c31",
   "metadata": {},
   "source": [
    "## Exploring LangChain’s building blocks\n",
    "### Model interfaces\n",
    "#### LLM interaction patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "# Both can be used with the same interface\n",
    "response = llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02175e00",
   "metadata": {},
   "source": [
    "#### Development testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import FakeListLLM\n",
    "\n",
    "# Create a fake LLM that always returns the same responses\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "\n",
    "result = fake_llm.invoke(\"Any input will return Hello\")\n",
    "print(result)  # Output: Hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b32849",
   "metadata": {},
   "source": [
    "#### Working with chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab261f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like_coder()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful programming assistant\"),\n",
    "    HumanMessage(content=\"Write a Python function to calculate factorial\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a3869",
   "metadata": {},
   "source": [
    "#### Reasoning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# initialize OpenAI-like model with reasoning_effort parameter\n",
    "llm = Config().new_openai_like_coder(\n",
    "    reasoning_effort=\"high\"  # Options: \"low\", \"medium\", \"high\"\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an experienced programmer and mathematical analyst.\"),\n",
    "        (\"user\", \"{problem}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "problem = \"\"\"\n",
    "Design an algorithm to find the kth largest element in an unsorted array\n",
    "with the optimal time complexity. Analyze the time and space complexity\n",
    "of your solution and explain why it's optimal.\n",
    "\"\"\"\n",
    "response = chain.invoke({\"problem\": problem})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754da7d4",
   "metadata": {},
   "source": [
    "### Prompts and templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English to French translator.\"),\n",
    "        (\"user\", \"Translate this to French: {text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_messages = template.format_messages(text=\"Hello, how are you?\")\n",
    "result = chat.invoke(formatted_messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133dd75",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a58f2",
   "metadata": {},
   "source": [
    "#### Simple workflows with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4da501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create components\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together using LCEL\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Use the chain\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a7e65",
   "metadata": {},
   "source": [
    "#### Complex chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "# Combine chains\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "\n",
    "# Run the combined chain\n",
    "result = story_with_analysis.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "\n",
    "# Using RunnablePassthrough.assign to preserve data\n",
    "enhanced_chain = RunnablePassthrough.assign(\n",
    "    story=story_chain  # Add 'story' key with generated content\n",
    ").assign(\n",
    "    analysis=analysis_chain  # Add 'analysis' key with analysis of the story\n",
    ")\n",
    "\n",
    "result = enhanced_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys())  # Output: dict_keys(['topic', 'story', 'analysis'])\n",
    "# dict_keys(['topic', 'story', 'analysis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "chat = Config().new_openai_like()\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "story_chain = story_prompt | chat | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"Analyze the following story's mood:\\n{story}\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | chat | StrOutputParser()\n",
    "\n",
    "output_prompt = PromptTemplate.from_template(\n",
    "    \"Here's the story: \\n{story}\\n\\nHere's the mood: \\n{mood}\"\n",
    ")\n",
    "\n",
    "# Alternative approach using dictionary construction\n",
    "manual_chain = (\n",
    "    RunnablePassthrough()  # Pass through input\n",
    "    | {\n",
    "        \"story\": story_chain,  # Add story result\n",
    "        \"topic\": itemgetter(\"topic\"),  # Preserve original topic\n",
    "    }\n",
    "    | RunnablePassthrough().assign(  # Add analysis based on story\n",
    "        analysis=analysis_chain\n",
    "    )\n",
    ")\n",
    "result = manual_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys())  # Output: dict_keys(['story', 'topic', 'analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546703e",
   "metadata": {},
   "source": [
    "## Running local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68292e22",
   "metadata": {},
   "source": [
    "### Getting started with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99748b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh run deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a492658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Ollama with your chosen model\n",
    "local_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0,\n",
    "    # 将 base_url 的地址部分替换为上一步打印出来的地址。\n",
    "    base_url=\"http://172.17.0.3:11434\",\n",
    ")\n",
    "# Create an LCEL chain using the local model\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "local_chain = prompt | local_llm | StrOutputParser()\n",
    "# Use the chain with your local model\n",
    "result = local_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ollama.sh down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75c495",
   "metadata": {},
   "source": [
    "### Working with Hugging Face models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d126502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install langchain-huggingface~=0.3 transformers~=4.57 torch==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# Create a pipeline with a small model:\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Use it like any other LangChain LLM\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Explain the concept of machine learning in simple terms\"),\n",
    "]\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667494a8",
   "metadata": {},
   "source": [
    "### Tips for local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741caa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def safe_model_call(llm, prompt, max_retries=2):\n",
    "    \"\"\"Safely call a local model with retry logic and graceful failure\"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except RuntimeError as e:\n",
    "            # Common error with local models when running out of VRAM\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\n",
    "                    f\"GPU memory error, waiting and retrying ({retries+1}/{max_retries+1})\"\n",
    "                )\n",
    "                time.sleep(2)  # Give system time to free resources\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"Runtime error: {e}\")\n",
    "                return \"An error occurred while processing your request.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error calling model: {e}\")\n",
    "            return \"An error occurred while processing your request.\"\n",
    "    # If we exhausted retries\n",
    "    return \"Model is currently experiencing high load. Please try again later.\"\n",
    "\n",
    "\n",
    "# Use the safety wrapper in your LCEL chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = Config().new_openai_like()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\n",
    "safe_llm = RunnableLambda(lambda x: safe_model_call(llm, x))\n",
    "safe_chain = prompt | safe_llm\n",
    "response = safe_chain.invoke({\"concept\": \"quantum computing\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22f578",
   "metadata": {},
   "source": [
    "## Multimodal AI applications\n",
    "### Text-to-image\n",
    "#### Using DALL-E through OpenAI\n",
    "TODO：找到或实现支持 langchain 接口的千义通问文生图模型（候选 qwen-image-plus）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515f244",
   "metadata": {},
   "source": [
    "### Image understanding\n",
    "#### Using Qwen-VL (similar to Gemini 1.5 Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "with open(\"static/stable-diffusion.png\", \"rb\") as image_file:\n",
    "    image_bytes = image_file.read()\n",
    "    base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "prompt = [\n",
    "    {\"type\": \"text\", \"text\": \"Describe the image: \"},\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_bytes}\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# initialize OpenAI-like model\n",
    "llm = Config().new_openai_like_vl()\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def analyze_image(image_path: str, question: str) -> str:\n",
    "    chat = Config().new_openai_like_vl(max_tokens=256)\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_bytes}\",\n",
    "                    \"detail\": \"auto\",\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = chat.invoke([message])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = \"static/skyscrapers.png\"\n",
    "questions = [\n",
    "    \"What objects do you see in this image?\",\n",
    "    \"What is the overall mood or atmosphere?\",\n",
    "    \"Are there any people in the image?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {analyze_image(image_path, question)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter02 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
